


<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Yale Center for Research Computing Documentation Pages">
      
      
        <link rel="canonical" href="https://docs.ycrc.yale.edu/clusters-at-yale/clusters/farnam/">
      
      
        <meta name="author" content="YCRC">
      
      <link rel="shortcut icon" href="../../../assets/images/favicon.ico">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-5.5.7">
    
    
      
        <title>Farnam - Yale Center for Research Computing</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.b8ac9624.min.css">
      
      
    
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    
      
        
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-133385440-1","docs.ycrc.yale.edu"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){if(this.value){var e=document.location.pathname;ga("send","pageview",e+"?q="+this.value)}})}),document.addEventListener("DOMContentSwitch",function(){ga("send","pageview",document.location.pathname)})</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#farnam" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <!--
  Copyright (c) 2016-2020 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->

<!-- Application header -->
<header class="md-header" data-md-component="header">

  <!-- Top-level navigation -->
  <nav class="md-header-nav md-grid" aria-label="Header">

    <!-- Link to home -->
    <a
      href="https://docs.ycrc.yale.edu"
      title="Yale Center for Research Computing"
      class="md-header-nav__button md-logo"
      aria-label="Yale Center for Research Computing"
    >
      
  <img src="../../../img/manual.svg" alt="logo">

    </a>

    <!-- Button to open drawer -->
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>

    <!-- Header title -->
    <div class="md-header-nav__title" data-md-component="header-title">
        <div class="md-header-nav__ellipsis">
          <span class="md-header-nav__topic md-ellipsis branding">
            <kern style="letter-spacing:-0.09em">Y</kern>ale <em>Center for Research Computing</em>
          </span>
          <span class="md-header-nav__topic md-ellipsis">
            
              Farnam
            
          </span>
        </div>
    </div>

    <!-- Button to open search dialogue -->
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>

      <!-- Search interface -->
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active">
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    

    <!-- Repository containing source -->
    
      <div class="md-header-nav__source">
        
<a href="https://github.com/ycrc/ycrc.github.io/tree/src/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    github/ycrc/docs
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
        
      
      
        
          <!--
  Copyright (c) 2016-2019 Martin Donath <martin.donath@squidfunk.com>
  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:
  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.
  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->

<!-- Determine class according to level -->


  


<!-- Tabs with outline -->
<nav class="md-tabs md-tabs--active" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      <li class="md-tabs__item"><a class="md-tabs__link" href="http://research.computing.yale.edu/about">About</a></li>
      <li class="md-tabs__item"><a class="md-tabs__link" href="http://research.computing.yale.edu/news-events">News & Events</a></li>
      <li class="md-tabs__item"><a class="md-tabs__link" href="http://research.computing.yale.edu/research">Research</a></li>
      <li class="md-tabs__item"><a class="md-tabs__link" href="http://research.computing.yale.edu/services">Services</a></li>
      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../.." class="md-tabs__link md-tabs__link--active">
          Support
        </a>
      
    </li>
  

      
      <li class="md-tabs__item"><a class="md-tabs__link" href="http://research.computing.yale.edu/training">Training</a></li>
      <li class="md-tabs__item status"><a class="md-tabs__link" href="http://research.computing.yale.edu/system-status"><span id='status-icon'></span> System Status</a></li>
    </ul>
  </div>
</nav>

<div id="system-status-message"></div>
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="https://docs.ycrc.yale.edu" title="Yale Center for Research Computing" class="md-nav__button md-logo" aria-label="Yale Center for Research Computing">
      
  <img src="../../../img/manual.svg" alt="logo">

    </a>
    Yale Center for Research Computing
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/ycrc/ycrc.github.io/tree/src/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    github/ycrc/docs
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-1" type="checkbox" id="nav-1" checked>
    
    <label class="md-nav__link" for="nav-1">
      Support
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="Support" data-md-level="1">
      <label class="md-nav__title" for="nav-1">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
        </span>
        Support
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../.." title="Index" class="md-nav__link">
      Index
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-1-2" type="checkbox" id="nav-1-2" checked>
    
    <label class="md-nav__link" for="nav-1-2">
      Cluster Computing at Yale
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="Cluster Computing at Yale" data-md-level="2">
      <label class="md-nav__title" for="nav-1-2">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
        </span>
        Cluster Computing at Yale
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../" title="Getting Started" class="md-nav__link">
      Getting Started
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="https://research.computing.yale.edu/account-request" title="Request an Account" class="md-nav__link">
      Request an Account
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../troubleshoot/" title="Troubleshoot Login" class="md-nav__link">
      Troubleshoot Login
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-1-2-4" type="checkbox" id="nav-1-2-4">
    
    <label class="md-nav__link" for="nav-1-2-4">
      Access the Clusters
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="Access the Clusters" data-md-level="3">
      <label class="md-nav__title" for="nav-1-2-4">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
        </span>
        Access the Clusters
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../access/accounts/" title="About Cluster Accounts" class="md-nav__link">
      About Cluster Accounts
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../access/" title="Log on to the Clusters" class="md-nav__link">
      Log on to the Clusters
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../access/ood/" title="Open OnDemand" class="md-nav__link">
      Open OnDemand
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../access/vpn/" title="Access from Off Campus (VPN)" class="md-nav__link">
      Access from Off Campus (VPN)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../access/x11/" title="Graphical Interfaces (X11)" class="md-nav__link">
      Graphical Interfaces (X11)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../access/vnc/" title="VNC" class="md-nav__link">
      VNC
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../access/mfa/" title="Multi-factor Authentication" class="md-nav__link">
      Multi-factor Authentication
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../access/advanced-config/" title="Advanced SSH Configuration" class="md-nav__link">
      Advanced SSH Configuration
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-1-2-5" type="checkbox" id="nav-1-2-5">
    
    <label class="md-nav__link" for="nav-1-2-5">
      Applications & Software
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="Applications & Software" data-md-level="3">
      <label class="md-nav__title" for="nav-1-2-5">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
        </span>
        Applications & Software
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../applications/" title="Overview" class="md-nav__link">
      Overview
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../applications/compile/" title="Build Software" class="md-nav__link">
      Build Software
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../applications/modules/" title="Software Modules" class="md-nav__link">
      Software Modules
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../applications/toolchains/" title="Module Toolchains" class="md-nav__link">
      Module Toolchains
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../applications/lifecycle/" title="Module Lifecycle" class="md-nav__link">
      Module Lifecycle
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-1-2-6" type="checkbox" id="nav-1-2-6">
    
    <label class="md-nav__link" for="nav-1-2-6">
      Software Guides
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="Software Guides" data-md-level="3">
      <label class="md-nav__title" for="nav-1-2-6">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
        </span>
        Software Guides
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../guides/" title="Overview" class="md-nav__link">
      Overview
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../guides/cesm/" title="CESM/CAM" class="md-nav__link">
      CESM/CAM
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../guides/clustershell/" title="ClusterShell" class="md-nav__link">
      ClusterShell
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../guides/comsol/" title="COMSOL" class="md-nav__link">
      COMSOL
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../guides/conda/" title="Conda" class="md-nav__link">
      Conda
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../guides/cryoem/" title="Cryo-EM on Farnam" class="md-nav__link">
      Cryo-EM on Farnam
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../guides/gaussian/" title="Gaussian" class="md-nav__link">
      Gaussian
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../guides/github_pages/" title="GitHub Pages" class="md-nav__link">
      GitHub Pages
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../guides/gpus-cuda/" title="GPUs and CUDA" class="md-nav__link">
      GPUs and CUDA
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../guides/jupyter/" title="Jupyter Notebooks" class="md-nav__link">
      Jupyter Notebooks
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../guides/matlab/" title="MATLAB" class="md-nav__link">
      MATLAB
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../guides/mathematica/" title="Mathematica" class="md-nav__link">
      Mathematica
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../guides/mpi4py/" title="MPI Parallelism with Python" class="md-nav__link">
      MPI Parallelism with Python
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../guides/python/" title="Python" class="md-nav__link">
      Python
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../guides/handson-ml2/" title="Python+TensorFlow Example" class="md-nav__link">
      Python+TensorFlow Example
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../guides/parallel/" title="Parallel" class="md-nav__link">
      Parallel
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../guides/r/" title="R" class="md-nav__link">
      R
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../guides/rclone/" title="Rclone" class="md-nav__link">
      Rclone
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../guides/singularity/" title="Singularity" class="md-nav__link">
      Singularity
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../guides/tmux/" title="tmux" class="md-nav__link">
      tmux
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../guides/vasp/" title="VASP" class="md-nav__link">
      VASP
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../guides/xvfb/" title="Batch-mode grapical programs with XVFB" class="md-nav__link">
      Batch-mode grapical programs with XVFB
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../guides/github/" title="Version Control with GitHub" class="md-nav__link">
      Version Control with GitHub
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-1-2-7" type="checkbox" id="nav-1-2-7">
    
    <label class="md-nav__link" for="nav-1-2-7">
      Job Scheduling
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="Job Scheduling" data-md-level="3">
      <label class="md-nav__title" for="nav-1-2-7">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
        </span>
        Job Scheduling
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../job-scheduling/" title="Run Jobs with Slurm" class="md-nav__link">
      Run Jobs with Slurm
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../job-scheduling/resource-requests/" title="Request Compute Resources" class="md-nav__link">
      Request Compute Resources
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../job-scheduling/common-job-failures/" title="Common Job Failures" class="md-nav__link">
      Common Job Failures
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../job-scheduling/resource-usage/" title="Monitor CPU and Memory" class="md-nav__link">
      Monitor CPU and Memory
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../job-scheduling/fairshare/" title="Factors Affecting Scheduling" class="md-nav__link">
      Factors Affecting Scheduling
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../job-scheduling/scavenge/" title="Scavenge Partition" class="md-nav__link">
      Scavenge Partition
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../job-scheduling/dependency/" title="Jobs with Dependencies" class="md-nav__link">
      Jobs with Dependencies
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../job-scheduling/scrontab/" title="Recurring Jobs" class="md-nav__link">
      Recurring Jobs
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../job-scheduling/mpi/" title="MPI Partition" class="md-nav__link">
      MPI Partition
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../job-scheduling/dsq/" title="Submit Job Arrays with dSQ" class="md-nav__link">
      Submit Job Arrays with dSQ
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../job-scheduling/slurm-examples/" title="Submission Script Examples" class="md-nav__link">
      Submission Script Examples
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-1-2-8" type="checkbox" id="nav-1-2-8">
    
    <label class="md-nav__link" for="nav-1-2-8">
      Data & Storage
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="Data & Storage" data-md-level="3">
      <label class="md-nav__title" for="nav-1-2-8">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
        </span>
        Data & Storage
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../data/" title="Cluster Storage" class="md-nav__link">
      Cluster Storage
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../data/transfer/" title="Transfer Data" class="md-nav__link">
      Transfer Data
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../data/staging/" title="Stage Data for Compute Jobs" class="md-nav__link">
      Stage Data for Compute Jobs
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../data/globus/" title="Large Transfers with Globus" class="md-nav__link">
      Large Transfers with Globus
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../data/permissions/" title="Manage Permissions for Sharing" class="md-nav__link">
      Manage Permissions for Sharing
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../data/archived-sequencing/" title="Ruddle Archived Sequence Data" class="md-nav__link">
      Ruddle Archived Sequence Data
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-1-2-9" type="checkbox" id="nav-1-2-9" checked>
    
    <label class="md-nav__link" for="nav-1-2-9">
      The Clusters
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="The Clusters" data-md-level="3">
      <label class="md-nav__title" for="nav-1-2-9">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
        </span>
        The Clusters
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../" title="HPC Resources" class="md-nav__link">
      HPC Resources
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../grace/" title="Grace" class="md-nav__link">
      Grace
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Farnam
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 9h14V7H3v2m0 4h14v-2H3v2m0 4h14v-2H3v2m16 0h2v-2h-2v2m0-10v2h2V7h-2m0 6h2v-2h-2v2z"/></svg>
        </span>
      </label>
    
    <a href="./" title="Farnam" class="md-nav__link md-nav__link--active">
      Farnam
    </a>
    
      
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#access-the-cluster" class="md-nav__link">
    Access the Cluster
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#system-status-and-monitoring" class="md-nav__link">
    System Status and Monitoring
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#partitions-and-hardware" class="md-nav__link">
    Partitions and Hardware
  </a>
  
    <nav class="md-nav" aria-label="Partitions and Hardware">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#public-partitions" class="md-nav__link">
    Public Partitions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#private-partitions" class="md-nav__link">
    Private Partitions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#public-datasets" class="md-nav__link">
    Public Datasets
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#storage" class="md-nav__link">
    Storage
  </a>
  
</li>
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../ruddle/" title="Ruddle" class="md-nav__link">
      Ruddle
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../milgram/" title="Milgram" class="md-nav__link">
      Milgram
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-1-3" type="checkbox" id="nav-1-3">
    
    <label class="md-nav__link" for="nav-1-3">
      Research Data
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="Research Data" data-md-level="2">
      <label class="md-nav__title" for="nav-1-3">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
        </span>
        Research Data
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../data/" title="Data Storage Options" class="md-nav__link">
      Data Storage Options
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../data/archive/" title="Archive Your Data" class="md-nav__link">
      Archive Your Data
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../data/google-drive/" title="Google Drive" class="md-nav__link">
      Google Drive
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../data/external/" title="Serving Research Data Externally" class="md-nav__link">
      Serving Research Data Externally
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../user-group/" title="YCRC User Group" class="md-nav__link">
      YCRC User Group
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../national-hpcs/" title="National HPCs" class="md-nav__link">
      National HPCs
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../online-tutorials/" title="Online Tutorials" class="md-nav__link">
      Online Tutorials
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#access-the-cluster" class="md-nav__link">
    Access the Cluster
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#system-status-and-monitoring" class="md-nav__link">
    System Status and Monitoring
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#partitions-and-hardware" class="md-nav__link">
    Partitions and Hardware
  </a>
  
    <nav class="md-nav" aria-label="Partitions and Hardware">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#public-partitions" class="md-nav__link">
    Public Partitions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#private-partitions" class="md-nav__link">
    Private Partitions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#public-datasets" class="md-nav__link">
    Public Datasets
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#storage" class="md-nav__link">
    Storage
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/ycrc/ycrc.github.io/tree/src/docs/clusters-at-yale/clusters/farnam.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                  
                
                
                <h1 id="farnam">Farnam</h1>
<p><img alt="Louise" class="cluster-portrait" src="/img/Louise-Whitman-Farnam.jpg" /></p>
<p>Farnam is a shared-use resource for the <a href="https://medicine.yale.edu">Yale School of Medicine</a> (YSM). It consists of a variety of compute nodes networked over ethernet and mounts several shared filesystems.</p>
<p>The Farnam Cluster is named for <a href="http://archives.yalealumnimagazine.com/issues/2006_09/old_yale.html">Louise Whitman Farnam</a>, the first woman to graduate from the Yale School of Medicine, class of 1916.</p>
<hr />
<h2 id="access-the-cluster">Access the Cluster</h2>
<p>Once you have <a href="https://research.computing.yale.edu/support/hpc/account-request">an account</a>, the cluster can be accessed <a href="/clusters-at-yale/access">via ssh</a> or through the <a href="/clusters-at-yale/access/ood/">Open OnDemand web portal</a>.</p>
<h2 id="system-status-and-monitoring">System Status and Monitoring</h2>
<p>For system status messages and the schedule for upcoming maintenance, please see the <a href="https://research.computing.yale.edu/support/hpc/system-status">system status page</a>. For a current node-level view of job activity, see the <a href="http://cluster.ycrc.yale.edu/farnam/">cluster monitor page (VPN only)</a>.</p>
<h2 id="partitions-and-hardware">Partitions and Hardware</h2>
<p>Farnam is made up of several kinds of compute nodes. We group them into  (sometimes overlapping)  <a href="/clusters-at-yale/job-scheduling">Slurm partitions</a> meant to serve different purposes. By combining the <code>--partition</code> and <a href="/clusters-at-yale/job-scheduling/resource-requests#features-and-constraints"><code>--constraint</code></a> Slurm options you can more finely control what nodes your jobs can run on.</p>
<div class="admonition warning">
<p class="admonition-title">Job Submission Rate Limits</p>
<p>Job submissions are limited to <strong>200 jobs per hour</strong>. See the Rate Limits section in the <a href="/clusters-at-yale/job-scheduling/common-job-failures/#rate-limits">Common Job Failures</a> page for more info.</p>
</div>
<h3 id="public-partitions">Public Partitions</h3>
<p>See each tab below for more information about the available common use partitions.</p>
<div class="tabbed-set" data-tabs="1:8"><input checked="checked" id="__tabbed_1_1" name="__tabbed_1" type="radio" /><label for="__tabbed_1_1">general</label><div class="tabbed-content">
<p>Use the general partition for most batch jobs. This is the default if you don't specify one with <code>--partition</code>.</p>
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the general partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>30-00:00:00</code></td>
</tr>
<tr>
<td>Maximum CPUs per group</td>
<td><code>400</code></td>
</tr>
<tr>
<td>Maximum memory per group</td>
<td><code>2.50T</code></td>
</tr>
<tr>
<td>Maximum CPUs per user</td>
<td><code>200</code></td>
</tr>
<tr>
<td>Maximum memory per user</td>
<td><code>1280G</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>18</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>cascadelake, avx2, avx512, 6240, nogpu, standard, common, bigtmp</td>
</tr>
<tr>
<td>85</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td>haswell, avx2, E5-2660_v3, nogpu, standard, oldest, common</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_1_2" name="__tabbed_1" type="radio" /><label for="__tabbed_1_2">interactive</label><div class="tabbed-content">
<p>Use the interactive partition to jobs with which you need ongoing interaction. For example, exploratory analyses or debugging compilation.</p>
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=06:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the interactive partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>1-00:00:00</code></td>
</tr>
<tr>
<td>Maximum CPUs per user</td>
<td><code>20</code></td>
</tr>
<tr>
<td>Maximum memory per user</td>
<td><code>256G</code></td>
</tr>
<tr>
<td>Maximum running jobs per user</td>
<td><code>2</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>18</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>cascadelake, avx2, avx512, 6240, nogpu, standard, common, bigtmp</td>
</tr>
<tr>
<td>97</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td>haswell, avx2, E5-2660_v3, nogpu, standard, oldest, common</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_1_3" name="__tabbed_1" type="radio" /><label for="__tabbed_1_3">transfer</label><div class="tabbed-content">
<p>Use the transfer partition to stage data for your jobs to and from <a href="/clusters-at-yale/data/#staging-data">cluster storage</a>.</p>
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the transfer partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>1-00:00:00</code></td>
</tr>
<tr>
<td>Maximum running jobs per user</td>
<td><code>2</code></td>
</tr>
<tr>
<td>Maximum CPUs per job</td>
<td><code>1</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td>haswell, avx2, E5-2660_v3, nogpu, standard, oldest, common</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_1_4" name="__tabbed_1" type="radio" /><label for="__tabbed_1_4">gpu</label><div class="tabbed-content">
<p>Use the gpu partition for jobs that make use of GPUs. You must <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus">request GPUs explicitly</a> with the <code>--gpus</code> option in order to use them. For example, <code>--gpus=gtx1080ti:2</code> would request 2 GeForce GTX 1080Ti GPUs per node.</p>
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the gpu partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>2-00:00:00</code></td>
</tr>
<tr>
<td>Maximum CPUs per user</td>
<td><code>32</code></td>
</tr>
<tr>
<td>Maximum GPUs per user</td>
<td><code>12</code></td>
</tr>
<tr>
<td>Maximum memory per user</td>
<td><code>256G</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>6</td>
<td>5222</td>
<td>8</td>
<td>181</td>
<td>rtx5000</td>
<td>4</td>
<td>16</td>
<td>cascadelake, avx2, avx512, 5222, doubleprecision, common, bigtmp, rtx5000</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>370</td>
<td>a100</td>
<td>4</td>
<td>40</td>
<td>cascadelake, avx2, avx512, 6240, doubleprecision, bigtmp, a100</td>
</tr>
<tr>
<td>1</td>
<td>E5-2637_v4</td>
<td>8</td>
<td>119</td>
<td>gtx1080ti</td>
<td>4</td>
<td>11</td>
<td>broadwell, avx2, E5-2637_v4, singleprecision, common, gtx1080ti</td>
</tr>
<tr>
<td>9</td>
<td>E5-2637_v4</td>
<td>8</td>
<td>119</td>
<td>gtx1080ti</td>
<td>4</td>
<td>11</td>
<td>broadwell, avx2, E5-2637_v4, singleprecision, common, gtx1080ti</td>
</tr>
<tr>
<td>2</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td>k80</td>
<td>4</td>
<td>12</td>
<td>haswell, avx2, E5-2660_v3, doubleprecision, common, k80</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_1_5" name="__tabbed_1" type="radio" /><label for="__tabbed_1_5">gpu_devel</label><div class="tabbed-content">
<p>Use the gpu_devel partition to debug jobs that make use of GPUs, or to develop GPU-enabled code.</p>
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=00:10:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the gpu_devel partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>02:00:00</code></td>
</tr>
<tr>
<td>Maximum submitted jobs per user</td>
<td><code>1</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>E5-2623_v4</td>
<td>8</td>
<td>57</td>
<td>gtx1080ti</td>
<td>4</td>
<td>11</td>
<td>broadwell, avx2, E5-2623_v4, singleprecision, common, gtx1080ti</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_1_6" name="__tabbed_1" type="radio" /><label for="__tabbed_1_6">bigmem</label><div class="tabbed-content">
<p>Use the bigmem partition for jobs that have memory requirements other partitions can't handle.</p>
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the bigmem partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>3-00:00:00</code></td>
</tr>
<tr>
<td>Maximum CPUs per user</td>
<td><code>32</code></td>
</tr>
<tr>
<td>Maximum memory per user</td>
<td><code>1532G</code></td>
</tr>
<tr>
<td>Maximum running jobs per user</td>
<td><code>2</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>3</td>
<td>6240</td>
<td>36</td>
<td>1505</td>
<td>cascadelake, avx2, avx512, 6240, nogpu, common, bigtmp</td>
</tr>
<tr>
<td>2</td>
<td>6234</td>
<td>16</td>
<td>1505</td>
<td>cascadelake, avx2, avx512, 6234, nogpu, common, bigtmp</td>
</tr>
<tr>
<td>2</td>
<td>E7-4809_v3</td>
<td>32</td>
<td>1505</td>
<td>haswell, avx2, E7-4809_v3, nogpu, common</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_1_7" name="__tabbed_1" type="radio" /><label for="__tabbed_1_7">scavenge</label><div class="tabbed-content">
<p>Use the scavenge partition to run preemptable jobs on more resources than normally allowed. For more information about scavenge, see the <a href="/clusters-at-yale/job-scheduling/scavenge">Scavenge documentation</a>.</p>
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the scavenge partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
<tr>
<td>Maximum CPUs per user</td>
<td><code>800</code></td>
</tr>
<tr>
<td>Maximum memory per user</td>
<td><code>5T</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>7</td>
<td>8268</td>
<td>48</td>
<td>356</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx512, 8268, nogpu, standard, bigtmp, pi</td>
</tr>
<tr>
<td>6</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx2, avx512, 6240, nogpu, standard, pi, bigtmp</td>
</tr>
<tr>
<td>18</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx2, avx512, 6240, nogpu, standard, common, bigtmp</td>
</tr>
<tr>
<td>6</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx2, avx512, 6240, nogpu, pi, bigtmp</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>370</td>
<td>v100</td>
<td>4</td>
<td>16</td>
<td>cascadelake, avx2, avx512, 6240, pi, v100</td>
</tr>
<tr>
<td>1</td>
<td>6226r</td>
<td>32</td>
<td>181</td>
<td>rtx3090</td>
<td>4</td>
<td>24</td>
<td>cascadelake, avx2, avx512, 6226r, doubleprecision, pi, rtx3090</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>rtx3090</td>
<td>4</td>
<td>24</td>
<td>cascadelake, avx2, avx512, 6240, singleprecision, pi, bigtmp, rtx3090</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>rtx3090</td>
<td>8</td>
<td>24</td>
<td>cascadelake, avx2, avx512, 6240, singleprecision, pi, bigtmp, rtx3090</td>
</tr>
<tr>
<td>4</td>
<td>6240</td>
<td>36</td>
<td>370</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx2, avx512, 6240, nogpu, pi, bigtmp</td>
</tr>
<tr>
<td>4</td>
<td>6240</td>
<td>36</td>
<td>748</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx2, avx512, 6240, nogpu, pi, bigtmp</td>
</tr>
<tr>
<td>8</td>
<td>5222</td>
<td>8</td>
<td>181</td>
<td>rtx5000</td>
<td>4</td>
<td>16</td>
<td>cascadelake, avx2, avx512, 5222, doubleprecision, pi, bigtmp, rtx5000</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>1505</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx2, avx512, 6240, nogpu, pi, bigtmp</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>rtx2080ti</td>
<td>2</td>
<td>11</td>
<td>cascadelake, avx2, avx512, 6240, singleprecision, pi, bigtmp, rtx2080ti</td>
</tr>
<tr>
<td>1</td>
<td>6248r</td>
<td>48</td>
<td>370</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx2, avx512, 6248r, nogpu, pi, bigtmp</td>
</tr>
<tr>
<td>3</td>
<td>6240</td>
<td>36</td>
<td>1505</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx2, avx512, 6240, nogpu, common, bigtmp</td>
</tr>
<tr>
<td>1</td>
<td>6242</td>
<td>32</td>
<td>999</td>
<td>rtx8000</td>
<td>2</td>
<td>48</td>
<td>cascadelake, avx2, avx512, 6242, doubleprecision, pi, bigtmp, rtx8000</td>
</tr>
<tr>
<td>3</td>
<td>8268</td>
<td>48</td>
<td>370</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx2, avx512, 8268, nogpu, pi, bigtmp</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>370</td>
<td>a100</td>
<td>4</td>
<td>40</td>
<td>cascadelake, avx2, avx512, 6240, doubleprecision, bigtmp, a100</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>a100</td>
<td>4</td>
<td>40</td>
<td>cascadelake, avx2, avx512, 6240, doubleprecision, pi, a100, bigtmp</td>
</tr>
<tr>
<td>2</td>
<td>6132</td>
<td>28</td>
<td>181</td>
<td></td>
<td></td>
<td></td>
<td>skylake, avx2, avx512, 6132, nogpu, standard, pi, bigtmp</td>
</tr>
<tr>
<td>2</td>
<td>5122</td>
<td>8</td>
<td>181</td>
<td>rtx2080</td>
<td>4</td>
<td>8</td>
<td>skylake, avx2, avx512, 5122, singleprecision, pi, rtx2080</td>
</tr>
<tr>
<td>1</td>
<td>6132</td>
<td>28</td>
<td>749</td>
<td></td>
<td></td>
<td></td>
<td>skylake, avx2, avx512, 6132, nogpu, pi, bigtmp</td>
</tr>
<tr>
<td>38</td>
<td>E5-2680_v4</td>
<td>28</td>
<td>245</td>
<td></td>
<td></td>
<td></td>
<td>broadwell, avx2, E5-2680_v4, nogpu, standard, pi</td>
</tr>
<tr>
<td>1</td>
<td>E5-2637_v4</td>
<td>8</td>
<td>119</td>
<td>gtx1080ti</td>
<td>4</td>
<td>11</td>
<td>broadwell, avx2, E5-2637_v4, singleprecision, common, gtx1080ti</td>
</tr>
<tr>
<td>9</td>
<td>E5-2637_v4</td>
<td>8</td>
<td>119</td>
<td>gtx1080ti</td>
<td>4</td>
<td>11</td>
<td>broadwell, avx2, E5-2637_v4, singleprecision, common, gtx1080ti</td>
</tr>
<tr>
<td>11</td>
<td>E5-2637_v4</td>
<td>8</td>
<td>119</td>
<td>gtx1080ti</td>
<td>4</td>
<td>11</td>
<td>broadwell, avx2, E5-2637_v4, singleprecision, pi, gtx1080ti</td>
</tr>
<tr>
<td>1</td>
<td>E7-4820_v4</td>
<td>40</td>
<td>1505</td>
<td></td>
<td></td>
<td></td>
<td>broadwell, avx2, E7-4820_v4, nogpu, pi</td>
</tr>
<tr>
<td>3</td>
<td>E5-2680_v4</td>
<td>28</td>
<td>245</td>
<td>p100</td>
<td>2</td>
<td>16</td>
<td>broadwell, avx2, E5-2680_v4, doubleprecision, pi, p100</td>
</tr>
<tr>
<td>1</td>
<td>E5-2623_v4</td>
<td>8</td>
<td>57</td>
<td>gtx1080ti</td>
<td>4</td>
<td>11</td>
<td>broadwell, avx2, E5-2623_v4, singleprecision, common, gtx1080ti</td>
</tr>
<tr>
<td>1</td>
<td>E5-2637_v4</td>
<td>8</td>
<td>119</td>
<td>titanv</td>
<td>4</td>
<td>12</td>
<td>broadwell, avx2, E5-2637_v4, doubleprecision, pi, bigtmp, titanv</td>
</tr>
<tr>
<td>18</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td></td>
<td></td>
<td></td>
<td>haswell, avx2, E5-2660_v3, nogpu, standard, oldest, pi</td>
</tr>
<tr>
<td>99</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td></td>
<td></td>
<td></td>
<td>haswell, avx2, E5-2660_v3, nogpu, standard, oldest, common</td>
</tr>
<tr>
<td>2</td>
<td>E7-4809_v3</td>
<td>32</td>
<td>1505</td>
<td></td>
<td></td>
<td></td>
<td>haswell, avx2, E7-4809_v3, nogpu, common</td>
</tr>
<tr>
<td>3</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td>k80</td>
<td>4</td>
<td>12</td>
<td>haswell, avx2, E5-2660_v3, doubleprecision, pi, k80</td>
</tr>
<tr>
<td>2</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td>k80</td>
<td>4</td>
<td>12</td>
<td>haswell, avx2, E5-2660_v3, doubleprecision, common, k80</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_1_8" name="__tabbed_1" type="radio" /><label for="__tabbed_1_8">scavenge_gpu</label><div class="tabbed-content">
<p>Use the scavenge_gpu partition to run preemptable jobs on more GPU resources than normally allowed. For more information about scavenge, see the <a href="/clusters-at-yale/job-scheduling/scavenge">Scavenge documentation</a>.</p>
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the scavenge_gpu partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>2-00:00:00</code></td>
</tr>
<tr>
<td>Maximum GPUs per user</td>
<td><code>64</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>rtx2080ti</td>
<td>4</td>
<td>11</td>
<td>cascadelake, avx2, avx512, 6240, singleprecision, pi, bigtmp, rtx2080ti</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>370</td>
<td>v100</td>
<td>4</td>
<td>16</td>
<td>cascadelake, avx2, avx512, 6240, pi, v100</td>
</tr>
<tr>
<td>1</td>
<td>6226r</td>
<td>32</td>
<td>181</td>
<td>rtx3090</td>
<td>4</td>
<td>24</td>
<td>cascadelake, avx2, avx512, 6226r, doubleprecision, pi, rtx3090</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>rtx3090</td>
<td>4</td>
<td>24</td>
<td>cascadelake, avx2, avx512, 6240, singleprecision, pi, bigtmp, rtx3090</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>rtx3090</td>
<td>8</td>
<td>24</td>
<td>cascadelake, avx2, avx512, 6240, singleprecision, pi, bigtmp, rtx3090</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>370</td>
<td>a100</td>
<td>4</td>
<td>40</td>
<td>cascadelake, avx2, avx512, 6240, doubleprecision, bigtmp, a100</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>a100</td>
<td>4</td>
<td>40</td>
<td>cascadelake, avx2, avx512, 6240, doubleprecision, pi, a100, bigtmp</td>
</tr>
<tr>
<td>2</td>
<td>5122</td>
<td>8</td>
<td>181</td>
<td>rtx2080</td>
<td>4</td>
<td>8</td>
<td>skylake, avx2, avx512, 5122, singleprecision, pi, rtx2080</td>
</tr>
<tr>
<td>1</td>
<td>E5-2637_v4</td>
<td>8</td>
<td>119</td>
<td>gtx1080ti</td>
<td>4</td>
<td>11</td>
<td>broadwell, avx2, E5-2637_v4, singleprecision, common, gtx1080ti</td>
</tr>
<tr>
<td>9</td>
<td>E5-2637_v4</td>
<td>8</td>
<td>119</td>
<td>gtx1080ti</td>
<td>4</td>
<td>11</td>
<td>broadwell, avx2, E5-2637_v4, singleprecision, common, gtx1080ti</td>
</tr>
<tr>
<td>11</td>
<td>E5-2637_v4</td>
<td>8</td>
<td>119</td>
<td>gtx1080ti</td>
<td>4</td>
<td>11</td>
<td>broadwell, avx2, E5-2637_v4, singleprecision, pi, gtx1080ti</td>
</tr>
<tr>
<td>3</td>
<td>E5-2680_v4</td>
<td>28</td>
<td>245</td>
<td>p100</td>
<td>2</td>
<td>16</td>
<td>broadwell, avx2, E5-2680_v4, doubleprecision, pi, p100</td>
</tr>
<tr>
<td>1</td>
<td>E5-2623_v4</td>
<td>8</td>
<td>57</td>
<td>gtx1080ti</td>
<td>4</td>
<td>11</td>
<td>broadwell, avx2, E5-2623_v4, singleprecision, common, gtx1080ti</td>
</tr>
<tr>
<td>1</td>
<td>E5-2637_v4</td>
<td>8</td>
<td>119</td>
<td>titanv</td>
<td>4</td>
<td>12</td>
<td>broadwell, avx2, E5-2637_v4, doubleprecision, pi, bigtmp, titanv</td>
</tr>
<tr>
<td>3</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td>k80</td>
<td>4</td>
<td>12</td>
<td>haswell, avx2, E5-2660_v3, doubleprecision, pi, k80</td>
</tr>
<tr>
<td>2</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td>k80</td>
<td>4</td>
<td>12</td>
<td>haswell, avx2, E5-2660_v3, doubleprecision, common, k80</td>
</tr>
</tbody>
</table>
</div>
</div>
<h3 id="private-partitions">Private Partitions</h3>
<p>With few exceptions, jobs submitted to private partitions are not considered when calculating your group's <a href="/clusters-at-yale/job-scheduling/fairshare/">Fairshare</a>. Your group can purchase additional hardware for private use, which we will make available as a <code>pi_groupname</code> partition. These nodes are purchased by you, but supported and administered by us. After vendor support expires, we retire compute nodes. Compute nodes can range from $10K to upwards of $50K depending on your requirements. If you are interested in purchasing nodes for your group, please <a href="/#get-help">contact us</a>.</p>
<details class="summary"><summary>PI Partitions (click to expand)</summary><div class="tabbed-set" data-tabs="2:30"><input checked="checked" id="__tabbed_2_1" name="__tabbed_2" type="radio" /><label for="__tabbed_2_1">pi_breaker</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_breaker partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>24</td>
<td>E5-2680_v4</td>
<td>28</td>
<td>245</td>
<td>broadwell, avx2, E5-2680_v4, nogpu, standard, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_2" name="__tabbed_2" type="radio" /><label for="__tabbed_2_2">pi_butterwick</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_butterwick partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>a100</td>
<td>4</td>
<td>40</td>
<td>cascadelake, avx2, avx512, 6240, doubleprecision, pi, a100, bigtmp</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_3" name="__tabbed_2" type="radio" /><label for="__tabbed_2_3">pi_chenlab</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_chenlab partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>8268</td>
<td>48</td>
<td>370</td>
<td>cascadelake, avx2, avx512, 8268, nogpu, pi, bigtmp</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_4" name="__tabbed_2" type="radio" /><label for="__tabbed_2_4">pi_cryo_realtime</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_cryo_realtime partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
<tr>
<td>Maximum GPUs per user</td>
<td><code>12</code></td>
</tr>
<tr>
<td>Maximum running jobs per user</td>
<td><code>2</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>E5-2637_v4</td>
<td>8</td>
<td>119</td>
<td>gtx1080ti</td>
<td>4</td>
<td>11</td>
<td>broadwell, avx2, E5-2637_v4, singleprecision, pi, gtx1080ti</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_5" name="__tabbed_2" type="radio" /><label for="__tabbed_2_5">pi_cryoem</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_cryoem partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>365-00:00:00</code></td>
</tr>
<tr>
<td>Maximum GPUs per user</td>
<td><code>12</code></td>
</tr>
<tr>
<td>Maximum running jobs per user</td>
<td><code>2</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>9</td>
<td>E5-2637_v4</td>
<td>8</td>
<td>119</td>
<td>gtx1080ti</td>
<td>4</td>
<td>11</td>
<td>broadwell, avx2, E5-2637_v4, singleprecision, pi, gtx1080ti</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_6" name="__tabbed_2" type="radio" /><label for="__tabbed_2_6">pi_deng</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_deng partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>E5-2680_v4</td>
<td>28</td>
<td>245</td>
<td>p100</td>
<td>2</td>
<td>16</td>
<td>broadwell, avx2, E5-2680_v4, doubleprecision, pi, p100</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_7" name="__tabbed_2" type="radio" /><label for="__tabbed_2_7">pi_dewan</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_dewan partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>cascadelake, avx2, avx512, 6240, nogpu, pi, bigtmp</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_8" name="__tabbed_2" type="radio" /><label for="__tabbed_2_8">pi_dunn</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_dunn partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>cascadelake, avx2, avx512, 6240, nogpu, standard, pi, bigtmp</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_9" name="__tabbed_2" type="radio" /><label for="__tabbed_2_9">pi_edwards</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_edwards partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>cascadelake, avx2, avx512, 6240, nogpu, standard, pi, bigtmp</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_10" name="__tabbed_2" type="radio" /><label for="__tabbed_2_10">pi_falcone</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_falcone partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx2, avx512, 6240, nogpu, pi, bigtmp</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>370</td>
<td>v100</td>
<td>4</td>
<td>16</td>
<td>cascadelake, avx2, avx512, 6240, pi, v100</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>1505</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx2, avx512, 6240, nogpu, pi, bigtmp</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_11" name="__tabbed_2" type="radio" /><label for="__tabbed_2_11">pi_galvani</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_galvani partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>7</td>
<td>8268</td>
<td>48</td>
<td>356</td>
<td>cascadelake, avx512, 8268, nogpu, standard, bigtmp, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_12" name="__tabbed_2" type="radio" /><label for="__tabbed_2_12">pi_gerstein</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_gerstein partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>6132</td>
<td>28</td>
<td>181</td>
<td>skylake, avx2, avx512, 6132, nogpu, standard, pi, bigtmp</td>
</tr>
<tr>
<td>1</td>
<td>6132</td>
<td>28</td>
<td>749</td>
<td>skylake, avx2, avx512, 6132, nogpu, pi, bigtmp</td>
</tr>
<tr>
<td>11</td>
<td>E5-2680_v4</td>
<td>28</td>
<td>245</td>
<td>broadwell, avx2, E5-2680_v4, nogpu, standard, pi</td>
</tr>
<tr>
<td>1</td>
<td>E7-4820_v4</td>
<td>40</td>
<td>1505</td>
<td>broadwell, avx2, E7-4820_v4, nogpu, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_13" name="__tabbed_2" type="radio" /><label for="__tabbed_2_13">pi_gerstein_gpu</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_gerstein_gpu partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>rtx3090</td>
<td>4</td>
<td>24</td>
<td>cascadelake, avx2, avx512, 6240, singleprecision, pi, bigtmp, rtx3090</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>rtx3090</td>
<td>8</td>
<td>24</td>
<td>cascadelake, avx2, avx512, 6240, singleprecision, pi, bigtmp, rtx3090</td>
</tr>
<tr>
<td>2</td>
<td>E5-2680_v4</td>
<td>28</td>
<td>245</td>
<td>p100</td>
<td>2</td>
<td>16</td>
<td>broadwell, avx2, E5-2680_v4, doubleprecision, pi, p100</td>
</tr>
<tr>
<td>1</td>
<td>E5-2637_v4</td>
<td>8</td>
<td>119</td>
<td>titanv</td>
<td>4</td>
<td>12</td>
<td>broadwell, avx2, E5-2637_v4, doubleprecision, pi, bigtmp, titanv</td>
</tr>
<tr>
<td>3</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td>k80</td>
<td>4</td>
<td>12</td>
<td>haswell, avx2, E5-2660_v3, doubleprecision, pi, k80</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_14" name="__tabbed_2" type="radio" /><label for="__tabbed_2_14">pi_gruen</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_gruen partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>E5-2680_v4</td>
<td>28</td>
<td>245</td>
<td>broadwell, avx2, E5-2680_v4, nogpu, standard, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_15" name="__tabbed_2" type="radio" /><label for="__tabbed_2_15">pi_jadi</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_jadi partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>365-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>E5-2680_v4</td>
<td>28</td>
<td>245</td>
<td>broadwell, avx2, E5-2680_v4, nogpu, standard, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_16" name="__tabbed_2" type="radio" /><label for="__tabbed_2_16">pi_jetz</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_jetz partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>4</td>
<td>6240</td>
<td>36</td>
<td>370</td>
<td>cascadelake, avx2, avx512, 6240, nogpu, pi, bigtmp</td>
</tr>
<tr>
<td>4</td>
<td>6240</td>
<td>36</td>
<td>748</td>
<td>cascadelake, avx2, avx512, 6240, nogpu, pi, bigtmp</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_17" name="__tabbed_2" type="radio" /><label for="__tabbed_2_17">pi_kleinstein</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_kleinstein partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>cascadelake, avx2, avx512, 6240, nogpu, standard, pi, bigtmp</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>cascadelake, avx2, avx512, 6240, nogpu, pi, bigtmp</td>
</tr>
<tr>
<td>3</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td>haswell, avx2, E5-2660_v3, nogpu, standard, oldest, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_18" name="__tabbed_2" type="radio" /><label for="__tabbed_2_18">pi_krauthammer</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_krauthammer partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td>haswell, avx2, E5-2660_v3, nogpu, standard, oldest, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_19" name="__tabbed_2" type="radio" /><label for="__tabbed_2_19">pi_ma</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_ma partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>8268</td>
<td>48</td>
<td>370</td>
<td>cascadelake, avx2, avx512, 8268, nogpu, pi, bigtmp</td>
</tr>
<tr>
<td>2</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td>haswell, avx2, E5-2660_v3, nogpu, standard, oldest, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_20" name="__tabbed_2" type="radio" /><label for="__tabbed_2_20">pi_medzhitov</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_medzhitov partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>cascadelake, avx2, avx512, 6240, nogpu, pi, bigtmp</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_21" name="__tabbed_2" type="radio" /><label for="__tabbed_2_21">pi_miranker</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_miranker partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6248r</td>
<td>48</td>
<td>370</td>
<td>cascadelake, avx2, avx512, 6248r, nogpu, pi, bigtmp</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_22" name="__tabbed_2" type="radio" /><label for="__tabbed_2_22">pi_ohern</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_ohern partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>5</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td>haswell, avx2, E5-2660_v3, nogpu, standard, oldest, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_23" name="__tabbed_2" type="radio" /><label for="__tabbed_2_23">pi_reinisch</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_reinisch partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>5122</td>
<td>8</td>
<td>181</td>
<td>rtx2080</td>
<td>4</td>
<td>8</td>
<td>skylake, avx2, avx512, 5122, singleprecision, pi, rtx2080</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_24" name="__tabbed_2" type="radio" /><label for="__tabbed_2_24">pi_sigworth</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_sigworth partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>rtx2080ti</td>
<td>2</td>
<td>11</td>
<td>cascadelake, avx2, avx512, 6240, singleprecision, pi, bigtmp, rtx2080ti</td>
</tr>
<tr>
<td>1</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td></td>
<td></td>
<td></td>
<td>haswell, avx2, E5-2660_v3, nogpu, standard, oldest, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_25" name="__tabbed_2" type="radio" /><label for="__tabbed_2_25">pi_sindelar</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_sindelar partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>rtx2080ti</td>
<td>4</td>
<td>11</td>
<td>cascadelake, avx2, avx512, 6240, singleprecision, pi, bigtmp, rtx2080ti</td>
</tr>
<tr>
<td>1</td>
<td>E5-2637_v4</td>
<td>8</td>
<td>119</td>
<td>gtx1080ti</td>
<td>4</td>
<td>11</td>
<td>broadwell, avx2, E5-2637_v4, singleprecision, pi, gtx1080ti</td>
</tr>
<tr>
<td>1</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td></td>
<td></td>
<td></td>
<td>haswell, avx2, E5-2660_v3, nogpu, standard, oldest, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_26" name="__tabbed_2" type="radio" /><label for="__tabbed_2_26">pi_tomography</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_tomography partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>4-00:00:00</code></td>
</tr>
<tr>
<td>Maximum GPUs per user</td>
<td><code>12</code></td>
</tr>
<tr>
<td>Maximum running jobs per user</td>
<td><code>2</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>8</td>
<td>5222</td>
<td>8</td>
<td>181</td>
<td>rtx5000</td>
<td>4</td>
<td>16</td>
<td>cascadelake, avx2, avx512, 5222, doubleprecision, pi, bigtmp, rtx5000</td>
</tr>
<tr>
<td>1</td>
<td>6242</td>
<td>32</td>
<td>999</td>
<td>rtx8000</td>
<td>2</td>
<td>48</td>
<td>cascadelake, avx2, avx512, 6242, doubleprecision, pi, bigtmp, rtx8000</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_27" name="__tabbed_2" type="radio" /><label for="__tabbed_2_27">pi_townsend</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_townsend partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>5</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td>haswell, avx2, E5-2660_v3, nogpu, standard, oldest, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_28" name="__tabbed_2" type="radio" /><label for="__tabbed_2_28">pi_ya-chi_ho</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_ya-chi_ho partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>8268</td>
<td>48</td>
<td>370</td>
<td>cascadelake, avx2, avx512, 8268, nogpu, pi, bigtmp</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_29" name="__tabbed_2" type="radio" /><label for="__tabbed_2_29">pi_yong_xiong</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_yong_xiong partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6226r</td>
<td>32</td>
<td>181</td>
<td>rtx3090</td>
<td>4</td>
<td>24</td>
<td>cascadelake, avx2, avx512, 6226r, doubleprecision, pi, rtx3090</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_30" name="__tabbed_2" type="radio" /><label for="__tabbed_2_30">pi_zhao</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>

<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_zhao partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>cascadelake, avx2, avx512, 6240, nogpu, standard, pi, bigtmp</td>
</tr>
</tbody>
</table>
</div>
</div>
</details>
<h2 id="public-datasets">Public Datasets</h2>
<p>We host datasets of general interest in a loosely organized directory tree in <code>/gpfs/ysm/datasets</code>:</p>
<div class="highlight"><pre><span></span><code> cryoem
 db
  annovar
  blast
  busco
  Pfam
 genomes
     1000Genomes
     10xgenomics
     Aedes_aegypti
     Chelonoidis_nigra
     Danio_rerio
     Drosophila_melanogaster
     hisat2
     Homo_sapiens
     Mus_musculus
     PhiX
     Saccharomyces_cerevisiae
</code></pre></div>

<p>If you would like us to host a dataset or questions about what is currently available, please <a href="/#get-help">contact us</a>.</p>
<h2 id="storage">Storage</h2>
<p>Farnam has access to a number of GPFS filesystems. <code>/gpfs/ysm</code> is Farnam's primary filesystem where Home, Project, and Scratch60 directories are located. For more details on the different storage spaces, see our <a href="/clusters-at-yale/data/index">Cluster Storage</a> documentation.</p>
<p>You can check your current storage usage &amp; limits by running the <code>getquota</code> command. Your <code>~/project</code> and <code>~/scratch60</code> directories are shortcuts. Get a list of the absolute paths to your directories with the <code>mydirectories</code> command. If you want to share data in your Project or Scratch directory, see the <a href="/clusters-at-yale/data/permissions/">permissions</a> page.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Files stored in <code>scratch60</code> are purged if they are older than 60 days. You will receive an email alert one week before they are deleted.</p>
</div>
<table>
<thead>
<tr>
<th>Partition</th>
<th>Root Directory</th>
<th>Storage</th>
<th>File Count</th>
<th>Backups</th>
</tr>
</thead>
<tbody>
<tr>
<td>home</td>
<td><code>/gpfs/ysm/home</code></td>
<td>125GiB/user</td>
<td>500,000</td>
<td>Yes</td>
</tr>
<tr>
<td>project</td>
<td><code>/gpfs/ysm/project</code></td>
<td>1TiB/group, increase to 4TiB on request</td>
<td>5,000,000</td>
<td>No</td>
</tr>
<tr>
<td>scratch60</td>
<td><code>/gpfs/ysm/scratch60</code></td>
<td>20TiB/group</td>
<td>15,000,000</td>
<td>No</td>
</tr>
</tbody>
</table>
                
                  
                    

<hr>
<div class="md-source-date">
  <small>
    
      Last update: <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">April 28, 2021</span>
    
  </small>
</div>
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        <!--
  Copyright (c) 2016-2020 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->



<!-- Application footer -->
<footer class="md-footer">
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div>
        <a href="https://www.yale.edu" title="Yale" target="_blank">
          <img alt="Yale Logo" src="/img/yale-white.png" height="60" style="padding: 8px; height: 60px;">
	</a>
      </div>
 
      <!-- Accessability/Privacy information -->
      <div class="md-footer-copyright">
        <a href="https://usability.yale.edu/web-accessibility/accessibility-yale">Accessibility at Yale</a>
            
        <a title="Yale Privacy policy" href="http://www.yale.edu/privacy-policy">Privacy policy</a>
          
      </div>

      <!-- Social links -->
      
  <div class="md-footer-social">
    
      
      
        
        
      
      <a href="https://ycrc.yale.edu" target="_blank" rel="noopener" title="ycrc.yale.edu" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19.07 4.93C17.22 3 14.66 1.96 12 2c-2.66-.04-5.21 1-7.06 2.93C3 6.78 1.96 9.34 2 12c-.04 2.66 1 5.21 2.93 7.06C6.78 21 9.34 22.04 12 22c2.66.04 5.21-1 7.06-2.93C21 17.22 22.04 14.66 22 12c.04-2.66-1-5.22-2.93-7.07M17 12v6h-3.5v-5h-3v5H7v-6H5l7-7 7.5 7H17z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="http://help.ycrc.yale.edu/" target="_blank" rel="noopener" title="help.ycrc.yale.edu" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M15.07 11.25l-.9.92C13.45 12.89 13 13.5 13 15h-2v-.5c0-1.11.45-2.11 1.17-2.83l1.24-1.26c.37-.36.59-.86.59-1.41a2 2 0 00-2-2 2 2 0 00-2 2H8a4 4 0 014-4 4 4 0 014 4 3.2 3.2 0 01-.93 2.25M13 19h-2v-2h2M12 2A10 10 0 002 12a10 10 0 0010 10 10 10 0 0010-10c0-5.53-4.5-10-10-10z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="https://research.computing.yale.edu/system-status" target="_blank" rel="noopener" title="research.computing.yale.edu" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M7.5 4A5.5 5.5 0 002 9.5c0 .5.09 1 .22 1.5H6.3l1.27-3.37c.3-.8 1.48-.88 1.86 0L11.5 13l.59-1.42c.13-.33.48-.58.91-.58h8.78c.13-.5.22-1 .22-1.5A5.5 5.5 0 0016.5 4c-1.86 0-3.5.93-4.5 2.34C11 4.93 9.36 4 7.5 4M3 12.5a1 1 0 00-1 1 1 1 0 001 1h2.44L11 20c1 .9 1 .9 2 0l5.56-5.5H21a1 1 0 001-1 1 1 0 00-1-1h-7.6l-.93 2.3c-.4 1.01-1.55.87-1.92.03L8.5 9.5l-.96 2.33c-.15.38-.49.67-.94.67H3z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="https://github.com/ycrc" target="_blank" rel="noopener" title="github.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="https://twitter.com/yalecrc" target="_blank" rel="noopener" title="twitter.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
      </a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../../assets/javascripts/vendor.d1f5a259.min.js"></script>
      <script src="../../../assets/javascripts/bundle.d5fec882.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents"}</script>
      
      <script>
        app = initialize({
          base: "../../..",
          features: ["tabs", "instant"],
          search: Object.assign({
            worker: "../../../assets/javascripts/worker/search.fae956e7.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
        <script src="../../../js/extra.js"></script>
      
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
      
    
  </body>
</html>