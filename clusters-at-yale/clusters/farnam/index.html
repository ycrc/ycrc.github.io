
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Yale Center for Research Computing Documentation Pages">
      
      
        <meta name="author" content="YCRC">
      
      
        <link rel="canonical" href="https://docs.ycrc.yale.edu/clusters-at-yale/clusters/farnam/">
      
      <link rel="icon" href="../../../assets/images/favicon.ico">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.2.9">
    
    
      
        <title>Farnam - Yale Center for Research Computing</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.120efc48.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.9647289d.min.css">
        
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  


  <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-QM84KPHSPL"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){this.value&&gtag("event","search",{search_term:this.value})}),"undefined"!=typeof location$&&location$.subscribe(function(e){gtag("config","G-QM84KPHSPL",{page_path:e.pathname})})})</script>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-QM84KPHSPL"></script>


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#farnam" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      <!--
  Copyright (c) 2016-2020 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->



  


<header class="md-header md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Yale Center for Research Computing" class="md-header__button md-logo" aria-label="Yale Center for Research Computing" data-md-component="logo">
      
  <img src="../../../img/manual.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>

    <!-- Header title -->
    <div class="md-header__title" data-md-component="header-title">
        <div class="md-header__ellipsis">
            <span class="md-header__topic md-ellipsis branding">
              <a href="../../.." >
                  <kern style="letter-spacing:-0.09em">Y</kern>ale <em>Center for Research Computing</em>
              </a>
            </span>
          <span class="md-header__topic md-ellipsis">
            
              Farnam
            
          </span>
        </div>

    </div>

    

    

    <!-- Search Interface -->
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    


    <!-- Repository containing source -->
    
      <div class="md-header-nav__source">
        <a href="https://github.com/ycrc/ycrc.github.io/tree/src" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    github/ycrc/docs
  </div>
</a>
      </div>
    

  </nav>
  
    
      <!--
  Copyright (c) 2016-2019 Martin Donath <martin.donath@squidfunk.com>
  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:
  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.
  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->

<!-- Determine class according to level -->


  



<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      <li class="md-tabs__item"><a class="md-tabs__link" href="http://research.computing.yale.edu/about">About</a></li>
      <li class="md-tabs__item"><a class="md-tabs__link" href="http://research.computing.yale.edu/news-events">News & Events</a></li>
      <li class="md-tabs__item"><a class="md-tabs__link" href="http://research.computing.yale.edu/research">Research</a></li>
      <li class="md-tabs__item"><a class="md-tabs__link" href="http://research.computing.yale.edu/services">Services</a></li>
      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link md-tabs__link--active">
        Support
      </a>
    </li>
  

      
      <li class="md-tabs__item"><a class="md-tabs__link" href="http://research.computing.yale.edu/training">Training</a></li>
      <li class="md-tabs__item status"><a class="md-tabs__link" href="http://research.computing.yale.edu/system-status"><span id='status-icon'></span> System Status</a></li>
    </ul>
 </div>
</nav>

<div id="system-status-message"></div>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Yale Center for Research Computing" class="md-nav__button md-logo" aria-label="Yale Center for Research Computing" data-md-component="logo">
      
  <img src="../../../img/manual.svg" alt="logo">

    </a>
    Yale Center for Research Computing
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/ycrc/ycrc.github.io/tree/src" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    github/ycrc/docs
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1" type="checkbox" id="__nav_1" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_1">
          Support
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Support" data-md-level="1">
        <label class="md-nav__title" for="__nav_1">
          <span class="md-nav__icon md-icon"></span>
          Support
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../news/" class="md-nav__link">
        News
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1_3" type="checkbox" id="__nav_1_3" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_1_3">
          Cluster Computing at Yale
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Cluster Computing at Yale" data-md-level="2">
        <label class="md-nav__title" for="__nav_1_3">
          <span class="md-nav__icon md-icon"></span>
          Cluster Computing at Yale
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../" class="md-nav__link">
        Getting Started
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="https://research.computing.yale.edu/account-request" class="md-nav__link">
        Request an Account
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../help-requests/" class="md-nav__link">
        Help Requests
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../troubleshoot/" class="md-nav__link">
        Troubleshoot Login
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1_3_5" type="checkbox" id="__nav_1_3_5" >
      
      
      
      
        <label class="md-nav__link" for="__nav_1_3_5">
          Access the Clusters
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Access the Clusters" data-md-level="3">
        <label class="md-nav__title" for="__nav_1_3_5">
          <span class="md-nav__icon md-icon"></span>
          Access the Clusters
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../access/accounts/" class="md-nav__link">
        About Cluster Accounts
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../access/" class="md-nav__link">
        Log on to the Clusters
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../access/ood/" class="md-nav__link">
        Open OnDemand
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../access/vpn/" class="md-nav__link">
        Access from Off Campus (VPN)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../access/x11/" class="md-nav__link">
        Graphical Interfaces (X11)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../access/vnc/" class="md-nav__link">
        VNC
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../access/mfa/" class="md-nav__link">
        Multi-factor Authentication
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../access/advanced-config/" class="md-nav__link">
        Advanced SSH Configuration
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1_3_6" type="checkbox" id="__nav_1_3_6" >
      
      
      
      
        <label class="md-nav__link" for="__nav_1_3_6">
          Applications & Software
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Applications & Software" data-md-level="3">
        <label class="md-nav__title" for="__nav_1_3_6">
          <span class="md-nav__icon md-icon"></span>
          Applications & Software
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../applications/" class="md-nav__link">
        Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../applications/compile/" class="md-nav__link">
        Build Software
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../applications/modules/" class="md-nav__link">
        Software Modules
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../applications/toolchains/" class="md-nav__link">
        Module Toolchains
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../applications/lifecycle/" class="md-nav__link">
        Module Lifecycle
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1_3_7" type="checkbox" id="__nav_1_3_7" >
      
      
      
      
        <label class="md-nav__link" for="__nav_1_3_7">
          Software Guides
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Software Guides" data-md-level="3">
        <label class="md-nav__title" for="__nav_1_3_7">
          <span class="md-nav__icon md-icon"></span>
          Software Guides
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/" class="md-nav__link">
        Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/cesm/" class="md-nav__link">
        CESM/CAM
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/clustershell/" class="md-nav__link">
        ClusterShell
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/cmd-line-args/" class="md-nav__link">
        Command-line Argements
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/comsol/" class="md-nav__link">
        COMSOL
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/conda/" class="md-nav__link">
        Conda
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/cryoem/" class="md-nav__link">
        Cryo-EM on Farnam
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/gaussian/" class="md-nav__link">
        Gaussian
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/github/" class="md-nav__link">
        GitHub
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/github_pages/" class="md-nav__link">
        GitHub Pages
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/gpus-cuda/" class="md-nav__link">
        GPUs and CUDA
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/jupyter/" class="md-nav__link">
        Jupyter Notebooks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/matlab/" class="md-nav__link">
        MATLAB
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/mathematica/" class="md-nav__link">
        Mathematica
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/mpi4py/" class="md-nav__link">
        mpi4py
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/mysql/" class="md-nav__link">
        Mysql
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/namd/" class="md-nav__link">
        NAMD
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/python/" class="md-nav__link">
        Python
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/handson-ml2/" class="md-nav__link">
        Python+TensorFlow Example
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/parallel/" class="md-nav__link">
        Parallel
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/r/" class="md-nav__link">
        R
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/rclone/" class="md-nav__link">
        Rclone
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/singularity/" class="md-nav__link">
        Singularity
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/tmux/" class="md-nav__link">
        tmux
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/vasp/" class="md-nav__link">
        VASP
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/xvfb/" class="md-nav__link">
        XVFB
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1_3_8" type="checkbox" id="__nav_1_3_8" >
      
      
      
      
        <label class="md-nav__link" for="__nav_1_3_8">
          Job Scheduling
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Job Scheduling" data-md-level="3">
        <label class="md-nav__title" for="__nav_1_3_8">
          <span class="md-nav__icon md-icon"></span>
          Job Scheduling
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../job-scheduling/" class="md-nav__link">
        Run Jobs with Slurm
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../job-scheduling/resource-requests/" class="md-nav__link">
        Request Compute Resources
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../job-scheduling/common-job-failures/" class="md-nav__link">
        Common Job Failures
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../job-scheduling/resource-usage/" class="md-nav__link">
        Monitor CPU and Memory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../job-scheduling/fairshare/" class="md-nav__link">
        Factors Affecting Scheduling
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../job-scheduling/scavenge/" class="md-nav__link">
        Scavenge Partition
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../job-scheduling/dependency/" class="md-nav__link">
        Jobs with Dependencies
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../job-scheduling/scrontab/" class="md-nav__link">
        Recurring Jobs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../job-scheduling/mpi/" class="md-nav__link">
        MPI Partition
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../job-scheduling/dsq/" class="md-nav__link">
        Submit Job Arrays with dSQ
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../job-scheduling/slurm-examples/" class="md-nav__link">
        Submission Script Examples
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1_3_9" type="checkbox" id="__nav_1_3_9" >
      
      
      
      
        <label class="md-nav__link" for="__nav_1_3_9">
          Data & Storage
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Data & Storage" data-md-level="3">
        <label class="md-nav__title" for="__nav_1_3_9">
          <span class="md-nav__icon md-icon"></span>
          Data & Storage
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../data/" class="md-nav__link">
        Cluster Storage
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../data/transfer/" class="md-nav__link">
        Transfer Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../data/staging/" class="md-nav__link">
        Stage Data for Compute Jobs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../data/globus/" class="md-nav__link">
        Large Transfers with Globus
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../data/permissions/" class="md-nav__link">
        Manage Permissions for Sharing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../data/archived-sequencing/" class="md-nav__link">
        Ruddle Archived Sequence Data
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1_3_10" type="checkbox" id="__nav_1_3_10" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_1_3_10">
          The Clusters
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="The Clusters" data-md-level="3">
        <label class="md-nav__title" for="__nav_1_3_10">
          <span class="md-nav__icon md-icon"></span>
          The Clusters
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        HPC Resources
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../grace/" class="md-nav__link">
        Grace
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Farnam
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Farnam
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#access-the-cluster" class="md-nav__link">
    Access the Cluster
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#system-status-and-monitoring" class="md-nav__link">
    System Status and Monitoring
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#partitions-and-hardware" class="md-nav__link">
    Partitions and Hardware
  </a>
  
    <nav class="md-nav" aria-label="Partitions and Hardware">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#public-partitions" class="md-nav__link">
    Public Partitions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#private-partitions" class="md-nav__link">
    Private Partitions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#public-datasets" class="md-nav__link">
    Public Datasets
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#storage" class="md-nav__link">
    Storage
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ruddle/" class="md-nav__link">
        Ruddle
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../milgram/" class="md-nav__link">
        Milgram
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1_4" type="checkbox" id="__nav_1_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_1_4">
          Research Data
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Research Data" data-md-level="2">
        <label class="md-nav__title" for="__nav_1_4">
          <span class="md-nav__icon md-icon"></span>
          Research Data
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../data/" class="md-nav__link">
        Data Storage Options
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../data/archive/" class="md-nav__link">
        Archive Your Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../data/google-drive/" class="md-nav__link">
        Google Drive
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../data/external/" class="md-nav__link">
        Serving Research Data Externally
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../user-group/" class="md-nav__link">
        YCRC User Group
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../national-hpcs/" class="md-nav__link">
        National HPCs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../online-tutorials/" class="md-nav__link">
        Online Tutorials
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../glossary/" class="md-nav__link">
        Glossary
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#access-the-cluster" class="md-nav__link">
    Access the Cluster
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#system-status-and-monitoring" class="md-nav__link">
    System Status and Monitoring
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#partitions-and-hardware" class="md-nav__link">
    Partitions and Hardware
  </a>
  
    <nav class="md-nav" aria-label="Partitions and Hardware">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#public-partitions" class="md-nav__link">
    Public Partitions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#private-partitions" class="md-nav__link">
    Private Partitions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#public-datasets" class="md-nav__link">
    Public Datasets
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#storage" class="md-nav__link">
    Storage
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
  <a href="https://github.com/ycrc/ycrc.github.io/tree/src/docs/clusters-at-yale/clusters/farnam.md" title="Edit this page" class="md-content__button md-icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
  </a>



<h1 id="farnam">Farnam</h1>
<p><img alt="Louise" class="cluster-portrait" src="/img/Louise-Whitman-Farnam.jpg" /></p>
<p>Farnam is a shared-use resource for the <a href="https://medicine.yale.edu">Yale School of Medicine</a> (YSM). It consists of a variety of compute nodes networked over ethernet and mounts several shared filesystems.</p>
<p>The Farnam Cluster is named for <a href="http://archives.yalealumnimagazine.com/issues/2006_09/old_yale.html">Louise Whitman Farnam</a>, the first woman to graduate from the Yale School of Medicine, class of 1916.</p>
<hr />
<h2 id="access-the-cluster">Access the Cluster</h2>
<p>Once you have <a href="https://research.computing.yale.edu/support/hpc/account-request">an account</a>, the cluster can be accessed <a href="/clusters-at-yale/access">via ssh</a> or through the <a href="/clusters-at-yale/access/ood/">Open OnDemand web portal</a>.</p>
<h2 id="system-status-and-monitoring">System Status and Monitoring</h2>
<p>For system status messages and the schedule for upcoming maintenance, please see the <a href="https://research.computing.yale.edu/support/hpc/system-status">system status page</a>. For a current node-level view of job activity, see the <a href="http://cluster.ycrc.yale.edu/farnam/">cluster monitor page (VPN only)</a>.</p>
<h2 id="partitions-and-hardware">Partitions and Hardware</h2>
<p>Farnam is made up of several kinds of compute nodes. We group them into  (sometimes overlapping)  <a href="/clusters-at-yale/job-scheduling">Slurm partitions</a> meant to serve different purposes. By combining the <code>--partition</code> and <a href="/clusters-at-yale/job-scheduling/resource-requests#features-and-constraints"><code>--constraint</code></a> Slurm options you can more finely control what nodes your jobs can run on.</p>
<div class="admonition warning">
<p class="admonition-title">Job Submission Rate Limits</p>
<p>Job submissions are limited to <strong>200 jobs per hour</strong>. See the Rate Limits section in the <a href="/clusters-at-yale/job-scheduling/common-job-failures/#rate-limits">Common Job Failures</a> page for more info.</p>
</div>
<h3 id="public-partitions">Public Partitions</h3>
<p>See each tab below for more information about the available common use partitions.</p>
<div class="tabbed-set" data-tabs="1:8"><input checked="checked" id="__tabbed_1_1" name="__tabbed_1" type="radio" /><label for="__tabbed_1_1">general</label><div class="tabbed-content">
<p>Use the general partition for most batch jobs. This is the default if you don't specify one with <code>--partition</code>.</p>
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the general partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>30-00:00:00</code></td>
</tr>
<tr>
<td>Maximum CPUs per group</td>
<td><code>400</code></td>
</tr>
<tr>
<td>Maximum memory per group</td>
<td><code>2.50T</code></td>
</tr>
<tr>
<td>Maximum CPUs per user</td>
<td><code>200</code></td>
</tr>
<tr>
<td>Maximum memory per user</td>
<td><code>1280G</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>18</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>cascadelake, avx2, avx512, 6240, nogpu, standard, common, bigtmp</td>
</tr>
<tr>
<td>82</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>117</td>
<td>haswell, avx2, E5-2660_v3, nogpu, standard, oldest, common</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_1_2" name="__tabbed_1" type="radio" /><label for="__tabbed_1_2">interactive</label><div class="tabbed-content">
<p>Use the interactive partition to jobs with which you need ongoing interaction. For example, exploratory analyses or debugging compilation.</p>
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=06:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the interactive partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>1-00:00:00</code></td>
</tr>
<tr>
<td>Maximum CPUs per user</td>
<td><code>20</code></td>
</tr>
<tr>
<td>Maximum memory per user</td>
<td><code>256G</code></td>
</tr>
<tr>
<td>Maximum running jobs per user</td>
<td><code>2</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>cascadelake, avx2, avx512, 6240, nogpu, standard, pi, bigtmp</td>
</tr>
<tr>
<td>18</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>cascadelake, avx2, avx512, 6240, nogpu, standard, common, bigtmp</td>
</tr>
<tr>
<td>94</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>117</td>
<td>haswell, avx2, E5-2660_v3, nogpu, standard, oldest, common</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_1_3" name="__tabbed_1" type="radio" /><label for="__tabbed_1_3">transfer</label><div class="tabbed-content">
<p>Use the transfer partition to stage data for your jobs to and from <a href="/clusters-at-yale/data/#staging-data">cluster storage</a>.</p>
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the transfer partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>1-00:00:00</code></td>
</tr>
<tr>
<td>Maximum running jobs per user</td>
<td><code>2</code></td>
</tr>
<tr>
<td>Maximum CPUs per job</td>
<td><code>1</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>117</td>
<td>haswell, avx2, E5-2660_v3, nogpu, standard, oldest, common</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_1_4" name="__tabbed_1" type="radio" /><label for="__tabbed_1_4">gpu</label><div class="tabbed-content">
<p>Use the gpu partition for jobs that make use of GPUs. You must <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus">request GPUs explicitly</a> with the <code>--gpus</code> option in order to use them. For example, <code>--gpus=gtx1080ti:2</code> would request 2 GeForce GTX 1080Ti GPUs per node.</p>
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the gpu partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>2-00:00:00</code></td>
</tr>
<tr>
<td>Maximum CPUs per user</td>
<td><code>32</code></td>
</tr>
<tr>
<td>Maximum GPUs per user</td>
<td><code>12</code></td>
</tr>
<tr>
<td>Maximum memory per user</td>
<td><code>256G</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>370</td>
<td>a100</td>
<td>4</td>
<td>40</td>
<td>cascadelake, avx2, avx512, 6240, doubleprecision, common, bigtmp, a100</td>
</tr>
<tr>
<td>6</td>
<td>5222</td>
<td>8</td>
<td>181</td>
<td>rtx5000</td>
<td>4</td>
<td>16</td>
<td>cascadelake, avx2, avx512, 5222, doubleprecision, common, bigtmp, rtx5000</td>
</tr>
<tr>
<td>4</td>
<td>5222</td>
<td>8</td>
<td>181</td>
<td>rtx3090</td>
<td>4</td>
<td>24</td>
<td>cascadelake, avx2, avx512, 5222, doubleprecision, common, bigtmp, rtx3090</td>
</tr>
<tr>
<td>8</td>
<td>E5-2637_v4</td>
<td>8</td>
<td>119</td>
<td>gtx1080ti</td>
<td>4</td>
<td>11</td>
<td>broadwell, avx2, E5-2637_v4, singleprecision, common, gtx1080ti</td>
</tr>
<tr>
<td>2</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td>k80</td>
<td>4</td>
<td>12</td>
<td>haswell, avx2, E5-2660_v3, doubleprecision, common, k80</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_1_5" name="__tabbed_1" type="radio" /><label for="__tabbed_1_5">gpu_devel</label><div class="tabbed-content">
<p>Use the gpu_devel partition to debug jobs that make use of GPUs, or to develop GPU-enabled code.</p>
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=00:10:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the gpu_devel partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>02:00:00</code></td>
</tr>
<tr>
<td>Maximum submitted jobs per user</td>
<td><code>1</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>E5-2623_v4</td>
<td>8</td>
<td>57</td>
<td>gtx1080ti</td>
<td>4</td>
<td>11</td>
<td>broadwell, avx2, E5-2623_v4, singleprecision, common, gtx1080ti</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_1_6" name="__tabbed_1" type="radio" /><label for="__tabbed_1_6">bigmem</label><div class="tabbed-content">
<p>Use the bigmem partition for jobs that have memory requirements other partitions can't handle.</p>
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the bigmem partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>3-00:00:00</code></td>
</tr>
<tr>
<td>Maximum CPUs per user</td>
<td><code>32</code></td>
</tr>
<tr>
<td>Maximum memory per user</td>
<td><code>1505G</code></td>
</tr>
<tr>
<td>Maximum running jobs per user</td>
<td><code>2</code></td>
</tr>
<tr>
<td>Maximum nodes per job</td>
<td><code>1</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>3</td>
<td>6240</td>
<td>36</td>
<td>1505</td>
<td>cascadelake, avx2, avx512, 6240, nogpu, common, bigtmp</td>
</tr>
<tr>
<td>2</td>
<td>6234</td>
<td>16</td>
<td>1505</td>
<td>cascadelake, avx2, avx512, 6234, nogpu, common, bigtmp</td>
</tr>
<tr>
<td>2</td>
<td>E7-4809_v3</td>
<td>32</td>
<td>1505</td>
<td>haswell, avx2, E7-4809_v3, nogpu, common</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_1_7" name="__tabbed_1" type="radio" /><label for="__tabbed_1_7">scavenge</label><div class="tabbed-content">
<p>Use the scavenge partition to run preemptable jobs on more resources than normally allowed. For more information about scavenge, see the <a href="/clusters-at-yale/job-scheduling/scavenge">Scavenge documentation</a>.</p>
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the scavenge partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>7-00:00:00</code></td>
</tr>
<tr>
<td>Maximum CPUs per user</td>
<td><code>800</code></td>
</tr>
<tr>
<td>Maximum memory per user</td>
<td><code>5T</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>7</td>
<td>8268</td>
<td>48</td>
<td>356</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx512, 8268, nogpu, standard, bigtmp, pi</td>
</tr>
<tr>
<td>6</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx2, avx512, 6240, nogpu, standard, pi, bigtmp</td>
</tr>
<tr>
<td>18</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx2, avx512, 6240, nogpu, standard, common, bigtmp</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>370</td>
<td>rtx3090</td>
<td>8</td>
<td>24</td>
<td>cascadelake, avx2, avx512, 6240, doubleprecision, pi, bigtmp, rtx3090</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>370</td>
<td>a100</td>
<td>4</td>
<td>40</td>
<td>cascadelake, avx2, avx512, 6240, doubleprecision, common, bigtmp, a100</td>
</tr>
<tr>
<td>1</td>
<td>6226r</td>
<td>32</td>
<td>181</td>
<td>rtx3090</td>
<td>4</td>
<td>24</td>
<td>cascadelake, avx2, avx512, 6226r, doubleprecision, pi, rtx3090</td>
</tr>
<tr>
<td>4</td>
<td>6240</td>
<td>36</td>
<td>370</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx2, avx512, 6240, nogpu, pi, bigtmp</td>
</tr>
<tr>
<td>6</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx2, avx512, 6240, nogpu, pi, bigtmp</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>1505</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx2, avx512, 6240, nogpu, pi, bigtmp</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>rtx2080ti</td>
<td>4</td>
<td>11</td>
<td>cascadelake, avx2, avx512, 6240, singleprecision, pi, bigtmp, rtx2080ti</td>
</tr>
<tr>
<td>1</td>
<td>6248r</td>
<td>48</td>
<td>370</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx2, avx512, 6248r, nogpu, pi, bigtmp</td>
</tr>
<tr>
<td>3</td>
<td>6240</td>
<td>36</td>
<td>1505</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx2, avx512, 6240, nogpu, common, bigtmp</td>
</tr>
<tr>
<td>8</td>
<td>5222</td>
<td>8</td>
<td>181</td>
<td>rtx5000</td>
<td>4</td>
<td>16</td>
<td>cascadelake, avx2, avx512, 5222, doubleprecision, pi, bigtmp, rtx5000</td>
</tr>
<tr>
<td>1</td>
<td>6242</td>
<td>32</td>
<td>999</td>
<td>rtx8000</td>
<td>2</td>
<td>48</td>
<td>cascadelake, avx2, avx512, 6242, doubleprecision, pi, bigtmp, rtx8000</td>
</tr>
<tr>
<td>3</td>
<td>8268</td>
<td>48</td>
<td>370</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx2, avx512, 8268, nogpu, pi, bigtmp</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>370</td>
<td>v100</td>
<td>4</td>
<td>16</td>
<td>cascadelake, avx2, avx512, 6240, pi, v100</td>
</tr>
<tr>
<td>4</td>
<td>5222</td>
<td>8</td>
<td>181</td>
<td>rtx3090</td>
<td>4</td>
<td>24</td>
<td>cascadelake, avx2, avx512, 5222, doubleprecision, common, bigtmp, rtx3090</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>364</td>
<td>a100</td>
<td>4</td>
<td>40</td>
<td>cascadelake, avx2, avx512, 6240, doubleprecision, pi, bigtmp, a100</td>
</tr>
<tr>
<td>1</td>
<td>5222</td>
<td>36</td>
<td>750</td>
<td>a100</td>
<td>4</td>
<td>40</td>
<td>cascadelake, avx2, avx512, 5222, doubleprecision, bigtmp, a100</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>a100</td>
<td>4</td>
<td>40</td>
<td>cascadelake, avx2, avx512, 6240, doubleprecision, pi, a100, bigtmp</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>rtx3090</td>
<td>4</td>
<td>24</td>
<td>cascadelake, avx2, avx512, 6240, doubleprecision, pi, bigtmp, rtx3090</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>rtx3090</td>
<td>8</td>
<td>24</td>
<td>cascadelake, avx2, avx512, 6240, doubleprecision, pi, bigtmp, rtx3090</td>
</tr>
<tr>
<td>4</td>
<td>6240</td>
<td>36</td>
<td>748</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx2, avx512, 6240, nogpu, pi, bigtmp</td>
</tr>
<tr>
<td>2</td>
<td>6132</td>
<td>28</td>
<td>181</td>
<td></td>
<td></td>
<td></td>
<td>skylake, avx2, avx512, 6132, nogpu, standard, pi, bigtmp</td>
</tr>
<tr>
<td>1</td>
<td>6132</td>
<td>28</td>
<td>749</td>
<td></td>
<td></td>
<td></td>
<td>skylake, avx2, avx512, 6132, nogpu, pi, bigtmp</td>
</tr>
<tr>
<td>2</td>
<td>5122</td>
<td>8</td>
<td>181</td>
<td>rtx2080</td>
<td>4</td>
<td>8</td>
<td>skylake, avx2, avx512, 5122, singleprecision, pi, rtx2080</td>
</tr>
<tr>
<td>38</td>
<td>E5-2680_v4</td>
<td>28</td>
<td>245</td>
<td></td>
<td></td>
<td></td>
<td>broadwell, avx2, E5-2680_v4, nogpu, standard, pi</td>
</tr>
<tr>
<td>3</td>
<td>E5-2680_v4</td>
<td>28</td>
<td>245</td>
<td>p100</td>
<td>2</td>
<td>16</td>
<td>broadwell, avx2, E5-2680_v4, doubleprecision, pi, p100</td>
</tr>
<tr>
<td>1</td>
<td>E7-4820_v4</td>
<td>40</td>
<td>1505</td>
<td></td>
<td></td>
<td></td>
<td>broadwell, avx2, E7-4820_v4, nogpu, pi</td>
</tr>
<tr>
<td>1</td>
<td>E5-2623_v4</td>
<td>8</td>
<td>57</td>
<td>gtx1080ti</td>
<td>4</td>
<td>11</td>
<td>broadwell, avx2, E5-2623_v4, singleprecision, common, gtx1080ti</td>
</tr>
<tr>
<td>10</td>
<td>E5-2637_v4</td>
<td>8</td>
<td>119</td>
<td>gtx1080ti</td>
<td>4</td>
<td>11</td>
<td>broadwell, avx2, E5-2637_v4, singleprecision, common, gtx1080ti</td>
</tr>
<tr>
<td>11</td>
<td>E5-2637_v4</td>
<td>8</td>
<td>119</td>
<td>gtx1080ti</td>
<td>4</td>
<td>11</td>
<td>broadwell, avx2, E5-2637_v4, singleprecision, pi, gtx1080ti</td>
</tr>
<tr>
<td>1</td>
<td>E5-2637_v4</td>
<td>8</td>
<td>119</td>
<td>titanv</td>
<td>4</td>
<td>12</td>
<td>broadwell, avx2, E5-2637_v4, doubleprecision, pi, bigtmp, titanv</td>
</tr>
<tr>
<td>96</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>117</td>
<td></td>
<td></td>
<td></td>
<td>haswell, avx2, E5-2660_v3, nogpu, standard, oldest, common</td>
</tr>
<tr>
<td>18</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td></td>
<td></td>
<td></td>
<td>haswell, avx2, E5-2660_v3, nogpu, standard, oldest, pi</td>
</tr>
<tr>
<td>2</td>
<td>E7-4809_v3</td>
<td>32</td>
<td>1505</td>
<td></td>
<td></td>
<td></td>
<td>haswell, avx2, E7-4809_v3, nogpu, common</td>
</tr>
<tr>
<td>2</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td>k80</td>
<td>4</td>
<td>12</td>
<td>haswell, avx2, E5-2660_v3, doubleprecision, common, k80</td>
</tr>
<tr>
<td>3</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td>k80</td>
<td>4</td>
<td>12</td>
<td>haswell, avx2, E5-2660_v3, doubleprecision, pi, k80</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_1_8" name="__tabbed_1" type="radio" /><label for="__tabbed_1_8">scavenge_gpu</label><div class="tabbed-content">
<p>Use the scavenge_gpu partition to run preemptable jobs on more GPU resources than normally allowed. For more information about scavenge, see the <a href="/clusters-at-yale/job-scheduling/scavenge">Scavenge documentation</a>.</p>
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the scavenge_gpu partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>2-00:00:00</code></td>
</tr>
<tr>
<td>Maximum GPUs per user</td>
<td><code>64</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>rtx2080ti</td>
<td>4</td>
<td>11</td>
<td>cascadelake, avx2, avx512, 6240, singleprecision, pi, bigtmp, rtx2080ti</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>370</td>
<td>rtx3090</td>
<td>8</td>
<td>24</td>
<td>cascadelake, avx2, avx512, 6240, doubleprecision, pi, bigtmp, rtx3090</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>370</td>
<td>a100</td>
<td>4</td>
<td>40</td>
<td>cascadelake, avx2, avx512, 6240, doubleprecision, common, bigtmp, a100</td>
</tr>
<tr>
<td>1</td>
<td>6226r</td>
<td>32</td>
<td>181</td>
<td>rtx3090</td>
<td>4</td>
<td>24</td>
<td>cascadelake, avx2, avx512, 6226r, doubleprecision, pi, rtx3090</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>370</td>
<td>v100</td>
<td>4</td>
<td>16</td>
<td>cascadelake, avx2, avx512, 6240, pi, v100</td>
</tr>
<tr>
<td>4</td>
<td>5222</td>
<td>8</td>
<td>181</td>
<td>rtx3090</td>
<td>4</td>
<td>24</td>
<td>cascadelake, avx2, avx512, 5222, doubleprecision, common, bigtmp, rtx3090</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>364</td>
<td>a100</td>
<td>4</td>
<td>40</td>
<td>cascadelake, avx2, avx512, 6240, doubleprecision, pi, bigtmp, a100</td>
</tr>
<tr>
<td>1</td>
<td>5222</td>
<td>36</td>
<td>750</td>
<td>a100</td>
<td>4</td>
<td>40</td>
<td>cascadelake, avx2, avx512, 5222, doubleprecision, bigtmp, a100</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>a100</td>
<td>4</td>
<td>40</td>
<td>cascadelake, avx2, avx512, 6240, doubleprecision, pi, a100, bigtmp</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>rtx3090</td>
<td>4</td>
<td>24</td>
<td>cascadelake, avx2, avx512, 6240, doubleprecision, pi, bigtmp, rtx3090</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>rtx3090</td>
<td>8</td>
<td>24</td>
<td>cascadelake, avx2, avx512, 6240, doubleprecision, pi, bigtmp, rtx3090</td>
</tr>
<tr>
<td>2</td>
<td>5122</td>
<td>8</td>
<td>181</td>
<td>rtx2080</td>
<td>4</td>
<td>8</td>
<td>skylake, avx2, avx512, 5122, singleprecision, pi, rtx2080</td>
</tr>
<tr>
<td>3</td>
<td>E5-2680_v4</td>
<td>28</td>
<td>245</td>
<td>p100</td>
<td>2</td>
<td>16</td>
<td>broadwell, avx2, E5-2680_v4, doubleprecision, pi, p100</td>
</tr>
<tr>
<td>1</td>
<td>E5-2623_v4</td>
<td>8</td>
<td>57</td>
<td>gtx1080ti</td>
<td>4</td>
<td>11</td>
<td>broadwell, avx2, E5-2623_v4, singleprecision, common, gtx1080ti</td>
</tr>
<tr>
<td>10</td>
<td>E5-2637_v4</td>
<td>8</td>
<td>119</td>
<td>gtx1080ti</td>
<td>4</td>
<td>11</td>
<td>broadwell, avx2, E5-2637_v4, singleprecision, common, gtx1080ti</td>
</tr>
<tr>
<td>11</td>
<td>E5-2637_v4</td>
<td>8</td>
<td>119</td>
<td>gtx1080ti</td>
<td>4</td>
<td>11</td>
<td>broadwell, avx2, E5-2637_v4, singleprecision, pi, gtx1080ti</td>
</tr>
<tr>
<td>1</td>
<td>E5-2637_v4</td>
<td>8</td>
<td>119</td>
<td>titanv</td>
<td>4</td>
<td>12</td>
<td>broadwell, avx2, E5-2637_v4, doubleprecision, pi, bigtmp, titanv</td>
</tr>
<tr>
<td>2</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td>k80</td>
<td>4</td>
<td>12</td>
<td>haswell, avx2, E5-2660_v3, doubleprecision, common, k80</td>
</tr>
<tr>
<td>3</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td>k80</td>
<td>4</td>
<td>12</td>
<td>haswell, avx2, E5-2660_v3, doubleprecision, pi, k80</td>
</tr>
</tbody>
</table>
</div>
</div>
<h3 id="private-partitions">Private Partitions</h3>
<p>With few exceptions, jobs submitted to private partitions are not considered when calculating your group's <a href="/clusters-at-yale/job-scheduling/fairshare/">Fairshare</a>. Your group can purchase additional hardware for private use, which we will make available as a <code>pi_groupname</code> partition. These nodes are purchased by you, but supported and administered by us. After vendor support expires, we retire compute nodes. Compute nodes can range from $10K to upwards of $50K depending on your requirements. If you are interested in purchasing nodes for your group, please <a href="/#get-help">contact us</a>.</p>
<details class="summary">
<summary>PI Partitions (click to expand)</summary>
<div class="tabbed-set" data-tabs="2:34"><input checked="checked" id="__tabbed_2_1" name="__tabbed_2" type="radio" /><label for="__tabbed_2_1">pi_breaker</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_breaker partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>24</td>
<td>E5-2680_v4</td>
<td>28</td>
<td>245</td>
<td>broadwell, avx2, E5-2680_v4, nogpu, standard, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_2" name="__tabbed_2" type="radio" /><label for="__tabbed_2_2">pi_bunick</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_bunick partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>364</td>
<td>a100</td>
<td>4</td>
<td>40</td>
<td>cascadelake, avx2, avx512, 6240, doubleprecision, pi, bigtmp, a100</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_3" name="__tabbed_2" type="radio" /><label for="__tabbed_2_3">pi_butterwick</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_butterwick partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>a100</td>
<td>4</td>
<td>40</td>
<td>cascadelake, avx2, avx512, 6240, doubleprecision, pi, a100, bigtmp</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_4" name="__tabbed_2" type="radio" /><label for="__tabbed_2_4">pi_chenlab</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_chenlab partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>8268</td>
<td>48</td>
<td>370</td>
<td>cascadelake, avx2, avx512, 8268, nogpu, pi, bigtmp</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_5" name="__tabbed_2" type="radio" /><label for="__tabbed_2_5">pi_cryo_realtime</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_cryo_realtime partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
<tr>
<td>Maximum GPUs per user</td>
<td><code>12</code></td>
</tr>
<tr>
<td>Maximum running jobs per user</td>
<td><code>2</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>E5-2637_v4</td>
<td>8</td>
<td>119</td>
<td>gtx1080ti</td>
<td>4</td>
<td>11</td>
<td>broadwell, avx2, E5-2637_v4, singleprecision, pi, gtx1080ti</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_6" name="__tabbed_2" type="radio" /><label for="__tabbed_2_6">pi_cryoem</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_cryoem partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>365-00:00:00</code></td>
</tr>
<tr>
<td>Maximum GPUs per user</td>
<td><code>12</code></td>
</tr>
<tr>
<td>Maximum running jobs per user</td>
<td><code>2</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>9</td>
<td>E5-2637_v4</td>
<td>8</td>
<td>119</td>
<td>gtx1080ti</td>
<td>4</td>
<td>11</td>
<td>broadwell, avx2, E5-2637_v4, singleprecision, pi, gtx1080ti</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_7" name="__tabbed_2" type="radio" /><label for="__tabbed_2_7">pi_deng</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_deng partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>E5-2680_v4</td>
<td>28</td>
<td>245</td>
<td>p100</td>
<td>2</td>
<td>16</td>
<td>broadwell, avx2, E5-2680_v4, doubleprecision, pi, p100</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_8" name="__tabbed_2" type="radio" /><label for="__tabbed_2_8">pi_dewan</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_dewan partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>cascadelake, avx2, avx512, 6240, nogpu, pi, bigtmp</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_9" name="__tabbed_2" type="radio" /><label for="__tabbed_2_9">pi_dijk</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_dijk partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>370</td>
<td>rtx3090</td>
<td>8</td>
<td>24</td>
<td>cascadelake, avx2, avx512, 6240, doubleprecision, pi, bigtmp, rtx3090</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_10" name="__tabbed_2" type="radio" /><label for="__tabbed_2_10">pi_dunn</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_dunn partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>cascadelake, avx2, avx512, 6240, nogpu, standard, pi, bigtmp</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_11" name="__tabbed_2" type="radio" /><label for="__tabbed_2_11">pi_edwards</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_edwards partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>cascadelake, avx2, avx512, 6240, nogpu, standard, pi, bigtmp</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_12" name="__tabbed_2" type="radio" /><label for="__tabbed_2_12">pi_falcone</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_falcone partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>1505</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx2, avx512, 6240, nogpu, pi, bigtmp</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td></td>
<td></td>
<td></td>
<td>cascadelake, avx2, avx512, 6240, nogpu, pi, bigtmp</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>370</td>
<td>v100</td>
<td>4</td>
<td>16</td>
<td>cascadelake, avx2, avx512, 6240, pi, v100</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_13" name="__tabbed_2" type="radio" /><label for="__tabbed_2_13">pi_galvani</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_galvani partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>7</td>
<td>8268</td>
<td>48</td>
<td>356</td>
<td>cascadelake, avx512, 8268, nogpu, standard, bigtmp, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_14" name="__tabbed_2" type="radio" /><label for="__tabbed_2_14">pi_gerstein</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_gerstein partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>6132</td>
<td>28</td>
<td>181</td>
<td>skylake, avx2, avx512, 6132, nogpu, standard, pi, bigtmp</td>
</tr>
<tr>
<td>1</td>
<td>6132</td>
<td>28</td>
<td>749</td>
<td>skylake, avx2, avx512, 6132, nogpu, pi, bigtmp</td>
</tr>
<tr>
<td>11</td>
<td>E5-2680_v4</td>
<td>28</td>
<td>245</td>
<td>broadwell, avx2, E5-2680_v4, nogpu, standard, pi</td>
</tr>
<tr>
<td>1</td>
<td>E7-4820_v4</td>
<td>40</td>
<td>1505</td>
<td>broadwell, avx2, E7-4820_v4, nogpu, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_15" name="__tabbed_2" type="radio" /><label for="__tabbed_2_15">pi_gerstein_gpu</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_gerstein_gpu partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>rtx3090</td>
<td>4</td>
<td>24</td>
<td>cascadelake, avx2, avx512, 6240, doubleprecision, pi, bigtmp, rtx3090</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>rtx3090</td>
<td>8</td>
<td>24</td>
<td>cascadelake, avx2, avx512, 6240, doubleprecision, pi, bigtmp, rtx3090</td>
</tr>
<tr>
<td>2</td>
<td>E5-2680_v4</td>
<td>28</td>
<td>245</td>
<td>p100</td>
<td>2</td>
<td>16</td>
<td>broadwell, avx2, E5-2680_v4, doubleprecision, pi, p100</td>
</tr>
<tr>
<td>1</td>
<td>E5-2637_v4</td>
<td>8</td>
<td>119</td>
<td>titanv</td>
<td>4</td>
<td>12</td>
<td>broadwell, avx2, E5-2637_v4, doubleprecision, pi, bigtmp, titanv</td>
</tr>
<tr>
<td>3</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td>k80</td>
<td>4</td>
<td>12</td>
<td>haswell, avx2, E5-2660_v3, doubleprecision, pi, k80</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_16" name="__tabbed_2" type="radio" /><label for="__tabbed_2_16">pi_gruen</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_gruen partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>E5-2680_v4</td>
<td>28</td>
<td>245</td>
<td>broadwell, avx2, E5-2680_v4, nogpu, standard, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_17" name="__tabbed_2" type="radio" /><label for="__tabbed_2_17">pi_jadi</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_jadi partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>365-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>E5-2680_v4</td>
<td>28</td>
<td>245</td>
<td>broadwell, avx2, E5-2680_v4, nogpu, standard, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_18" name="__tabbed_2" type="radio" /><label for="__tabbed_2_18">pi_jetz</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_jetz partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>4</td>
<td>6240</td>
<td>36</td>
<td>370</td>
<td>cascadelake, avx2, avx512, 6240, nogpu, pi, bigtmp</td>
</tr>
<tr>
<td>4</td>
<td>6240</td>
<td>36</td>
<td>748</td>
<td>cascadelake, avx2, avx512, 6240, nogpu, pi, bigtmp</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_19" name="__tabbed_2" type="radio" /><label for="__tabbed_2_19">pi_kleinstein</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_kleinstein partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>cascadelake, avx2, avx512, 6240, nogpu, standard, pi, bigtmp</td>
</tr>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>cascadelake, avx2, avx512, 6240, nogpu, pi, bigtmp</td>
</tr>
<tr>
<td>3</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td>haswell, avx2, E5-2660_v3, nogpu, standard, oldest, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_20" name="__tabbed_2" type="radio" /><label for="__tabbed_2_20">pi_krauthammer</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_krauthammer partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td>haswell, avx2, E5-2660_v3, nogpu, standard, oldest, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_21" name="__tabbed_2" type="radio" /><label for="__tabbed_2_21">pi_krishnaswamy</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_krishnaswamy partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>5222</td>
<td>36</td>
<td>750</td>
<td>a100</td>
<td>4</td>
<td>40</td>
<td>cascadelake, avx2, avx512, 5222, doubleprecision, bigtmp, a100</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_22" name="__tabbed_2" type="radio" /><label for="__tabbed_2_22">pi_ma</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_ma partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>8268</td>
<td>48</td>
<td>370</td>
<td>cascadelake, avx2, avx512, 8268, nogpu, pi, bigtmp</td>
</tr>
<tr>
<td>2</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td>haswell, avx2, E5-2660_v3, nogpu, standard, oldest, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_23" name="__tabbed_2" type="radio" /><label for="__tabbed_2_23">pi_medzhitov</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_medzhitov partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>cascadelake, avx2, avx512, 6240, nogpu, pi, bigtmp</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_24" name="__tabbed_2" type="radio" /><label for="__tabbed_2_24">pi_miranker</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_miranker partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6248r</td>
<td>48</td>
<td>370</td>
<td>cascadelake, avx2, avx512, 6248r, nogpu, pi, bigtmp</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_25" name="__tabbed_2" type="radio" /><label for="__tabbed_2_25">pi_ohern</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_ohern partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>5</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td>haswell, avx2, E5-2660_v3, nogpu, standard, oldest, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_26" name="__tabbed_2" type="radio" /><label for="__tabbed_2_26">pi_reinisch</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_reinisch partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>5122</td>
<td>8</td>
<td>181</td>
<td>rtx2080</td>
<td>4</td>
<td>8</td>
<td>skylake, avx2, avx512, 5122, singleprecision, pi, rtx2080</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_27" name="__tabbed_2" type="radio" /><label for="__tabbed_2_27">pi_sigworth</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_sigworth partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>rtx2080ti</td>
<td>4</td>
<td>11</td>
<td>cascadelake, avx2, avx512, 6240, singleprecision, pi, bigtmp, rtx2080ti</td>
</tr>
<tr>
<td>1</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td></td>
<td></td>
<td></td>
<td>haswell, avx2, E5-2660_v3, nogpu, standard, oldest, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_28" name="__tabbed_2" type="radio" /><label for="__tabbed_2_28">pi_sindelar</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_sindelar partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>rtx2080ti</td>
<td>4</td>
<td>11</td>
<td>cascadelake, avx2, avx512, 6240, singleprecision, pi, bigtmp, rtx2080ti</td>
</tr>
<tr>
<td>1</td>
<td>E5-2637_v4</td>
<td>8</td>
<td>119</td>
<td>gtx1080ti</td>
<td>4</td>
<td>11</td>
<td>broadwell, avx2, E5-2637_v4, singleprecision, pi, gtx1080ti</td>
</tr>
<tr>
<td>1</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td></td>
<td></td>
<td></td>
<td>haswell, avx2, E5-2660_v3, nogpu, standard, oldest, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_29" name="__tabbed_2" type="radio" /><label for="__tabbed_2_29">pi_tomography</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_tomography partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>4-00:00:00</code></td>
</tr>
<tr>
<td>Maximum GPUs per user</td>
<td><code>12</code></td>
</tr>
<tr>
<td>Maximum running jobs per user</td>
<td><code>2</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>8</td>
<td>5222</td>
<td>8</td>
<td>181</td>
<td>rtx5000</td>
<td>4</td>
<td>16</td>
<td>cascadelake, avx2, avx512, 5222, doubleprecision, pi, bigtmp, rtx5000</td>
</tr>
<tr>
<td>1</td>
<td>6242</td>
<td>32</td>
<td>999</td>
<td>rtx8000</td>
<td>2</td>
<td>48</td>
<td>cascadelake, avx2, avx512, 6242, doubleprecision, pi, bigtmp, rtx8000</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_30" name="__tabbed_2" type="radio" /><label for="__tabbed_2_30">pi_townsend</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_townsend partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>5</td>
<td>E5-2660_v3</td>
<td>20</td>
<td>119</td>
<td>haswell, avx2, E5-2660_v3, nogpu, standard, oldest, pi</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_31" name="__tabbed_2" type="radio" /><label for="__tabbed_2_31">pi_tucci</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_tucci partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>cascadelake, avx2, avx512, 6240, nogpu, standard, pi, bigtmp</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_32" name="__tabbed_2" type="radio" /><label for="__tabbed_2_32">pi_ya-chi_ho</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_ya-chi_ho partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>8268</td>
<td>48</td>
<td>370</td>
<td>cascadelake, avx2, avx512, 8268, nogpu, pi, bigtmp</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_33" name="__tabbed_2" type="radio" /><label for="__tabbed_2_33">pi_yong_xiong</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">GPU jobs need GPUs!</p>
<p>Jobs submitted to this partition  do not request a GPU by default. You must request one with the <a href="/clusters-at-yale/job-scheduling/resource-requests/#request-gpus"><code>--gpus</code></a> option.</p>
</div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_yong_xiong partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>GPU Type</th>
<th>GPUs/Node</th>
<th>vRAM/GPU (GB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6226r</td>
<td>32</td>
<td>181</td>
<td>rtx3090</td>
<td>4</td>
<td>24</td>
<td>cascadelake, avx2, avx512, 6226r, doubleprecision, pi, rtx3090</td>
</tr>
</tbody>
</table>
</div>
<input id="__tabbed_2_34" name="__tabbed_2" type="radio" /><label for="__tabbed_2_34">pi_zhao</label><div class="tabbed-content">
<p><strong>Request Defaults</strong></p>
<p>Unless specified, your jobs will run with the following options to <code>srun</code> and <code>sbatch</code> options for this partition.</p>
<div class="highlight"><pre><span></span><code>--time=1-00:00:00 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem-per-cpu=5120
</code></pre></div>
<p><strong>Job Limits</strong></p>
<p>Jobs submitted to the pi_zhao partition are subject to the following limits:</p>
<table>
<thead>
<tr>
<th>Limit</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum job time limit</td>
<td><code>14-00:00:00</code></td>
</tr>
</tbody>
</table>
<p><strong>Available Compute Nodes</strong></p>
<p>Requests for <code>--cpus-per-task</code> and <code>--mem</code> can't exceed what is available on a single compute node.</p>
<table>
<thead>
<tr>
<th>Count</th>
<th>CPU Type</th>
<th>CPUs/Node</th>
<th>Memory/Node (GiB)</th>
<th>Node Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>6240</td>
<td>36</td>
<td>181</td>
<td>cascadelake, avx2, avx512, 6240, nogpu, standard, pi, bigtmp</td>
</tr>
</tbody>
</table>
</div>
</div>
</details>
<h2 id="public-datasets">Public Datasets</h2>
<p>We host datasets of general interest in a loosely organized directory tree in <code>/gpfs/ysm/datasets</code>:</p>
<div class="highlight"><pre><span></span><code> cryoem
 db
  annovar
  blast
  busco
  Pfam
 genomes
     1000Genomes
     10xgenomics
     Aedes_aegypti
     Chelonoidis_nigra
     Danio_rerio
     Drosophila_melanogaster
     hisat2
     Homo_sapiens
     Mus_musculus
     PhiX
     Saccharomyces_cerevisiae
</code></pre></div>
<p>If you would like us to host a dataset or questions about what is currently available, please <a href="/#get-help">contact us</a>.</p>
<h2 id="storage">Storage</h2>
<p>Farnam has access to a number of GPFS filesystems. <code>/gpfs/ysm</code> is Farnam's primary filesystem where Home, Project, and Scratch60 directories are located. For more details on the different storage spaces, see our <a href="/clusters-at-yale/data/index">Cluster Storage</a> documentation.</p>
<p>You can check your current storage usage &amp; limits by running the <code>getquota</code> command. Your <code>~/project</code> and <code>~/scratch60</code> directories are shortcuts. Get a list of the absolute paths to your directories with the <code>mydirectories</code> command. If you want to share data in your Project or Scratch directory, see the <a href="/clusters-at-yale/data/permissions/">permissions</a> page.</p>
<p>For information on data recovery, see the <a href="/clusters-at-yale/data/#backups-and-snapshots">Backups and Snapshots</a> documentation.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Files stored in <code>scratch60</code> are purged if they are older than 60 days. You will receive an email alert one week before they are deleted.</p>
</div>
<table>
<thead>
<tr>
<th>Partition</th>
<th>Root Directory</th>
<th>Storage</th>
<th>File Count</th>
<th>Backups</th>
<th>Snapshots</th>
</tr>
</thead>
<tbody>
<tr>
<td>home</td>
<td><code>/gpfs/ysm/home</code></td>
<td>125GiB/user</td>
<td>500,000</td>
<td>Yes</td>
<td>&gt;=2 days</td>
</tr>
<tr>
<td>project</td>
<td><code>/gpfs/ysm/project</code></td>
<td>1TiB/group, increase to 4TiB on request</td>
<td>5,000,000</td>
<td>No</td>
<td>&gt;=2 days</td>
</tr>
<tr>
<td>scratch60</td>
<td><code>/gpfs/ysm/scratch60</code></td>
<td>20TiB/group</td>
<td>15,000,000</td>
<td>No</td>
<td>&gt;=2 days</td>
</tr>
</tbody>
</table>

  <hr>
<div class="md-source-file">
  <small>
    
      Last update:
      <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">January 6, 2022</span>
      
    
  </small>
</div>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <!--
  Copyright (c) 2016-2020 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->



<!-- Application footer -->
<footer class="md-footer">
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div>
        <a href="https://www.yale.edu" title="Yale" target="_blank">
          <img alt="Yale Logo" src="/img/yale-white.png" height="60" style="padding: 8px; height: 60px;">
	</a>
      </div>
 
      <!-- Accessability/Privacy information -->
      <div class="md-footer-copyright">
        <a href="https://usability.yale.edu/web-accessibility/accessibility-yale">Accessibility at Yale</a>
            
        <a title="Yale Privacy policy" href="http://www.yale.edu/privacy-policy">Privacy policy</a>
          
      </div>

      <!-- Social links -->
      <div class="md-social">
  
    
    
      
      
    
    <a href="https://ycrc.yale.edu" target="_blank" rel="noopener" title="ycrc.yale.edu" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19.07 4.93C17.22 3 14.66 1.96 12 2c-2.66-.04-5.21 1-7.06 2.93C3 6.78 1.96 9.34 2 12c-.04 2.66 1 5.21 2.93 7.06C6.78 21 9.34 22.04 12 22c2.66.04 5.21-1 7.06-2.93C21 17.22 22.04 14.66 22 12c.04-2.66-1-5.22-2.93-7.07M17 12v6h-3.5v-5h-3v5H7v-6H5l7-7 7.5 7H17Z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="http://help.ycrc.yale.edu/" target="_blank" rel="noopener" title="help.ycrc.yale.edu" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m15.07 11.25-.9.92C13.45 12.89 13 13.5 13 15h-2v-.5c0-1.11.45-2.11 1.17-2.83l1.24-1.26c.37-.36.59-.86.59-1.41a2 2 0 0 0-2-2 2 2 0 0 0-2 2H8a4 4 0 0 1 4-4 4 4 0 0 1 4 4 3.2 3.2 0 0 1-.93 2.25M13 19h-2v-2h2M12 2A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10c0-5.53-4.5-10-10-10Z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://research.computing.yale.edu/system-status" target="_blank" rel="noopener" title="research.computing.yale.edu" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M7.5 4A5.5 5.5 0 0 0 2 9.5c0 .5.09 1 .22 1.5H6.3l1.27-3.37c.3-.8 1.48-.88 1.86 0L11.5 13l.59-1.42c.13-.33.48-.58.91-.58h8.78c.13-.5.22-1 .22-1.5A5.5 5.5 0 0 0 16.5 4c-1.86 0-3.5.93-4.5 2.34C11 4.93 9.36 4 7.5 4M3 12.5a1 1 0 0 0-1 1 1 1 0 0 0 1 1h2.44L11 20c1 .9 1 .9 2 0l5.56-5.5H21a1 1 0 0 0 1-1 1 1 0 0 0-1-1h-7.6l-.93 2.3c-.4 1.01-1.55.87-1.92.03L8.5 9.5l-.96 2.33c-.15.38-.49.67-.94.67H3Z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://github.com/ycrc" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://twitter.com/yalecrc" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
    </a>
  
</div>
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.instant", "navigation.tabs", "navigation.tabs.sticky"], "search": "../../../assets/javascripts/workers/search.2a1c317c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.6e54b5cd.min.js"></script>
      
        <script src="../../../js/extra.js"></script>
      
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
      
    
  </body>
</html>