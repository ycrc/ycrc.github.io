
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Yale Center for Research Computing Documentation">
      
      
      
        <meta name="author" content="YCRC">
      
      
        <link rel="canonical" href="https://docs.ycrc.yale.edu/news/">
      
      <link rel="icon" href="../assets/images/favicon.ico">
      <meta name="generator" content="mkdocs-1.3.1, mkdocs-material-7.3.6">
    
    
      
        <title>News - Yale Center for Research Computing</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.a57b2b03.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.3f5d1f46.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
      <link rel="stylesheet" href="../_static/stylesheets/extra.css">
    
    
      

  


  

  


  <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-QM84KPHSPL"),document.addEventListener("DOMContentLoaded",function(){"undefined"!=typeof location$&&location$.subscribe(function(t){gtag("config","G-QM84KPHSPL",{page_path:t.pathname})})})</script>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-QM84KPHSPL"></script>


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
  
    
    <script>function __prefix(e){return new URL("..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#news" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <!--
  Copyright (c) 2016-2020 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->



  


<header class="md-header md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Yale Center for Research Computing" class="md-header__button md-logo" aria-label="Yale Center for Research Computing" data-md-component="logo">
      
  <img src="../img/manual.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>

    <!-- Header title -->
    <div class="md-header__title" data-md-component="header-title">
        <div class="md-header__ellipsis">
            <span class="md-header__topic md-ellipsis branding">
              <a href=".." >
                  <kern style="letter-spacing:-0.09em">Y</kern>ale <em>Center for Research Computing</em>
              </a>
            </span>
          <span class="md-header__topic md-ellipsis">
            
              News
            
          </span>
        </div>

    </div>

    

    

    <div>
      <a href="/#get-help" class="get-help">Need Help?</a>
    </div>
    <!-- Search Interface -->
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    




  </nav>
  
    
      <!--
  Copyright (c) 2016-2019 Martin Donath <martin.donath@squidfunk.com>
  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:
  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.
  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->

<!-- Determine class according to level -->


  



<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link md-tabs__link--active">
        About
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../clusters-at-yale/" class="md-tabs__link">
        User Guide
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../data/" class="md-tabs__link">
        Data & Storage
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../ai/" class="md-tabs__link">
        AI & Machine Learning
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../clusters-at-yale/guides/" class="md-tabs__link">
        Software & Tools
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../resources/" class="md-tabs__link">
        Training & Other Resources
      </a>
    </li>
  

      
      <li class="md-tabs__item status"><a class="md-tabs__link" href="http://research.computing.yale.edu/system-status"><span id='status-icon'></span> Systems Status</a></li>
    </ul>
  </div>
</nav>

<div id="system-status-message"></div>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Yale Center for Research Computing" class="md-nav__button md-logo" aria-label="Yale Center for Research Computing" data-md-component="logo">
      
  <img src="../img/manual.svg" alt="logo">

    </a>
    Yale Center for Research Computing
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/ycrc/ycrc.github.io/tree/src" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    github/ycrc/docs
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1" type="checkbox" id="__nav_1" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_1">
          About
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="About" data-md-level="1">
        <label class="md-nav__title" for="__nav_1">
          <span class="md-nav__icon md-icon"></span>
          About
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        News
      </a>
      
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1_3" type="checkbox" id="__nav_1_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_1_3">
          HPC Clusters
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="HPC Clusters" data-md-level="2">
        <label class="md-nav__title" for="__nav_1_3">
          <span class="md-nav__icon md-icon"></span>
          HPC Clusters
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters/" class="md-nav__link">
        Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters/bouchet/" class="md-nav__link">
        Bouchet
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters/hopper/" class="md-nav__link">
        Hopper
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters/grace/" class="md-nav__link">
        Grace
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters/mccleary/" class="md-nav__link">
        McCleary
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters/milgram/" class="md-nav__link">
        Milgram
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters/misha/" class="md-nav__link">
        Misha
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters/grace-mccleary-decommission/" class="md-nav__link">
        Grace & McCleary Decommission
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters/maintenance/" class="md-nav__link">
        Maintenance
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../user-group/" class="md-nav__link">
        YCRC User Group
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="https://research.computing.yale.edu/about" class="md-nav__link">
        About the YCRC
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../glossary/" class="md-nav__link">
        Glossary
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          User Guide
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="User Guide" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          User Guide
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/" class="md-nav__link">
        Getting Started
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/access/accounts/" class="md-nav__link">
        Accounts & Best Practices
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="https://research.computing.yale.edu/account-request" class="md-nav__link">
        Request an Account
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/help-requests/" class="md-nav__link">
        Help Requests
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/troubleshoot/" class="md-nav__link">
        Troubleshoot Login
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/policies/" class="md-nav__link">
        Cluster Usage Policies
      </a>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_7" type="checkbox" id="__nav_2_7" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_7">
          Access the Clusters
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Access the Clusters" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_7">
          <span class="md-nav__icon md-icon"></span>
          Access the Clusters
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/access/" class="md-nav__link">
        Log on to the Clusters
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_7_2" type="checkbox" id="__nav_2_7_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_7_2">
          Web Portal
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Web Portal" data-md-level="3">
        <label class="md-nav__title" for="__nav_2_7_2">
          <span class="md-nav__icon md-icon"></span>
          Web Portal
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/access/ood/" class="md-nav__link">
        Access the Web Portal
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/access/ood-remote-desktop/" class="md-nav__link">
        Remote Desktop
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/access/ood-jupyter/" class="md-nav__link">
        Jupyter
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/access/ood-rstudio/" class="md-nav__link">
        RStudio
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/access/ood-vscode/" class="md-nav__link">
        VSCode
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_7_3" type="checkbox" id="__nav_2_7_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_7_3">
          SSH Connection
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="SSH Connection" data-md-level="3">
        <label class="md-nav__title" for="__nav_2_7_3">
          <span class="md-nav__icon md-icon"></span>
          SSH Connection
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/access/ssh/" class="md-nav__link">
        Connect with SSH
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/access/x11/" class="md-nav__link">
        Graphical Interfaces (X11)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/access/advanced-config/" class="md-nav__link">
        Advanced SSH Configuration
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/access/vpn/" class="md-nav__link">
        Access from Off Campus (VPN)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/access/mfa/" class="md-nav__link">
        Multi-factor Authentication
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/access/courses/" class="md-nav__link">
        Courses
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_8" type="checkbox" id="__nav_2_8" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_8">
          Applications & Software
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Applications & Software" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_8">
          <span class="md-nav__icon md-icon"></span>
          Applications & Software
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../applications/" class="md-nav__link">
        Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../applications/compile/" class="md-nav__link">
        Build Software
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../applications/modules/" class="md-nav__link">
        Software Modules
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../applications/toolchains/" class="md-nav__link">
        Module Toolchains
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../applications/lifecycle/" class="md-nav__link">
        Module Lifecycle
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_9" type="checkbox" id="__nav_2_9" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_9">
          Job Scheduling
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Job Scheduling" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_9">
          <span class="md-nav__icon md-icon"></span>
          Job Scheduling
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/job-scheduling/" class="md-nav__link">
        Run Jobs with Slurm
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/job-scheduling/resource-requests/" class="md-nav__link">
        Request Compute Resources
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/job-scheduling/job_defense/" class="md-nav__link">
        Automated Detection of Idle Resources
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/job-scheduling/getusage/" class="md-nav__link">
        Monitor Overall Slurm Usage
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/job-scheduling/common-job-failures/" class="md-nav__link">
        Common Job Failures
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/job-scheduling/jobstats/" class="md-nav__link">
        Job Performance Monitoring
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/job-scheduling/priority-tier/" class="md-nav__link">
        Priority Tier
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/job-scheduling/fairshare/" class="md-nav__link">
        Priority & Wait Time
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/job-scheduling/dsq/" class="md-nav__link">
        Job Arrays
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/job-scheduling/scavenge/" class="md-nav__link">
        Scavenge Partition
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/job-scheduling/mpi/" class="md-nav__link">
        MPI Partition
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/job-scheduling/dependency/" class="md-nav__link">
        Jobs with Dependencies
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/job-scheduling/scrontab/" class="md-nav__link">
        Recurring Jobs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/job-scheduling/cmd-line-args/" class="md-nav__link">
        Pass Values into Jobs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/job-scheduling/slurm-examples/" class="md-nav__link">
        Submission Script Examples
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/glossary/" class="md-nav__link">
        Glossary
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          Data & Storage
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Data & Storage" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Data & Storage
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../data/" class="md-nav__link">
        Data Storage
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../data/hpc-storage/" class="md-nav__link">
        HPC Storage
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../data/backups/" class="md-nav__link">
        Backups and Snapshots
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../data/google-drive/" class="md-nav__link">
        Google Drive
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../data/ycga-data/" class="md-nav__link">
        YCGA Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_6" type="checkbox" id="__nav_3_6" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3_6">
          Transfer Data
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Transfer Data" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_6">
          <span class="md-nav__icon md-icon"></span>
          Transfer Data
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../data/transfer/" class="md-nav__link">
        Transfer to Cluster
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../data/globus/" class="md-nav__link">
        Large Transfers with Globus
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/rclone/" class="md-nav__link">
        Rclone
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../data/staging/" class="md-nav__link">
        Stage Data for Compute Jobs
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_7" type="checkbox" id="__nav_3_7" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3_7">
          Manage & Share
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Manage & Share" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_7">
          <span class="md-nav__icon md-icon"></span>
          Manage & Share
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../data/permissions/" class="md-nav__link">
        Share with Cluster Users
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../data/archive/" class="md-nav__link">
        Archive Your Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../data/external/" class="md-nav__link">
        Share Data Outside Yale
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../data/group-change/" class="md-nav__link">
        Group Change
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../data/hosting-UCSC-trackhub/" class="md-nav__link">
        Hosting a UCSC Track Hub
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../data/glossary/" class="md-nav__link">
        Glossary
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          AI & Machine Learning
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="AI & Machine Learning" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          AI & Machine Learning
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ai/" class="md-nav__link">
        AI & ML Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ai/gpu-jobstats/" class="md-nav__link">
        GPU monitoring and detection
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ai/resources/" class="md-nav__link">
        LLMs and GPU Availability
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ai/ollama/" class="md-nav__link">
        Ollama
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ai/exercises/" class="md-nav__link">
        Ollama Exercises
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ai/huggingface/" class="md-nav__link">
        Hugging Face
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ai/miniconda-multigpu/" class="md-nav__link">
        Multi-GPU Submission Scripts
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ai/nairr/" class="md-nav__link">
        National AI Research Resource (NAIRR)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ai/pythonpackages/" class="md-nav__link">
        Common AI/ML python package Installation Procedures
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ai/aicodingtools/" class="md-nav__link">
        AI Coding Tools
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ai/clarity/" class="md-nav__link">
        Using Closed-Source Models: API Keys and Clarity
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5">
          Software & Tools
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Software & Tools" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Software & Tools
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/" class="md-nav__link">
        Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_2" type="checkbox" id="__nav_5_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5_2">
          Software
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Software" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_2">
          <span class="md-nav__icon md-icon"></span>
          Software
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_2_1" type="checkbox" id="__nav_5_2_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5_2_1">
          Python
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Python" data-md-level="3">
        <label class="md-nav__title" for="__nav_5_2_1">
          <span class="md-nav__icon md-icon"></span>
          Python
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/python/" class="md-nav__link">
        Python
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/conda/" class="md-nav__link">
        Conda
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/pytorch/" class="md-nav__link">
        Pytorch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/tensorflow/" class="md-nav__link">
        Tensorflow
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/mpi4py/" class="md-nav__link">
        MPI with Python
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/atlas/" class="md-nav__link">
        ATLAS Computing Environment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/cesm/" class="md-nav__link">
        CESM/CAM
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/comsol/" class="md-nav__link">
        COMSOL
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/gaussian/" class="md-nav__link">
        Gaussian
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/isca/" class="md-nav__link">
        Isca
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/matlab/" class="md-nav__link">
        MATLAB
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/mathematica/" class="md-nav__link">
        Mathematica
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/namd/" class="md-nav__link">
        NAMD
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/nextflow/" class="md-nav__link">
        Nextflow
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/r/" class="md-nav__link">
        R
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/vasp/" class="md-nav__link">
        VASP
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_3" type="checkbox" id="__nav_5_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5_3">
          Tools
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Tools" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_3">
          <span class="md-nav__icon md-icon"></span>
          Tools
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/containers/" class="md-nav__link">
        Containers
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/mysql/" class="md-nav__link">
        Mysql
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/parallel/" class="md-nav__link">
        Parallel
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/tmux/" class="md-nav__link">
        tmux
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/virtualgl/" class="md-nav__link">
        VirtualGL
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/xvfb/" class="md-nav__link">
        XVFB
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_4" type="checkbox" id="__nav_5_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5_4">
          Guides
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Guides" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_4">
          <span class="md-nav__icon md-icon"></span>
          Guides
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/cryoem/" class="md-nav__link">
        Cryo-EM on McCleary
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/github/" class="md-nav__link">
        GitHub
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/github_pages/" class="md-nav__link">
        GitHub Pages
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/gpus-cuda/" class="md-nav__link">
        GPUs and CUDA
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../clusters-at-yale/guides/spark/" class="md-nav__link">
        Spark
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" type="checkbox" id="__nav_6" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6">
          Training & Other Resources
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Training & Other Resources" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          Training & Other Resources
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../resources/" class="md-nav__link">
        Overview
      </a>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6_2" type="checkbox" id="__nav_6_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6_2">
          Tutorials
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Tutorials" data-md-level="2">
        <label class="md-nav__title" for="__nav_6_2">
          <span class="md-nav__icon md-icon"></span>
          Tutorials
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../resources/intro_to_hpc_tutorial/" class="md-nav__link">
        Introduction to HPC Tutorials
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../resources/online-tutorials/" class="md-nav__link">
        Online Tutorials
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../resources/yale_library/" class="md-nav__link">
        Yale Library
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../resources/sw_carpentry/" class="md-nav__link">
        Software Carpentry
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6_3" type="checkbox" id="__nav_6_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6_3">
          Training
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Training" data-md-level="2">
        <label class="md-nav__title" for="__nav_6_3">
          <span class="md-nav__icon md-icon"></span>
          Training
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="https://research.computing.yale.edu/training" class="md-nav__link">
        YCRC Workshops
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="https://ycrc.yale.edu/youtube" class="md-nav__link">
        YCRC YouTube Channel
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../resources/national-hpcs/" class="md-nav__link">
        National HPCs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../resources/glossary/" class="md-nav__link">
        Glossary
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/ycrc/ycrc.github.io/tree/src/docs/news.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                <h1 id="news">News</h1>
<p>




<style>
    .md-typeset .blog-post:first-of-type h3 {
        margin-top: 0;
    }

    .md-typeset .blog-post-title {
        margin-bottom: 0;
    }

    .md-typeset .blog-post-extra {
        color: var(--md-default-fg-color--light);
    }

    .md-typeset .blog-center {
        text-align: center;
    }

    .md-typeset .blog-pagination {
        display: inline-block;
    }

    .md-typeset .blog-pagination a {
        color: var(--md-typeset-color);
        float: left;
        padding: .25em 1em;
        text-decoration: none;
        border-radius: 1em;
        margin-left: .25em;
        margin-right: .25em;
        transition: all .15s ease-in-out;
    }

    .md-typeset .blog-pagination a.active {
        background-color: var(--md-typeset-a-color);
        color: white;
        font-weight: bold;
    }

    .md-typeset .blog-pagination a:hover:not(.active) {
        background-color: #dddddda1;
    }

    .md-typeset .blog-hidden {
        display: none;
    }
</style>

<style>

</style>
















    <style>
    .md-typeset .blogging-tags-grid {
        display: flex;
        flex-direction: row;
        flex-wrap: wrap;
        gap: 8px;
        margin-top: 5px;
    }

    .md-typeset .blogging-tag {
        color: var(--md-typeset-color);
        background-color: var(--md-typeset-code-color);
    }

    .md-typeset .blogging-tag code {
        border-radius: 5px;
    }
</style>

    






<div class="pages">
    
        
        <div class="page" id="page1">

            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="september-2025">September 2025</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="announcing-hopper-the-new-ycrc-secure-hpc-cluster">Announcing Hopper, the new YCRC Secure HPC Cluster</h4>
<p>We are thrilled to announce the availability of the Hopper HPC cluster. Hopper is a NIST 800-171/HIPAA compliant secure computing environment for all researchers at Yale University for high performance computation of electronic Protected Health Information (ePHI), NIH Controlled-Access Data, Controlled Unclassified Information (CUI) and other types of sensitive data. </p>
<p>More information about Hopper can be found at <a href="https://docs.ycrc.yale.edu/clusters/hopper/">https://docs.ycrc.yale.edu/clusters/hopper/</a>.</p>
<p>Email us at <a href="mailto:research.computing@yale.edu">research.computing@yale.edu</a> to inquire about using Hopper.</p>
<h4 id="beta-testing-complete-for-h200-gpus-on-bouchet">Beta Testing Complete for H200 GPUs on Bouchet</h4>
<p>The beta testing of the H200 GPUs on the Bouchet cluster is now complete and the cluster is available to all Yale researchers for projects with low-risk data. Similar to Grace and McCleary, compute resources are available to all researchers at no cost. If you have used Grace or McCleary since July 1, 2024, we have automatically created an account for you on Bouchet. If you have uploaded ssh keys to Grace or McCleary, your key will already be installed on Bouchet. If you are unsure if you have an account, please try logging into <a href="https://ood-bouchet.ycrc.yale.edu">Bouchets Open OnDemand web portal</a> (Yale VPN required) before submitting an account request.
If an account was not automatically created for you on Bouchet, please <a href="https://research.computing.yale.edu/account-request">submit an account request</a>.</p>
<p>Bouchet follows most of the same paradigms as Grace and McCleary, but there are a <a href="/clusters/bouchet_getting_started/">small number of key differences</a>. We encourage you to review those differences before starting work on Bouchet. If you are new to the YCRC clusters, we recommend getting started with <a href="https://research.computing.yale.edu/intro-to-hpc">our Intro to HPC training video</a>.</p>
<h4 id="expanded-office-hours-now-available">Expanded Office Hours Now Available</h4>
<p>We're excited to announce that we've expanded our Office Hours to make it even easier for you to get the research computing support you need! Join us for drop-in consultations on Wednesdays 11am-12pm and our <strong>new</strong> Thursday 2-3pm slot <a href="https://yale.zoom.us/my/ycrcsupport">via Zoom</a> - no appointment needed. </p>
<p>Please check the OHs schedule at <a href="https://research.computing.yale.edu/training-events">https://research.computing.yale.edu/training-events</a> for any changes.</p>
<p>Our Research Support team is here to help you navigate YCRC computing resources, evaluate your project's computational needs, and optimize your workflows from basic setup to advanced applications. Office Hours are perfect for quick consultations (approximately 15 minutes) on general YCRC questions, novice user support, resource monitoring, and troubleshooting common issues like missing libraries. For more complex needs such as custom software installations, detailed code troubleshooting, or miniconda environment setup, we encourage you to contact us at research.computing@yale.edu to schedule a dedicated 1-on-1 consultation where we can provide the focused attention your project deserves. </p>
<p>Whether you're just getting started or working with exceptional computational requirements, we're here to collaborate with you and ensure you're making the most of Yale's research computing resources.</p>
<p>Learn more about the computational research support services our team provides at https://research.computing.yale.edu/research-support/services and <a href="/#get-help">contact us</a> with any questions. </p>
<h4 id="new-ycrc-website">New YCRC Website</h4>
<p>In early August, we launched our new website--<a href="https://research.computing.yale.edu">research.computing.yale.edu</a>, which was built on the new Yale Sites platform. We hope the new site will prove to be user friendly and informative. Please share any feedback and suggestions for improvement by <a href="https://research.computing.yale.edu/contact-web-editor">contacting the web editor</a>.</p>
    </div>
    <div class="blog-post-extra">
        Published: September 01, 2025
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="august-2025">August 2025</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="ai-initiative-gpus-now-available">AI Initiative GPUs Now Available</h4>
<p>Following the <a href="https://provost.yale.edu/yale-task-force-artificial-intelligence">AI Task Force's recommendation</a> that the university invest in high-end GPUs over the next several years, we are thrilled to announce the first set of GPUs funded by the <a href="https://provost.yale.edu/news/advancing-yales-leadership-artificial-intelligence-support-faculty-students-and-staff">Provosts AI Initiative</a> are now available on the YCRC clusters.</p>
<ul>
<li>80 H200 NVIDIA GPUs on Bouchet (for research without sensitive data).</li>
<li>32 H200 NVIDIA GPUs on Hopper (for research requiring NIST 800-171, HIPAA, or other sensitive data)</li>
<li>12 H100 NVIDIA GPUs on Milgram (for research with unregulated sensitive data)</li>
</ul>
<p>Please <a href="/#get-help">contact us</a> with questions and for assistance. </p>
<h4 id="priority-tier-fast-lane-available">Priority Tier Fast-Lane Available</h4>
<p>On December 1st, 2024, we introduced Priority Tier partitions on Bouchet, Grace, McCleary and Milgram. Jobs submitted to a Priority Tier partition precede all pending jobs in the corresponding standard tier partitions in the scheduling queue to provide a fast-lane. The Priority Tier partitions are also composed of the YCRCs newest nodes and GPUs, ensuring priority jobs run on our fastest and most powerful resources. If you are interested in using Priority Tier, you can learn more and request access on <a href="https://docs.ycrc.yale.edu/clusters-at-yale/job-scheduling/priority-tier/">our Priority Tier docs page</a>.</p>
<h4 id="research-support-at-pearc25">Research Support at PEARC25</h4>
<p>During the week of July 20th, the YCRC Research Support team attended the <a href="https://pearc.acm.org/pearc25/">annual PEARC conference</a>, a conference specifically focused on the research computing community. The team had the opportunity to meet with many of our peer institutions to discuss our common challenges and opportunities and attend sessions to learn about new solutions. We look forward to incorporating new ideas and solutions from PEARC to the YCRCs offerings this year!</p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li>We are adding commonly used software from Grace &amp; McCleary to Bouchet. Be sure to check the available software via <code>module avail</code> to see if we have pre-installed the software you need and <a href="/#get-help">contact us</a> with questions and requests for software. </li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: August 01, 2025
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="july-2025">July 2025</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="bouchet-open-to-all-yale-researchers">Bouchet Open to All Yale Researchers</h4>
<p>The Bouchet HPC cluster is now available for all Yale researchers. Bouchet contains approximately 10,000 direct-liquid-cooled cores as well as 48 NVIDIA A5000Ada GPUs. 80 NVIDIA H200 GPUs will be added to Bouchet in early July. Accounts on Bouchet can be requested now via the <a href="https://research.computing.yale.edu/account-request">YCRC Account Request Form</a>. For more information and updates visit the <a href="/clusters/bouchet/">Bouchet Documentation page</a>. Information sessions on transitioning from Grace and McCleary to Bouchet will be held later this summer. Dates will be announced soon! </p>
<h4 id="new-ycga-sequence-data-archive-enhanced-access-to-your-files">New YCGA Sequence Data Archive - Enhanced Access to Your Files</h4>
<p>Weve launched the new YCGA sequence data archive with a user-friendly web interface to enable simplified retrieval of the archived sequence files. This is a replacement for the existing methods using the ycgaFastq and URLFetch scripts. The new system offers improved performance, reliability, and security, including required CAS login and research group access restrictions.  All of the data in the previous sequence archive has been migrated to the new archive.
For more information and instructions on how to retrieve your files,visit <a href="https://archive.ycga.yale.edu/">https://archive.ycga.yale.edu/</a> (You must be on Yale network and log in via CAS to access) and <a href="/data/ycga-data/">https://docs.ycrc.yale.edu/data/ycga-data/</a> If you need assistance, please <a href="/#get-help">contact us</a> for assistance. </p>
<h4 id="troubleshooting-open-ondemand-issues">Troubleshooting Open OnDemand Issues</h4>
<p>If you are experiencing problems with Open OnDemand, such as inability to access the portal or  failures when launching apps, we suggest trying the following solutions to improve performance.</p>
<ul>
<li><strong>Check your storage space.</strong> Open OnDemand won't launch if your home directory is full. Verify you haven't exceeded your user quota.</li>
<li><strong>Clear your browser cache completely.</strong> Use a private browsing window or clear your entire browser cache (not just the last 24 hours). This resolves most session-related issues.</li>
<li><strong>For unresponsive RStudio sessions:</strong> If RStudio becomes slow or stops responding, terminate the session and run this command at the shell prompt:</li>
</ul>
<div class="highlight"><pre><span></span><code>ycrc_clean_rstudio.sh
</code></pre></div>
<p>This removes temporary RStudio files and allows a fresh start.</p>
<p>Need more help? Additional troubleshooting steps are available in <a href="/access/ood/">our R documentation</a>.
Please <a href="/#get-help">contact us</a> for assistance. </p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong>OpenMPI/5.0.3</strong> is now available on Grace and Bouchet</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: July 01, 2025
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="grace-mccleary-milgram-maintenance">Grace, McCleary, &amp; Milgram Maintenance</h2>
<p><em>June 10-12, 2025</em></p>
<h3 id="software-updates">Software Updates</h3>
<ul>
<li>Security updates have been applied</li>
<li>OpenMPI 5 is now available (Grace only)</li>
<li>Open OnDemand has been updated to version 3.1.</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: June 12, 2025
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="bouchet-maintenance">Bouchet Maintenance</h2>
<p><em>June 2-4, 2025</em></p>
<h3 id="software-updates">Software Updates</h3>
<ul>
<li>Security updates have been applied</li>
<li>OpenMPI 5 is now available</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: June 04, 2025
    </div>
    <hr />
</div>

                
            
        </div>
    
        
        <div class="page" id="page2">

            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="june-2025">June 2025</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="faster-startups-for-modules-and-cluster-apps-including-rstudio-r-and-relion">Faster startups for modules and cluster apps including RStudio, R and RELION</h4>
<p>We've introduced a new "fast module loading" feature that significantly reduces wait times when loading software modules on our McCleary and Grace clusters.</p>
<p>What You'll Notice
- The <a href="https://ood-mccleary.ycrc.yale.edu/pun/sys/dashboard/batch_connect/sys/ycrc_rstudio_server/session_contexts/new">RStudio OpenOnDemand app</a> now launches in about 10 seconds (down from 40 seconds previously), thanks to this optimization running automatically in the background.
- From the command line, load times of large and complex <a href="/applications/modules">modules</a> like R and RELION can be reduced to a few seconds by loading the new <a href="/modules/#fast-module-loading">mlq</a> module and using ml to load these modules, i.e.: 'module load mlq; ml R instead of module load R
- If you encounter any problems with fast module loading in our RStudio app, it can be disabled with a new checkbox that appears at the top of the web form.  We developed this tool in house and continue testing and refining it.</p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong>PyTorch/2.1.2-foss-2022b-CUDA-12.1.1</strong> (with CUDA support) is now available on Bouchet</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: June 01, 2025
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="may-2025">May 2025</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="announcing-hopper-secure-computing-environment">Announcing Hopper Secure Computing Environment</h4>
<p>We are pleased to announce the upcoming launch of <a href="https://docs.ycrc.yale.edu/clusters/hopper/">Hopper</a>, our new secure high performance computing cluster, scheduled for deployment in Summer 2025. Upon completion, Hopper will have undergone rigorous external auditing for NIST 800-171 and HIPAA compliance, making it suitable for high performance computation of electronic Protected Health Information (ePHI), NIH Controlled-Access Data, Controlled Unclassified Information (CUI), and other sensitive data types.</p>
<p>This secure computing environment represents a collaborative effort between the YCRC and Health Sciences IT, to ensure enhanced support for secure computational research. Hopper complements Yale's secure computing portfolio, which includes SpinUp+ and <a href="https://medicine.yale.edu/ybic/computational-resources/ynhhs/#computational-health-platform">Computational Health Platform (CHP)</a>, developed to address the diverse secure computing needs of the Yale research community.</p>
<p>We anticipate Hopper will be fully operational and available to Yale researchers by July 2025, with access procedures to be announced in June. </p>
<h4 id="nih-controlled-access-data-on-mccleary">NIH Controlled Access Data on McCleary</h4>
<p>Effective January 25, 2025, all new or renewed Data Use Certifications for NIH Controlled-Access Data and Repositories must comply with the <a href="https://sharing.nih.gov/sites/default/files/flmngr/NIH-Security-BPs-for-Users-of-Controlled-Access-Data.pdf">NIH Security Best Practices for Users of Controlled-Access Data</a>.</p>
<p>During the development phase of the YCRC's new cluster <a href="https://docs.ycrc.yale.edu/clusters/hopper/">Hopper</a>,  Yale University has successfully completed the required documentation to designate McCleary as an approved environment for housing NIH Controlled-Access Data and Repositories, subject to specific conditions.</p>
<p>For more information and access requests, please consult our <a href="https://docs.ycrc.yale.edu/data/nih-data/">NIH Controlled-Access Data documentation</a>.</p>
<h4 id="ycrc-user-town-hall-meetings">YCRC User Town Hall Meetings</h4>
<p>Throughout April, the YCRC Leadership hosted a series of Town Hall meetings with Principal Investigators on Science Hill and at the Medical Library and welcoming all users to our 160 St Ronan Street location.</p>
<p>During these sessions, we presented significant updates regarding our computing infrastructure expansion, team growth initiatives, and strategic calibration of services to address the evolving computational needs of the Yale research community. The meetings provided a forum for us to hear directly from both PI and non-PI users about their specific requirements, concerns, and innovative suggestions for enhancing our support framework.</p>
<p>We extend our sincere appreciation to everyone who participated in these discussions, contributing to our mission of advancing computational research at Yale. We look forward to continuing this productive dialogue in the future Town Hall meetings. </p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong>FreeSurfer/8.0.0</strong> now available on Grace, McCleary and Milgram</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: May 01, 2025
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="april-2025">April 2025</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="cluster-user-town-hall-meeting">Cluster User Town Hall Meeting</h4>
<p>Please join us in shaping a meaningful long-term vision and strategy for YCRC as we advance our mission of supporting cutting-edge research. The session will include a brief presentation on recent developments and upcoming plans, followed by an open forum for questions and feedback. We look forward to hearing from you.</p>
<p><em>YCRC User Town Hall<br />
Date: Monday, April 14, 2025<br />
Time: 12:30 PM - 2:00 PM<br />
Location:  YCRC Auditorium, 160 St Ronan Street</em></p>
<p>This meeting is in-person-only to enable the conversations to be the most interactive. Please register to assist us in getting an accurate headcount. Pizza will be served.</p>
<h4 id="bouchet-expansion-coming-this-spring">Bouchet Expansion Coming This Spring</h4>
<p>The Bouchet cluster will be expanded later this spring to include NVIDIA GPUs and additional CPU resources. As part of the AI Initiative, we will introduce 80 H200 GPU cards, each with 140GB of VRAM and advanced networking capabilities. This upgrade will enable large-scale AI work to be performed on our computing systems.  Additionally, we will make available 48 A5000 ADA cards, each with 32GB of VRAM, for general-purpose workflows. Furthermore, 6,000 CPUs will enhance the existing resources for general computation, including several nodes equipped with 4TB of RAM.</p>
<p>Once these nodes are operational, we will grant access to the Bouchet cluster for researchers from all domains.</p>
<h4 id="using-nextflow-on-the-clusters">Using Nextflow on the clusters</h4>
<p>Nextflow is a widely used workflow tool with an active community and numerous prebuilt pipelines, especially in the field of bioinformatics. You can learn more and access training, documentation, and examples at [Nextflow webpage](https://www.nextflow.io/.</p>
<p>Our team has developed specific guidelines to help our users run Nextflow pipelines on YCRC's clusters most efficiently. For more information please visit <a href="https://docs.ycrc.yale.edu/clusters-at-yale/guides/nextflow">docs.ycrc.yale.edu/clusters-at-yale/guides/nextflow</a>.</p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong>Mathematica/14.2.0</strong> now available on Grace and McCleary</li>
<li><strong>AlphaFold/3.0.1</strong> now available on Grace and McCleary</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: April 01, 2025
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="march-2025">March 2025</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="bouchet-open-ondemand">Bouchet Open OnDemand</h4>
<p>We are excited to announce that Bouchets Open OnDemand web portal is now live! Currently, Remote Desktop and Jupyter applications are available, and we are working to expand this list in the coming months. Please visit <a href="https://ood-bouchet.ycrc.yale.edu">ood-bouchet.ycrc.yale.edu</a> (VPN login required) and email hpc@yale.edu if you would like to request specific applications that would enhance your workflows.</p>
<h4 id="llm-user-documentation">LLM User Documentation</h4>
<p>The YCRC has released instructions for installing and using HuggingFace and Ollama to conduct LLM research on research computing hardware. You may find them by visiting <a href="https://docs.ycrc.yale.edu/clusters-at-yale/guides/LLMs/">docs.ycrc.yale.edu/clusters-at-yale/guides/LLMs/</a>. These instructions cover both interactive sessions, such as Jupyter notebooks, and batch submissions for larger jobs.</p>
<p>This documentation is the first in a series planned for this year that will focus on AI and machine learning. For additional information and user guidance, please contact hpc@yale.edu.</p>
<h4 id="ollama-module-available">Ollama Module Available</h4>
<p>A module has been installed on McCleary, Grace, and Milgram clusters to facilitate easy access to various LLM models through the Ollama program.
With Ollama, researchers can download and run popular LLMs offline, which enhances the protection of their data.</p>
<p>For instructions on how to run the module, please refer to our documentation page located at <a href="https://docs.ycrc.yale.edu/clusters-at-yale/guides/LLMs/">docs.ycrc.yale.edu/clusters-at-yale/guides/LLMs/</a>.</p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li>The latest <strong>RELION</strong> version 5.0.0 cryo-EM image-processing package is now available on McCleary and Grace.</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: March 01, 2025
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="february-2025">February 2025</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="bouchet-is-now-in-production">Bouchet is Now in Production</h4>
<p>We are excited to announce that the beta testing period for the <a href="https://docs.ycrc.yale.edu/clusters/bouchet/">Bouchet</a> cluster has concluded, and the cluster is now officially in production. This marks our first cluster at the <a href="https://www.mghpcc.org/">Massachusetts Green High Performance Computing Center</a> and signifies an important milestone in our transition to a purpose-built data center for academic research computing.</p>
<p>Currently, the cluster is limited to tightly coupled parallel workflows. However, we are actively working on acquiring general-purpose compute and GPU nodes, which will expand access to all Yale researchers whose computational work involves low-risk data. We will provide more information about the timeline for availability of these nodes soon.</p>
<h4 id="milgram-maintenance-tuesday-february-4th-2025">Milgram Maintenance - Tuesday, February 4th, 2025</h4>
<p>Due to the small number of updates needed on Milgram, the upcoming February maintenance will result in limited disruptions. The Milgram cluster and storage will remain online and available throughout the maintenance period, and there will be no disruption to running or pending batch jobs. However, certain services will be unavailable for short periods during the maintenance window. Users might experience temporarily increased wait times, as maintenance activities will, at times, reduce the availability of compute nodes.</p>
<p>Please refer to the Maintenance Announcement email sent on January 14, 2025 for more details.</p>
    </div>
    <div class="blog-post-extra">
        Published: February 01, 2025
    </div>
    <hr />
</div>

                
            
        </div>
    
        
        <div class="page" id="page3">

            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="january-2025">January 2025</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="bouchet-beta">Bouchet Beta</h4>
<p>The YCRCs first installation at MGHPCC, Bouchet, is currently available for beta testing. For the beta period, we are explicitly and exclusively seeking tightly coupled, parallel workloads. In early 2025, we will be acquiring and installing a large number of general purpose compute nodes as well as GPU-enabled compute nodes. At that point Bouchet will be available to all Yale researchers for computational work involving low-risk data. If you have a suitable workload we encourage you to <a href="https://docs.google.com/forms/d/e/1FAIpQLScjJYSkpyu-UqGH09Tgp2HS3xQ0NFjLOdg6_pRAMDKr4uiU0w/viewform">request access</a> to participate in beta testing.</p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong>MATLAB/2023b now available on Grace and McCleary</strong></li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: January 01, 2025
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="december-2024">December 2024</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="new-ycrc-hpc-compute-charging-model">New YCRC HPC Compute Charging Model</h4>
<p>Effective December 1st 2024, the current YCRC CPU-Hour based service charges have been replaced with new Priority Tier service charges. The YCRC has added a new Priority Tier of partitions that is an opt-in, fast lane for computational jobs. All computation on the standard tier of partitions (e.g. day, week, mpi, gpu) no longer incur charges. Private nodes and scavenge partitions continue to not incur charges. Visit <a href="https://docs.ycrc.yale.edu/clusters-at-yale/job-scheduling/priority-tier/">Priority Tier documentation</a> for more information and to request access. The new compute charging model was developed in close collaboration with faculty, YCRC staff, and university administrators to ensure the YCRC service charging models support the researchers relying on our systems as well as the needs of the University.</p>
<h4 id="upcoming-grace-maintenance">Upcoming Grace Maintenance</h4>
<p>Due to the limited updates needed on Grace at this time, the current maintenance period (Dec 3-Dec 4) will bring forth only limited disruptions to our services. The Grace cluster and storage will remain online and available throughout the maintenance period and there will be no disruption to running or pending batch jobs. However, certain services will be unavailable for short periods during the maintenance window.  The availability of compute nodes will be reduced at times, so users might experience temporarily increased wait times. Please refer to the Maintenance Announcement email for more details.</p>
    </div>
    <div class="blog-post-extra">
        Published: December 01, 2024
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="november-2024">November 2024</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="ycrc-welcomes-ruth-marinshaw-as-the-new-executive-director">YCRC Welcomes Ruth Marinshaw as the New Executive Director</h4>
<p>YCRC Team is delighted to welcome Ruth Marinshaw as our new Executive Director. Ruth joined Yale this week to serve as the university's primary technologist to support the computing needs of Yale's research community. She will work with colleagues across campus to implement and operate computational technologies that support Yale faculty, students, and staff in conducting cutting-edge research.</p>
<p>Ruth brings to Yale over twenty years of experience leading technology and research services in higher education. Ruth had held multiple positions at The University of North Carolina Chapel Hill, overseeing research computing services, staff, and systems and creating new service partnerships across the university. Over the last twelve years, Ruth served as chief technology officer for research computing at Stanford University. Under Ruth's leadership, Stanford's research computing and cyberinfrastructure services, systems, facilities, and support have grown significantly. She developed a team of research computing professionals and forged critical partnerships across the institution. Ruth was pivotal in establishing an NVIDIA SuperPOD - a data science and AI-focused research instrument envisioned by Stanfords faculty.</p>
<p>Throughout her career, Ruth has contributed expertise to national conversations on research computing and currently serves as co-chair of the National Science Foundation's Advisory Committee for Cyberinfrastructure.</p>
<p>With exciting initiatives on the horizon, we look forward to Ruth's leadership and guidance in supporting Yale's research community's current and emerging needs. Welcome aboard, Ruth!</p>
<p><a href="https://research.yale.edu/announcements/welcoming-ruth-marinshaw-executive-director-research-computing-technologies">Read the announcement from John Barden, Vice President for Information Technology and Campus Services, and Michael Crair, Vice Provost for Research.</a></p>
<h4 id="bouchet-beta-testing-for-tightly-coupled-parallel-workflows">Bouchet Beta Testing for Tightly Coupled Parallel Workflows</h4>
<p>The YCRCs first installation at Massachusetts Green High Performance Computing Center will be the HPC cluster Bouchet*.</p>
<p>The first installation of nodes, approximately 4,000 direct-liquid-cooled cores, will be dedicated to tightly coupled parallel workflows, such as those run in the mpi partition on the Grace cluster.</p>
<p>We would like to invite you to participate in Bouchet beta testing. We are seeking tightly coupled, parallel workloads only to participate in this phase of development. If you have a suitable parallel workload and would like to participate in testing, please complete the <a href="https://docs.google.com/forms/d/e/1FAIpQLScjJYSkpyu-UqGH09Tgp2HS3xQ0NFjLOdg6_pRAMDKr4uiU0w/viewform?usp=send_form">Bouchet Beta Request Form</a> and we will contact you with additional information about accessing and using Bouchet.</p>
<p>Following the beta testing (early 2025), we will be acquiring and installing thousands of general purpose compute cores as well as GPU-enabled compute nodes. At that time Bouchet will become available to all Yale researchers for computational work involving low-risk data. Stay tuned for more information on availability of the cluster. </p>
<p>*The cluster named after Edward Bouchet (1852-1918) who earned a PhD in physics at Yale University in 1876, making him the first self-identified African American to earn a doctorate from an American university. </p>
    </div>
    <div class="blog-post-extra">
        Published: November 01, 2024
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="october-2024">October 2024</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="announcing-bouchet">Announcing Bouchet</h4>
<p>The Bouchet HPC cluster, YCRC's first installation at MGHPCC, will be in beta in Fall 2024. The first installation of nodes, approximately 4,000 direct-liquid-cooled cores, will be dedicated to tightly coupled parallel workflows, such as those run in the mpi partition on the Grace cluster. Later this year, we will acquire and install a large number of general-purpose compute nodes and GPU-enabled compute nodes. At that point, Bouchet will be available to all Yale researchers for computational work involving low-risk data.  Visit the <a href="/clusters/bouchet">Bouchet page</a> for more information and updates.</p>
<h4 id="jobstats-on-the-web">Jobstats on the Web</h4>
<p>In our quest to provide detailed information about job performance and efficiency, we have recently enhanced the web-based jobstats portal to show summary statistics and plots of CPU, Memory, GPU, and GPU memory usage over time. </p>
<p>These plots are helpful diagnostics for understanding why jobs fail or how to more efficiently request resources. These plots and statistics are available for in-progress jobs, a great way to keep track of performance while jobs are still running. This tool is part of the User Portal which can be accessed via Open OnDemand for each cluster:</p>
<ul>
<li><a href="https://ood-grace.ycrc.yale.edu/pun/sys/ycrc_userportal">Grace</a></li>
<li><a href="https://ood-mccleary.ycrc.yale.edu/pun/sys/ycrc_userportal">McCleary</a></li>
<li><a href="https://ood-milgram.ycrc.yale.edu/pun/sys/ycrc_userportal">Milgram</a></li>
</ul>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong>GROMACS/2023.3-foss-2022b-CUDA-12.1.1-PLUMED-2.9.2</strong> is now available on Grace and McCleary</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: October 01, 2024
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="september-2024">September 2024</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="the-ycrc-is-hiring">The YCRC is Hiring!</h4>
<p>The YCRC is looking to add permanent members to our Research Support team. If helping others use the clusters and learning about other work done at the YCRC interests you, consider joining the YCRC! If you have any questions about the position, contact Kaylea Nelson (<a href="mailto:kaylea.nelson@yale.edu">kaylea.nelson@yale.edu</a>).</p>
<p><a href="https://research.computing.yale.edu/about/careers">https://research.computing.yale.edu/about/careers</a></p>
<h4 id="clarity-access">Clarity Access</h4>
<p>Yale is launching the <a href="https://ai.yale.edu/yales-ai-tools-and-resources/clarity-platform">Clarity platform</a>. In its initial phase, Clarity offers an AI chatbot powered by OpenAIs ChatGPT-4o. Importantly, Clarity provides a walled-off environment; its use is limited to Yale faculty, students, and staff, and information entered into its chatbot is not saved or used to train external AI models. Clarity is appropriate for use with all data types, including [high-risk data](https://your.yale.edu/policies-procedures/policies/1604-data-classification-policy, provided that <a href="https://cybersecurity.yale.edu/mss">all security standards</a> are observed. Its chatbot is capable of content creation, coding assistance, data and image analysis, text-to-speech, and more. Over time, the platform may expand to incorporate additional AI tools, including other <a href="https://ai.yale.edu/guidance/learn-about-ai#keyterms">large language models</a>. Clarity is designed to evolve as generative AI develops and the community offers feedback. </p>
<p><em>Before using the Clarity AI chatbot, please review <a href="https://ai.yale.edu/yales-ai-tools-and-resources/clarity-platform#training">training resources</a> and <a href="https://ai.yale.edu/yales-ai-tools-and-resources/clarity-platform/clarity-platform-faqs">guidance</a> on appropriate use.</em></p>
<h4 id="job-performance-monitoring">Job Performance Monitoring</h4>
<p>We have recently deployed a new tool for measuring and monitoring job performance called <code>jobstats</code>. Available on all clusters, <code>jobstats</code> provides a report of the utilization of CPU, Memory, and GPU resources for in-progress and recently completed jobs. To generate the report simply run (replacing the ID number of the job in question):</p>
<div class="highlight"><pre><span></span><code>[ab123@grace ~]$ jobstats 123456789

======================================================================
                         Slurm Job Statistics
======================================================================
         Job ID: 123456789
  NetID/Account: ab123/agroup
       Job Name: my_job
          State: RUNNING
          Nodes: 1
      CPU Cores: 4
     CPU Memory: 256GB (64GB per CPU-core)
  QOS/Partition: normal/week
        Cluster: grace
     Start Time: Thu Sep 5, 2024 at 10:58 AM
       Run Time: 1-06:43:41 (in progress)
     Time Limit: 4-04:00:00

                         Overall Utilization
======================================================================
  CPU utilization  [|||||||||||||                            26%]
  CPU memory usage [||||                                      8%]

                         Detailed Utilization
======================================================================
  CPU utilization per node (CPU time used/run time)
      r816u29n04: 1-07:48:36/5-02:54:45 (efficiency=25.9%)

  CPU memory usage per node - used/allocated
      r816u29n04: 19.9GB/256.0GB (5.0GB/64.0GB per core of 4)
</code></pre></div>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong>R/4.4.1-foss-2022b</strong> is now available on all clusters</li>
<li><strong>R-bundle-Bioconductor/3.19-foss-2022b-R-4.4.1</strong> (INCLUDES SEURAT) is now available on all clusters</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: September 01, 2024
    </div>
    <hr />
</div>

                
            
        </div>
    
        
        <div class="page" id="page4">

            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="milgram-maintenance">Milgram Maintenance</h2>
<p><em>August 20-22, 2024</em></p>
<h3 id="software-updates">Software Updates</h3>
<ul>
<li>Slurm updated to 23.11.9</li>
<li>NVIDIA drivers updated to 555.42.06</li>
<li>Apptainer updated to 1.3.3</li>
</ul>
<h3 id="hardware-updates">Hardware Updates</h3>
<ul>
<li>We proactively replaced two core file servers, as part of our best practices for GPFS (our high-performance parallel filesystem).  This does not change our storage capacity, but allows us to maintain existing services and access future upgrades.</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: August 22, 2024
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="august-2024">August 2024</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="milgram-maintenance">Milgram Maintenance</h4>
<p>The biannual scheduled maintenance for the Milgram cluster will be occurring August 20-22. During this time, the cluster will be unavailable. See the Milgram maintenance email announcements for more details.</p>
<h4 id="ycrc-hpc-user-portal">YCRC HPC User Portal</h4>
<p>We have recently deployed a web-based User Portal for Grace, McCleary, and Milgram clusters to help researchers view information about their activity on our clusters. Accessible under the Utilities tab on Open OnDemand (or at the links below), the portal currently features five pages with personalized data about your cluster usage and guidance on navigating and operating the clusters. Users can easily track their jobs and visualize their utilization through charts, tables, and graphs, helping to optimize their cluster use. To try out the User portal please visit either:</p>
<ul>
<li><a href="https://ood-grace.ycrc.yale.edu/pun/sys/ycrc_userportal">Grace</a></li>
<li><a href="https://ood-mccleary.ycrc.yale.edu/pun/sys/ycrc_userportal">McCleary</a></li>
<li><a href="https://ood-milgram.ycrc.yale.edu/pun/sys/ycrc_userportal">Milgram</a></li>
</ul>
<p>If you have any suggestions for useful pages for the User Portal, please email hpc@yale.edu</p>
<h4 id="yale-task-force-on-artificial-intelligence">Yale Task Force on Artificial Intelligence</h4>
<p>As recommended by the <a href="https://provost.yale.edu/news/report-yale-task-force-artificial-intelligence">Yale Task Force on Artificial Intelligence</a>, the YCRC will be adding a large number of new high-end GPUs to the clusters to meet growing demand for AI compute resources. Stay tuned for details and updates on availability in the coming months!</p>
<h4 id="research-support-at-pearc24">Research Support at PEARC24</h4>
<p>During the week of July 22nd, the YCRC Research Support team attended the annual <a href="https://pearc.acm.org/">PEARC conference</a>, a conference specifically focused on the research computing community. The team had the opportunity to meet with many of our peer institutions to discuss our common challenges and opportunities and attend sessions to learn about new solutions. We look forward to bringing some new ideas to the YCRC this year!</p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong>ORCA/6.0.0-gompi-2022b</strong> is now available on Grace and McCleary</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: August 01, 2024
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="july-2024">July 2024</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="compute-charges-rate-freeze">Compute Charges Rate Freeze</h4>
<p>The compute charging model for the YCRC clusters is currently under review.  As a result, we are freezing the per-CPU-hour charge at its current value of $0.0025, effective immediately. For more information on the compute charging model, please see the Billing for HPC services page on the YCRC website.</p>
<h4 id="matlab-proxy-server">MATLAB Proxy Server</h4>
<p>"MATLAB (Web)" is now available as an Open OnDemand app. A MATLAB session is connected directly to your web browser tab, rather than launched via a Remote Desktop session as with the traditional MATLAB app. This allows more of the requested resources to be dedicated to MATLAB itself.  Page through the full App list in Open OnDemand to launch. (Note that this is a work in progress that might not yet have all the functionality of a regular MATLAB session.)</p>
<h4 id="fairshare-weights-adjustment">FairShare Weights Adjustment</h4>
<p>Periodically we adjust the relative impact of resource allocations on a groups FairShare (the way that the scheduler determines which job gets scheduled next). We have adjusted the service unit weights for memory and GPUs to better match their cost to acquire and maintain:</p>
<div class="highlight"><pre><span></span><code>CPU: 1 SU
Memory: 0.067 (15G/SU)
A100 GPU: 100 SU
non-A100 GPU: 15 SU
</code></pre></div>
<p>For more information about FairShare and how we use it to ensure equity in scheduling, please visit <a href="https://docs.ycrc.yale.edu/clusters-at-yale/job-scheduling/fairshare/">our docs page</a>.</p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li>SBGrid is available on McCleary. <a href="/">Contact us</a> for more information on access.</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: July 01, 2024
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="grace-maintenance">Grace Maintenance</h2>
<p><em>June 4-6, 2024</em></p>
<h3 id="software-updates">Software Updates</h3>
<ul>
<li>Slurm updated to 23.11.7</li>
<li>NVIDIA drivers updated to 555.42.02</li>
<li>Apptainer updated to 1.3.1</li>
</ul>
<h3 id="hardware-updates">Hardware Updates</h3>
<ul>
<li>The remaining Broadwell generation nodes have been decommissioned.</li>
<li>The <code>oldest</code> node constraint now returns Cascade Lake generation nodes.</li>
<li>The <code>devel</code> partition is now composed of 5 Cascade Lake generation 6240 nodes and 1 Skylake generation (same as the mpi partition) node.</li>
<li>The FDR InfiniBand fabric has been fully decommissioned, and networking has been updated across the Grace cluster.</li>
<li>The Slayman storage system is no longer available from Grace (but remains accessible from McCleary).</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: June 06, 2024
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="june-2024">June 2024</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="grace-maintenance">Grace Maintenance</h4>
<p>The biannual scheduled maintenance for the Grace cluster will be occurring Jun 4-6. During this time, the cluster will be unavailable. See the Grace maintenance email announcements for more details. </p>
<h4 id="compute-usage-monitoring-in-web-portal">Compute Usage Monitoring in Web Portal</h4>
<p>We have developed a suite of tools to enable research groups to monitor their combined utilization of cluster resources. We perform nightly queries of Slurm's database to aggregate cluster usage (in cpu_hours) broken down by user, account, and partition. These data are available both as a command-line utility (getusage) and a recently deployed web-application built into Open OnDemand. This can be accessed directly via:</p>
<p>Grace: <a href="ood-grace.ycrc.yale.edu/pun/sys/ycrc_getusage">ood-grace.ycrc.yale.edu/pun/sys/ycrc_getusage</a>
McCleary: <a href="ood-mccleary.ycrc.yale.edu/pun/sys/ycrc_getusage">ood-mccleary.ycrc.yale.edu/pun/sys/ycrc_getusage</a>
Milgram: <a href="ood-milgram.ycrc.yale.edu/pun/sys/ycrc_getusage">ood-milgram.ycrc.yale.edu/pun/sys/ycrc_getusage</a></p>
<p>Additionally, the Getusage web-app can be accessed via the Utilities pull-down menu after logging into Open OnDemand.</p>
<h4 id="nairr-resources-for-ai">NAIRR Resources for AI</h4>
<p>Looking for compute resource for your AI or AI-enabled research? In the NAIRR Pilot, the US National Science Foundation (NSF), the US Department of Energy (DOE), and numerous other partners are providing access to a set of computing, model, platform and educational resources for projects related to advancing AI research. Applications for resources from the NAIRR pilot are lightweight and we are happy to assist with any questions you may have. </p>
<p><a href="https://nairrpilot.org/opportunities/allocations">https://nairrpilot.org/opportunities/allocations</a></p>
    </div>
    <div class="blog-post-extra">
        Published: June 01, 2024
    </div>
    <hr />
</div>

                
            
        </div>
    
        
        <div class="page" id="page5">

            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="may-2024">May 2024</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="yale-joins-mghpcc">Yale Joins MGHPCC</h4>
<p>We are excited to share that Yale University has recently become a member of the Massachusetts Green High Performance Computing Center (MGHPCC), a not-for-profit data center designed for computationally-intensive research. The construction of a dedicated space for Yale within the facility and the installation of high-speed networking between Yale's campus and MGHPCC are currently underway. The first High-Performance Computing (HPC) hardware installations are expected to take place later this year. As more information becomes available, we will keep our users updated</p>
<h4 id="oneit-conference">OneIT Conference</h4>
<p>Earlier this year Yale ITS hosted the first in-person One IT conference, [Advancing Collaborations: One IT as a Catalyst](https://your.yale.edu/news/2024/04/conference-edition-capturing-spirit-one-it. IT and IT-adjacent personnel from across campus came together to discuss topics that impact research and university operations. YCRC team members participated in a variety of sessions ranging from Research Storage and Software to the role of AI in higher education. </p>
<p>Additionally, YCRC team members presented two posters. The first, [A Graphical Interface for Research and Education](https://image.s10.sfmc-content.com/lib/fe4515707564047b751572/m/1/83cc1a22-d9d3-498d-8a17-8088de496674.pdf, highlighted Open OnDemand and its barrier-reducing impact on courses and research alike. The second, [Globus: a platform for secure, efficient file transfer](https://image.s10.sfmc-content.com/lib/fe4515707564047b751572/m/1/52559eed-25ab-49c9-80be-6ce99fb95b25.pdf, demonstrated our successful deployment of Globus to improve data management and cross-institutional sharing of research materials. </p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong>Spark</strong> is now available on Grace, McCleary and Milgram</li>
<li><strong>Nextflow/22.10.6</strong> is now available on Grace and McCleary</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: May 01, 2024
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="april-2024">April 2024</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="new-grace-nodes">New Grace Nodes</h4>
<p>We are pleased to announce the addition of 84 new direct-liquid-cooled compute nodes to the commons partitions (day and week) on Grace.  These new nodes are of the Intel Icelake generation and have 48 cores each. These nodes also have increased RAM compared to other nodes on Grace, with 10GB per core. The day partition is now close to 11,000 cores and the week partition is now entirely composed of these nodes. A significant number of purchased nodes of similar design have also been added to respective private partitions and are available to all users via the scavenge partition.</p>
<h4 id="limited-mccleary-maintenance-april-2nd">Limited McCleary Maintenance - April 2nd</h4>
<p>Due to the limited updates needed on McCleary at this time, the upcoming April maintenance will not be a full 3-day downtime, but rather a one-day maintenance with limited disruption.  The McCleary cluster and storage will remain online and available throughout the maintenance period and there will be no disruption to running or pending batch jobs.  However, certain services will be unavailable for short periods throughout the day.  See maintenance announcement email for full details.</p>
<h4 id="cluster-node-status-in-open-ondemand">Cluster Node Status in Open OnDemand</h4>
<p>A Cluster Node Status app is now available in the Open OnDemand web portal on all clusters. This new app presents information about CPU, GPU and memory utilization for each compute node with the cluster. The app can be found under Utilities -&gt; Cluster Node Status. </p>
<h4 id="retirement-of-grace-rhel7-apps-tree">Retirement of Grace RHEL7 Apps Tree</h4>
<p>As part of our routine deprecation of older software, we removed Grace's old application tree (from before the RedHat 8 upgrade) from the default Standard Environment on March 6th. After March 6th, the older module tree will no longer appear when <code>module avail</code> is run and will fail to load. If you have concerns about missing software, please contact us at hpc@yale.edu.</p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong>R/4.3.2-foss-2022b</strong> is now available on Grace and McCleary<ul>
<li>Corresponding Bioconductor and CRAN bundles are also now available</li>
</ul>
</li>
<li><strong>PyTorch/2.1.2-foss-2022b-CUDA-12.0.0</strong> with CUDA is available on Grace and McCleary</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: April 01, 2024
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="march-2024">March 2024</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="cpu-usage-reporting-with-getusage">CPU Usage Reporting with <code>getusage</code></h4>
<p>Researchers frequently wish to get a breakdown of their groups' cluster-usage. While Slurm provides tooling for querying the database, it is not particularly user-friendly. We have developed a tool, <code>getusage</code>, which allows researchers to quickly get insight into their groups usage, broken down by date and user, including a monthly summary report. Please try this tool and let us know if there are any enhancement requests or ideas.   </p>
<h4 id="changes-to-rstudio-on-the-web-portal">Changes to RStudio on the Web Portal</h4>
<p><a href="https://code.visualstudio.com/">Visual Studio Code</a> (VSCode) is a popular development tool that is widely used by our researchers. While there are several extensions that allow users to connect to remote servers over SSH, these are imperfect and often drops connection. Additionally, these remote sessions connect to the clusters' login nodes, where resources are limited. </p>
<p>To meet the growing demand for this particular development tool, we have deployed an application for Open OnDemand that launches VS Code Server directly on a compute node which can then be accessed via web-browser. This OOD application is called <code>code_server</code> and is available on all clusters. For more information see [our OOD docs page](https://docs.ycrc.yale.edu/clusters-at-yale/access/ood/.</p>
<h4 id="retirement-of-grace-rhel7-apps-tree">Retirement of Grace RHEL7 Apps Tree</h4>
<p>As part of our routine deprecation of older software we removed Grace's old application tree (from before the RedHat 8 upgrade) from the default Standard Environment on March 1st. After March 1st, the older module tree will no longer appear when <code>module avail</code> is run and will fail to load going forward. If you have concerns about any missing software, please contact us at hpc@yale.edu.</p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong>FSL/6.0.5.2-centos7_64</strong> is now available on Milgram</li>
<li><strong>Nextflow/23.10.1</strong> is now available on Grace &amp; McCleary</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: March 01, 2024
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="milgram-maintenance">Milgram Maintenance</h2>
<p><em>Februrary 6-8, 2024docs/news/2024-02-milgram.md</em></p>
<h3 id="software-updates">Software Updates</h3>
<ul>
<li>Slurm updated to 23.02.7</li>
<li>NVIDIA drivers updated to 545.23.08</li>
<li>Apptainer updated to 1.2.5</li>
<li>Lmod updated to 8.7.32</li>
</ul>
<h3 id="upgrade-to-red-hat-8">Upgrade to Red Hat 8</h3>
<p>As part of this maintenance, the operating system on Milgram has been upgraded to Red Hat 8.</p>
<p>Jobs submitted prior to maintenance that were held and now released will run under RHEL8 (instead of RHEL7).  This may cause some jobs to not run properly, so we encourage you to check on your job output.  Our <a href="/clusters/milgram_rhel8/">docs page</a> provides information on the RHEL8 upgrade, including fixes for common problems.  Please notify hpc@yale.edu if you require assistance. </p>
<h3 id="changes-to-interactive-partitions-and-jobs">Changes to Interactive Partitions and Jobs</h3>
<p>We have made the following changes to interactive jobs during the maintenance.  </p>
<p>The 'interactive' and 'psych_interactive` partitions have been renamed to 'devel' and 'psych_devel', respectively, to bring Milgram in alignment with the other clusters.  This change was made on other clusters in recognition that interactive-style jobs (such as OnDemand and 'salloc' jobs) are commonly run outside of the 'interactive' partition.  Please adjust your workflows accordingly after the maintenance.</p>
<p>Additionally, all users are limited to 4 interactive app instances (of any type) at one time.  Additional instances will be rejected until you delete older open instances.  For OnDemand jobs, closing the window does not terminate the interactive app job.  To terminate the job, click the "Delete" button in your "My Interactive Apps" page in the web portal.</p>
<p>Please visit the status page at <a href="http://research.computing.yale.edu/system-status">research.computing.yale.edu/system-status</a> for the latest updates.  If you have questions, comments, or concerns, please contact us at hpc@yale.edu.</p>
    </div>
    <div class="blog-post-extra">
        Published: February 08, 2024
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="february-2024">February 2024</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="milgram-maintenance">Milgram Maintenance</h4>
<p>The biannual scheduled maintenance for the Milgram cluster will be occurring Feb 6-8. During this time, the cluster will be unavailable. See the Milgram maintenance email announcements for more details. </p>
<h4 id="changes-to-rstudio-on-the-web-portal">Changes to RStudio on the Web Portal</h4>
<p>The RStudio Server app on the Open OnDemand web portal has been upgraded to support both the R software modules and R installed via conda. As such the RStudio Desktop app has been retired and removed from the web portal. If you still require RStudio Desktop, we provide <a href="https://docs.ycrc.yale.edu/clusters-at-yale/access/ood/#start-rstudio-desktop-in-remote-desktop">instructions for running under the Remote Desktop app</a> (please note that this is not a recommended practice for most users).</p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li>ChimeraX is now available as an app on the McCleary Open OnDemand web portal</li>
<li><strong>FSL/6.0.5.2-centos7_64</strong> is now available on McCleary</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: February 01, 2024
    </div>
    <hr />
</div>

                
            
        </div>
    
        
        <div class="page" id="page6">

            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="january-2024">January 2024</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="upcoming-milgram-rhel8-upgrade">Upcoming Milgram RHEL8 Upgrade</h4>
<p>As Red Hat Enterprise Linux (RHEL) 7 approaches its end of life, we will be upgrading the Milgram cluster from RHEL7 to RHEL8 during the February maintenance window.  This will bring Milgram in line with our other clusters and provide a number of key benefits:
continued security patches and support beyond 2024,
updated system libraries to better support modern software,
improved node management system</p>
<p>We have set aside rhel8_devel and rhel8_day partitions for use in debugging and testing of workflows before the February maintenance. For more information on testing your workflows <a href="https://docs.ycrc.yale.edu/clusters/milgram_rhel8/">see our explainer</a>.</p>
    </div>
    <div class="blog-post-extra">
        Published: January 01, 2024
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="grace-maintenance">Grace Maintenance</h2>
<p><em>December 5-7, 2023</em></p>
<h3 id="software-updates">Software Updates</h3>
<ul>
<li>Slurm updated to 23.02.6</li>
<li>NVIDIA drivers updated to 545.23.08</li>
<li>Lmod updated to 8.7.32</li>
<li>Apptainer updated to 1.2.4</li>
</ul>
<h3 id="multifactor-authentication-mfa">Multifactor Authentication (MFA)</h3>
<p><a href="https://docs.ycrc.yale.edu/clusters-at-yale/access/mfa/">Multi-Factor authentication via Duo</a> is now required for ssh for all users on Grace after the maintenance.  For most usage, this additional step is minimally invasive and makes our clusters much more secure. However, for users who use graphical transfer tools such as Cyberduck, please see <a href="https://docs.ycrc.yale.edu/data/transfer/#cyberduck-with-mfa">our MFA transfer documentation</a>.</p>
<h3 id="transfer-node-host-key-change">Transfer Node Host Key Change</h3>
<p>The ssh host key for Grace's transfer node was changed during the maintenance, which will result in a "WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!" error when you attempt to login. To access the cluster again, first remove the old host keys with the following command (if accessing the cluster via command line):</p>
<div class="highlight"><pre><span></span><code>ssh-keygen -R transfer-grace.ycrc.yale.edu
</code></pre></div>
<p>If you are using a GUI, such as MobaXterm, you will need to manually edit your known host file and remove the list related to Grace.
For MobaXterm, this file is located (by default) in <code>Documents/MobaXterm/home/.ssh</code>.</p>
<p>Then attempt a new login and accept the new host key.</p>
    </div>
    <div class="blog-post-extra">
        Published: December 07, 2023
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="december-2023">December 2023</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="grace-maintenance-multi-factor-authentication">Grace Maintenance - Multi-Factor Authentication</h4>
<p>The biannual scheduled maintenance for the Grace cluster will be occurring Dec 5-7. During this time, the cluster will be unavailable. See the Grace maintenance email announcements for more details.</p>
<p><a href="https://docs.ycrc.yale.edu/clusters-at-yale/access/mfa/">Multi-Factor authentication via Duo</a> will be required for ssh for all users on Grace after the maintenance.  For most usage, this additional step is minimally invasive and makes our clusters much more secure. However, for users who use graphical transfer tools such as Cyberduck, please see our <a href="https://docs.ycrc.yale.edu/data/transfer/#cyberduck-on-mccleary-and-milgram">MFA transfer documentation</a>.</p>
<h4 id="scavenge_gpu-and-scavenge_mpi"><code>scavenge_gpu</code> and <code>scavenge_mpi</code></h4>
<p>In addition to the general purpose scavenge partition, we also have two resource specific scavenge partitions, <code>scavenge_gpu</code> (Grace, McCleary) and <code>scavenge_mpi</code> (Grace only). The <code>scavenge_gpu</code> partition contains all GPU enabled nodes, commons and privately owned. Similarly, the <code>scavenge_mpi</code> partition contains all nodes similar to the <code>mpi</code> partition. Both partitions have higher priority for their respective nodes than normal scavenge (meaning jobs submitted to <code>scavenge_gpu</code> or <code>scavenge_mpi</code> will preempt normal scavenge jobs). All scavenge partitions are exempt from CPU charges.</p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong>IMOD/4.12.56_RHEL7-64_CUDA12.1</strong> is now available on McCleary and Grace</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: December 01, 2023
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="november-2023">November 2023</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="globus-available-on-milgram">Globus Available on Milgram</h4>
<p>Globus is now available to move data in and out from Milgram. For increased security, Globus only has access to a staging directory (<code>/gpfs/milgram/globus/$NETID</code>) where you can temporarily store data. Please see <a href="https://docs.ycrc.yale.edu/data/globus/">our documentation page</a> for more information and reach out to <a href="mailto:hpc@yale.edu">hpc@yale.edu</a> if you have any questions.</p>
<h4 id="rstudio-server-updates">RStudio Server Updates</h4>
<p>RStudio Server on the OpenDemand web portal for all clusters now starts an R session in a clean environment and will not save the session when you finish. If you want to save your session and reuse it next time, please select the checkbox "Start R from your last saved session".</p>
    </div>
    <div class="blog-post-extra">
        Published: November 01, 2023
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="mccleary-maintenance">McCleary Maintenance</h2>
<p><em>October</em> 3-5, 2023_</p>
<h3 id="software-updates">Software Updates</h3>
<ul>
<li>Slurm updated to 23.02.5</li>
<li>NVIDIA drivers updated to 535.104.12</li>
<li>Lmod updated to 8.7.30</li>
<li>Apptainer updated to 1.2.3</li>
<li>System Python updated to 3.11</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: October 05, 2023
    </div>
    <hr />
</div>

                
            
        </div>
    
        
        <div class="page" id="page7">

            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="october-2023">October 2023</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="mccleary-maintenance">McCleary Maintenance</h4>
<p>The biannual scheduled maintenance for the McCleary cluster will be occurring Oct 3-5. During this time, the cluster will be unavailable. See the McCleary maintenance email announcements for more details.</p>
<h4 id="interactive-jobs-on-day-on-mccleary">Interactive jobs on <code>day</code> on McCleary</h4>
<p>Interactive jobs are now allowed to be run on the <code>day</code> partition on McCleary. Note you are still limited to 4 interactive-style jobs of any kind (salloc or OpenOnDemand) at one time. Additional instances will be rejected until you delete older open instances. For OnDemand jobs, closing the window does not terminate the interactive app job. To terminate the job, click the "Delete" button in your "My Interactive Apps" page in the web portal.</p>
<h4 id="papermill-for-jupyter-command-line-execution">"Papermill" for Jupyter Command-Line Execution</h4>
<p>Many scientific workflows start as interactive Jupyter notebooks, and our Open OnDemand portal has dramatically simplified deploying these notebooks on cluster resources.  However, the step from running notebooks interactively to running jobs as a batch script can be challenging and is often a barrier to migrating to using <code>sbatch</code> to run workflows non-interactively. </p>
<p>To help solve this problem, there are a handful of utilities that can execute a notebook as if you were manually hitting "shift-Enter" for each cell. Of note is <a href="https://papermill.readthedocs.io/en/latest/">Papermill</a> which provides a powerful set of tools to bridge between interactive and batch-mode computing.</p>
<p>To get started, install papermill into your Conda environments:</p>
<div class="highlight"><pre><span></span><code>module load miniconda
conda install papermill
</code></pre></div>
<p>Then you can simply evaluate a notebook, preserving figures and output inside the notebook, like this:</p>
<div class="highlight"><pre><span></span><code>papermill /path/to/notebook.ipynb
</code></pre></div>
<p>This can be run inside a batch job that might look like this:</p>
<div class="highlight"><pre><span></span><code>#!/bin/bash
#SBATCH -p day
#SBATCH -c 1
#SBATCH -t 6:00:00

module unload miniconda
conda activate my_env
papermill /path/to/notebook.ipynb
</code></pre></div>
<p>Variables can also be parameterized and passed in as command-line options so that you can run multiple copies simultaneously with different input variables. For more information see the [Papermill docs pages](https://papermill.readthedocs.io/.</p>
    </div>
    <div class="blog-post-extra">
        Published: October 01, 2023
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="september-2023">September 2023</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="grace-rhel8-upgrade">Grace RHEL8 Upgrade</h4>
<p>As Red Hat Enterprise Linux (RHEL) 7 approaches its end of life, we upgraded the Grace cluster from RHEL7 to RHEL8 during the August maintenance window.  This brings Grace in line with McCleary and provide a number of key benefits:</p>
<ul>
<li>continued security patches and support beyond 2023</li>
<li>updated system libraries to better support modern software</li>
<li>improved node management system to facilitate the growing number of nodes on Grace</li>
<li>shared application tree between McCleary and Grace, which brings software parity between clusters</li>
</ul>
<p>There are a small number of compute nodes in the <code>legacy</code> partition with the old RHEL7 operating system installed for workloads that still need to be migrated. We expect to retire this partition during the Grace December 2023 maintenance. Please contact us if you need help upgrading to RHEL8 in the coming months.</p>
<h4 id="grace-old-software-deprecation">Grace Old Software Deprecation</h4>
<p>The RHEL7 application module tree (<code>/gpfs/loomis/apps/avx</code>) is now deprecated and will be removed from the default module environment during the Grace December maintenance. The software will still be available on Grace, but YCRC will no longer provide support for those old packages after December. If you are using a software package in that tree that is not yet installed into the new shared module tree, please let us know as soon as possible so we can help avoid any disruptions.</p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li>
<p><strong>intel/2022b</strong> toolchain is now available on Grace and McCleary</p>
<ul>
<li>MKL 2022.2.1</li>
<li>Intel MPI 2022.2.1</li>
<li>Intel Compilers 2022.2.1</li>
</ul>
</li>
<li>
<p><strong>foss/2022b</strong> toolchain is now available on Grace and McCleary</p>
<ul>
<li>FFTW 3.3.10</li>
<li>ScaLAPACK 2.2.0</li>
<li>OpenMPI 4.1.4</li>
<li>GCC 12.2.0</li>
</ul>
</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: September 01, 2023
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="milgram-maintenance">Milgram Maintenance</h2>
<p><em>August</em> 22, 2023_</p>
<h3 id="software-updates">Software Updates</h3>
<ul>
<li>Slurm updated to 22.05.9</li>
<li>NVIDIA drivers updated to 535.86.10</li>
<li>Apptainer updated to 1.2.42</li>
<li>Open OnDemand updated to 2.0.32</li>
</ul>
<h3 id="multi-factor-authentication">Multi-Factor Authentication</h3>
<p><a href="https://docs.ycrc.yale.edu/clusters-at-yale/access/mfa/">Multi-factor authentication</a> is now required for ssh for all users on Milgram.  For most usage, this additional step is minimally invasive and makes our clusters much more secure.  However, for users who use graphical transfer tools such as Cyberduck, please see our <a href="https://docs.ycrc.yale.edu/data/transfer/#cyberduck-on-mccleary-and-ruddle">MFA transfer documentation</a>.</p>
    </div>
    <div class="blog-post-extra">
        Published: August 22, 2023
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="grace-maintenance">Grace Maintenance</h2>
<p><em>August 15-17, 2023</em></p>
<h3 id="software-updates">Software Updates</h3>
<ul>
<li>Red Hat Enterprise Linux (RHEL) updated to 8.8</li>
<li>Slurm updated to 22.05.9</li>
<li>NVIDIA drivers updated to 535.86.10</li>
<li>Apptainer updated to 1.2.2</li>
<li>Open OnDemand updated to 2.0.32</li>
</ul>
<h3 id="upgrade-to-red-hat-8">Upgrade to Red Hat 8</h3>
<p>As part of this maintenance, the operating system on Grace has been upgraded to Red Hat 8.  A new unified software tree that is shared with the McCleary cluster has been created.</p>
<p>The ssh host keys for Grace's login nodes were changed during the maintenance, which will result in a "WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!" error when you attempt to login. To access the cluster again, first remove the old host keys with the following command (if accessing the cluster via command line):</p>
<div class="highlight"><pre><span></span><code>ssh-keygen -R grace.hpc.yale.edu
</code></pre></div>
<p>If you are using a GUI, such as MobaXterm, you will need to manually edit your known host file and remove the list related to Grace.
For MobaXterm, this file is located (by default) in <code>Documents/MobaXterm/home/.ssh</code>.</p>
<p>Then attempt a new login and accept the new host key.</p>
<h3 id="new-open-ondemand-web-portal-url">New Open OnDemand (Web Portal) URL</h3>
<p>The new URL for the Grace Open OnDemand web portal is <a href="https://ood-grace.ycrc.yale.edu">https://ood-grace.ycrc.yale.edu</a>.</p>
    </div>
    <div class="blog-post-extra">
        Published: August 17, 2023
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="august-2023">August 2023</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="ruddle-farewell-july-24-2023">Ruddle Farewell: July 24, 2023</h4>
<p>On the occasion of decommissioning the Ruddle cluster on July 24, the Yale Center for Genome Analysis (YCGA) and the Yale Center for Research Computing (YCRC) would like to acknowledge the profound impact Ruddle has had on computing at Yale. Ruddle provided the compute resources for YCGA's high throughput sequencing and supported genomic computing for hundreds of research groups at YSM and across the University. In February 2016, Ruddle replaced the previous biomedical cluster BulldogN. Since then, it has run more than 24 million user jobs comprising more than 73 million compute hours.</p>
<p>Funding for Ruddle came from NIH grant 1S10OD018521-01, with Shrikant Mane as PI. Ruddle is replaced by a dedicated partition and storage on the new McCleary cluster, which were funded by NIH grant 1S10OD030363-01A1, also awarded to Dr. Mane.</p>
<h4 id="upcoming-grace-maintenance-august-15-17-2023">Upcoming Grace Maintenance: August 15-17, 2023</h4>
<p>Scheduled maintenance will be performed on the Grace cluster starting on Tuesday, August 15, 2023, at 8:00 am. Maintenance is expected to be completed by the end of day, Thursday, August 17, 2023.</p>
<h4 id="upcoming-milgram-maintenance-august-22-24-2023">Upcoming Milgram Maintenance: August 22-24, 2023</h4>
<p>Scheduled maintenance will be performed on the Milgram cluster starting on Tuesday, August 22, 2023, at 8:00 am. Maintenance is expected to be completed by the end of day, Thursday, August 24, 2023.</p>
<h4 id="grace-operating-system-upgrade">Grace Operating System Upgrade</h4>
<p>As Red Hat Enterprise Linux (RHEL) 7 approaches its end of life, we will be upgrading the Grace cluster from RHEL7 to RHEL8 during the August maintenance window. This will bring Grace in line with McCleary and provide a number of key benefits:</p>
<ul>
<li>continued security patches and support beyond 2023</li>
<li>updated system libraries to better support modern software</li>
<li>improved node management system to facilitate the growing number of nodes on Grace</li>
<li>shared application tree between McCleary and Grace, which brings software parity between clusters</li>
</ul>
<p>Three test partitions are available (<code>rhel8_day</code>, <code>rhel8_gpu</code>, and <code>rhel8_mpi</code>) for use in debugging workflows before the upgrade. These partitions should be accessed from the <code>rhel8_login</code> node.</p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong>Julia/1.9.2-linux-x86_64</strong> available on Grace</li>
<li><strong>Kraken2/2.1.3-gompi-2020b</strong> available on McCleary</li>
<li><strong>QuantumESPRESSO/7.0-intel-2020b</strong> available on Grace</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: August 01, 2023
    </div>
    <hr />
</div>

                
            
        </div>
    
        
        <div class="page" id="page8">

            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="july-2023">July 2023</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="red-hat-8-test-partitions-on-grace">Red Hat 8 Test partitions on Grace</h4>
<p>As Red Hat Enterprise Linux (RHEL) 7 approaches its end of life, we will be upgrading the Grace cluster to RHEL8 during the August 15th-17th maintenance. This will bring Grace in line with McCleary and provide a number of key benefits:</p>
<ul>
<li>continued security patches and support beyond 2023</li>
<li>updated system libraries to better support modern software</li>
<li>improved node management system to facilitate the growing number of nodes on Grace</li>
<li>shared application tree between McCleary and Grace, which brings software parity between clusters</li>
</ul>
<p>While we have performed extensive testing, both internally and with the new McCleary cluster, we recognize that there are large numbers of custom workflows on Grace that may need to be modified to work with the new operating system.</p>
<p>Please note:  To enable debugging and testing of workflows ahead of the scheduled maintenance, we have set aside <code>rhel8_day</code>, <code>rhel8_gpu</code>, and <code>rhel8_mpi</code> partitions. You should access them from the <code>rhel8_login</code> node.</p>
<h4 id="two-factor-authentication-for-mccleary">Two-factor Authentication for McCleary</h4>
<p>To assure the security of the cluster and associated services, we have implemented two-factor authentication on the McCleary cluster. To simplify the transition, we have collected a set of best-practices and configurations of many of the commonly used access tools, including CyberDuck, MobaXTerm, and WinSCPon, which you can access on <a href="/clusters-at-yale/access/mfa/">our docs page</a>. If you are using other tools and experiencing issues, please <a href="/">contact us</a> for assistance.  </p>
<h4 id="new-gpu-nodes-on-mccleary-and-grace">New GPU Nodes on McCleary and Grace</h4>
<p>We have installed new GPU nodes for McCleary and Grace, dramatically increasing the number of GPUs available on both clusters. McCleary has 14 new nodes (56 GPUs) added to the gpu partition and six nodes (24 GPUs) added to <code>pi_cryoem</code>.  Grace has 12 new nodes, available in the <code>rhel8_gpu</code> partition. Each of the new nodes contains 4 <a href="https://www.nvidia.com/en-us/design-visualization/rtx-a5000/">NVIDIA A5000 GPUs</a>, with 24GB of on-board VRAM and PCIe4 connection to improve data-transport time.</p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong>MATLAB/2023a</strong> available on all clusters</li>
<li><strong>Beast/2.7.4-GCC-12.2.0</strong> available on McCleary</li>
<li><strong>AFNI/2023.1.07-foss-2020b</strong> available on McCleary</li>
<li><strong>FSL 6.0.5.1</strong> (CPU-only and GPU-enabled versions) available on McCleary</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: July 01, 2023
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="june-2023">June 2023</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="mccleary-officially-launches">McCleary Officially Launches</h4>
<p>Today marks the official beginning of the McCleary clusters service. In addition to compute nodes migrated from Farnam and Ruddle, McCleary features our first set of direct-to-chip liquid cooled (DLC) nodes, moving YCRC into a more environmentally friendly future. McCleary is significantly larger than the Farnam and Ruddle clusters combined. The new DLC compute nodes are able to run faster and with higher CPU density due to their superior cooling system.</p>
<p>McCleary is named for Beatrix McCleary Hamburg, who received her medical degree in 1948 and was the first female African American graduate of Yale School of Medicine. </p>
<h4 id="farnam-farewell-june-1-2023">Farnam Farewell: June 1, 2023</h4>
<p>On the occasion of decommissioning the Farnam cluster on June 1, YCRC would like to acknowledge the profound impact Farnam has had on computing at Yale. Farnam supported biomedical computing at YSM and across the University providing compute resources to hundreds of research groups. Farnam replaced the previous biomedical cluster Louise, and began production in October 2016. Since then, it has run user jobs comprising more than 139 million compute hours. Farnam is replaced by the new cluster McCleary. </p>
<p>Please note: Read-only access to Farnams storage system (/gpfs/ysm) will be available on McCleary until July 13, 2023. For more information see <a href="/data/mccleary-transfer/">McCleary transfer documentation</a>.</p>
<h3 id="ruddle-decommission-july-1-2023">Ruddle Decommission: July 1, 2023</h3>
<p>The Ruddle cluster will be decommissioned and access will be disabled July 1, 2023. We will be migrating project and sequencing directories from Ruddle to McCleary.</p>
<p>Please note: Users are responsible for moving home and scratch data to McCleary prior to July 1, 2023. For more information and instructions, see our <a href="/data/mccleary-transfer/">McCleary transfer documentation</a>.</p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong> R/4.3.0-foss-2020b+</strong> available on all clusters. The newest version of R is now available on Grace, McCleary, and Milgram. This updates nearly 1000 packages and can be used in batch jobs and in RStudio sessions via Open OnDemand. </li>
<li><strong>AlphaFold/2.3.2-foss-2020b-CUDA-11.3.1</strong> The latest version of AlphaFold (2.3.2, released in April) has been installed on McCleary and is ready for use. This version fixes a number of bugs and should improve GPU memory usage enabling longer proteins to be studied. </li>
<li><strong>LAMMPS/23Jun2022-foss-2020b-kokkos</strong> available on McCleary</li>
<li><strong>RevBayes/1.2.1-GCC-10.2.0</strong> available on McCleary</li>
<li><strong>Spark 3.1.1</strong> (CPU-only and GPU-enabled versions) available on McCleary</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: June 01, 2023
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="upcoming-maintenances">Upcoming Maintenances</h2>
<ul>
<li>
<p>The McCleary cluster will be unavailable from 9am-1pm on Tuesday May 30 while maintenance is performed on the YCGA storage.</p>
</li>
<li>
<p>The Milgram, Grace and McCleary clusters will not be available from 2pm on Monday June 19 until 10am on Wednesday June 21, due to electrical work being performed in the HPC data center.  No changes will be made that impact users of the clusters.</p>
</li>
<li>
<p>The regular Grace maintenance that had been scheduled for June 6-8 will be performed on August 15-17.  This change is being made in preparation for the upgrade to RHEL 8 on Grace.</p>
</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: May 23, 2023
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="may-2023">May 2023</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="farnam-decommission-june-1-2023">Farnam Decommission: June 1, 2023</h4>
<p>After many years of supporting productive science, the Farnam cluster will be decommissioned this summer as we transition to the newly deployed McCleary cluster. Logins will be disabled June 1, 2023, which will mark the official end of Farnams service. Read-only access to Farnams storage system (/gpfs/ysm) will be available on McCleary until July 13, 2023. All data on YSM (that you want to keep) will need to be transferred off YSM, either to non-HPC storage or to McCleary project space by you prior to YSMs retirement.</p>
<h4 id="ruddle-decommission-july-1-2023">Ruddle Decommission: July 1, 2023</h4>
<p>After many years of serving YCGA, the Ruddle cluster will also be decommissioned this summer as we transition to the newly deployed McCleary cluster. Logins will be disabled July 1, 2023, which will mark the official end of Ruddles service. We will be migrating project and sequencing directories from Ruddle to McCleary. However, you are responsible for moving home and scratch data to McCleary before July 1, 2023.  </p>
<p>Please begin to migrate your data and workloads to McCleary at your earliest convenience and <a href="/">reach out</a> with any questions.</p>
<h3 id="mccleary-transition-reminder">McCleary Transition Reminder</h3>
<p>With our McCleary cluster now in a production stable state, we ask all Farnam users to ensure all home, project and scratch data the group wishes to keep is migrated to the new cluster ahead of the June 1st decommission. As June 1st is the formal retirement of Farnam, compute service charges on McCleary commons partitions will begin at this time. Ruddle users will have until July 1st to access the Ruddle and migrate their home and scratch data as needed. Ruddle users will NOT need to migrate their project directories; those will be automatically transferred to McCleary. As previously established on Ruddle, all jobs in the YCGA partitions will be exempt from compute service charges on the new cluster. For more information visit <a href="https://docs.ycrc.yale.edu/clusters/mccleary-farnam-ruddle/">our McCleary Transition documentation</a>.</p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong>Libmamba solver for conda 23.1.0+</strong> available on all clusters. Conda installations 23.1.0 and newer are now configured to use the faster environment solving algorithm developed by <code>mamba</code> by default. You can simply use <code>conda install</code> and enjoy the significantly faster solve times.</li>
<li><strong>GSEA</strong> available in McCleary and Ruddle OOD. Gene Set Enrichment Analysis (GSEA) is now available in McCleary OOD and Ruddle OOD for all users. You can access it by clicking Interactive Apps'' and then selecting GSEA. GSEA is a popular computational method to do functional analysis of multi omics data. Data files for GSEA are not centrally stored on the clusters, so you will need to download them from the GSEA website by yourself.</li>
<li><strong>NAG/29-GCCcore-11.2.0</strong> available on Grace</li>
<li><strong>AFNI/2023.1.01-foss-2020b-Python-3.8.6</strong> on McCleary</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: May 01, 2023
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="april-2023">April 2023</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="mccleary-in-production-status">McCleary in Production Status</h4>
<p>During March, we have been adding nodes to McCleary, including large memory nodes (4 TiB), GPU nodes and migrating most of the commons nodes from Farnam to McCleary (that are not being retired). Moreover, we have finalized the setup of McCleary and the system is now production stable. Please feel comfortable to migrate your data and workloads from Farnam and Ruddle to McCleary at your earliest convenience.</p>
<h4 id="new-ycga-nodes-online-on-mccleary">New YCGA Nodes Online on McCleary</h4>
<p>McCleary now has over 3000 new cores dedicated to YCGA work!
We encourage you to test your workloads and prepare to migrate from Ruddle to McCleary at your earliest convenience. More information can be found <a href="https://docs.ycrc.yale.edu/clusters/mccleary/">here</a>. </p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong>QuantumESPRESSO/7.1-intel-2020b</strong> available on Grace</li>
<li><strong>RELION/4.0.1</strong> available on McCleary</li>
<li><strong>miniconda/23.1.0</strong> available on all clusters</li>
<li><strong>scikit-learn/0.23.2-foss-2020b</strong> on Grace and McCleary</li>
<li><strong>seff-array</strong> updated to 0.4 on Grace, McCleary and Milgram</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: April 01, 2023
    </div>
    <hr />
</div>

                
            
        </div>
    
        
        <div class="page" id="page9">

            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="march-2023">March 2023</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="mccleary-now-available">McCleary Now Available</h4>
<p>The <a href="/clusters/mccleary">new McCleary HPC cluster</a> is now available for active Farnam and Ruddle usersall other researchers who conduct life sciences research can request an account using our <a href="https://research.computing.yale.edu/support/hpc/account-request">Account Request form</a>. Farnam and Ruddle will be retired in mid-2023 so we encourage all users on those clusters to transition their work to McCleary at your earliest convenience. If you see any issues on the new cluster or have any questions, please let us know at <a href="mailto:hpc@yale.edu">hpc@yale.edu</a>.</p>
<h4 id="open-ondemand-vscode-available-everywhere">Open OnDemand VSCode Available Everywhere</h4>
<p>A new OOD app code-server is now available on all YCRC clusters, including Milgram and McCleary. The new code-server allows you to run VSCode in a browser on a compute node. All users who have been running VSCode on a login node via the ssh extension should switch to code-server at their earliest convenience. Unlike VSCode on the login node, the new app also enables you to use GPUs, to allocate large memory nodes, and to specify a private partition (if applicable) The app is still in beta version and your feedback is much appreciated.</p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li>GPU-enabled LAMMPS (<strong>LAMMPS/23Jun2022-foss-2020b-kokkos-CUDA-11.3.1</strong>) is now available on Grace.</li>
<li><strong>AlphaFold/2.3.1-fosscuda-2020b</strong> is now available on Farnam and McCleary.</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: March 01, 2023
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="milgram-maintenance">Milgram Maintenance</h2>
<p><em>February 7, 2023</em></p>
<h3 id="software-updates">Software Updates</h3>
<ul>
<li>Slurm updated to 22.05.7</li>
<li>NVIDIA drivers updated to 525.60.13</li>
<li>Apptainer updated to 1.1.4</li>
<li>Open OnDemand updated to 2.0.29</li>
</ul>
<h3 id="hardware-updates">Hardware Updates</h3>
<ul>
<li>Milgrams network was restructured to reduce latency, and improve resiliency.</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: February 07, 2023
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="february-2023">February 2023</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="milgram-maintenance">Milgram Maintenance</h4>
<p>The biannual scheduled maintenance for the Milgram cluster will be occurring Feb 7-9. During this time, the cluster will be unavailable. See the Milgram maintenance email announcements for more details.</p>
<h4 id="mccleary-launch">McCleary Launch</h4>
<p>The YCRC is pleased to announce the launch of the new McCleary HPC cluster.  The McCleary HPC cluster will be Yale's first direct-to-chip liquid cooled cluster, moving the YCRC and the Yale research computing community into a more environmentally friendly future. McCleary will be available in a beta phase to Farnam and Ruddle users later on this month. Keep an eye on your email for further announcements about McClearys availability.</p>
    </div>
    <div class="blog-post-extra">
        Published: February 01, 2023
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="january-2023">January 2023</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="open-ondemand-vscode">Open OnDemand VSCode</h4>
<p>A new OOD app code-server is now available on all clusters, except Milgram (coming in Feb). Code-server allows you to run VSCode in a browser on a compute node. All users who have been running VSCode on a login node via the ssh extension should switch to code-server immediately.  The app allows you to use GPUs, to allocate large memories, and to specify a private partition (if you have the access), things you wont be able to do if you run VSCode on a login node. The app is still in beta version and your feedback is much appreciated.</p>
<h4 id="milgram-transfer-node">Milgram Transfer Node</h4>
<p>Milgram now has a node dedicated to data transfers to and from the cluster. To access the node from within Milgram, run <code>ssh transfer</code> from the login node. To upload or download data from Milgram via the transfer node, use the hostname <code>transfer-milgram.hpc.yale.edu</code> (must be on VPN). More information can be found in our <a href="/data/transfer/">Transfer Data documentation</a>.</p>
<p>With the addition of the new transfer node, we ask that the login nodes are no longer used for data transfers to limit impact on regular login activities.</p>
    </div>
    <div class="blog-post-extra">
        Published: January 01, 2023
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="grace-maintenance">Grace Maintenance</h2>
<p><em>December 6-8, 2022</em></p>
<h3 id="software-updates">Software Updates</h3>
<ul>
<li>Slurm updated to 22.05.6</li>
<li>NVIDIA drivers updated to 520.61.05</li>
<li>Apptainer updated to 1.1.3 </li>
<li>Open OnDemand updated to 2.0.28</li>
</ul>
<h3 id="hardware-updates">Hardware Updates</h3>
<ul>
<li>Roughly 2 racks worth of equipment were moved to upgrade the effective InfiniBand connection speeds of several compute nodes (from 56 to 100 Gbps)</li>
<li>The InfiniBand network was modified to increase capacity and allow for additional growth</li>
<li>Some parts of the regular network were improved to shorten network paths and increase shared-uplink bandwidth</li>
</ul>
<h3 id="loomis-decommission">Loomis Decommission</h3>
<p>The Loomis GPFS filesystem has been retired and unmounted from Grace, Farnam, and Ruddle.  For additional information please see the <a href="https://docs.ycrc.yale.edu/data/loomis-decommission/">Loomis Decommission page</a>.</p>
    </div>
    <div class="blog-post-extra">
        Published: December 08, 2022
    </div>
    <hr />
</div>

                
            
        </div>
    
        
        <div class="page" id="page10">

            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="december-2022">December 2022</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="grace-gibbs-maintenance">Grace &amp; Gibbs Maintenance</h4>
<p>The biannual scheduled maintenance for the Grace cluster will be occurring December 6-8. During this time, the cluster will be unavailable. Additionally, the Gibbs filesystem will be unavailable on Farnam and Ruddle on Tuesday, December 6th to deploy a critical firmware upgrade. See the maintenance email announcements for more details.</p>
<h4 id="loomis-decommission">Loomis Decommission</h4>
<p>The Loomis GPFS filesystem will be retired and unmounted from Grace and Farnam during the Grace December maintenance starting on December 6th. All data except for a few remaining private filesets have already been transferred to other systems (e.g., current software, home, scratch to Palmer and project to Gibbs). The remaining private filesets are being transferred to Gibbs in advance of the maintenance and owners should have received communications accordingly. The only potential user impact of the retirement is on anyone using the older, deprecated software trees. Otherwise, the Loomis retirement should have no user impact but please reach out if you have any concerns or believe you are still using data located on Loomis. <a href="/data/loomis-decommission/">See the Loomis Decommission documentation for more information.</a> </p>
<h4 id="apptainer-upgrade-on-grace-and-ruddle">Apptainer Upgrade on Grace and Ruddle</h4>
<p>The newest version of Apptainer (v1.1, available now on Ruddle and, after December maintenance, on Grace) comes the ability to create containers without needing elevated privileges (i.e. <code>sudo</code> access). This greatly simplifies the container workflow as you no longer need a separate system to build a container from a definition file. You can simply create a definition file and run the build command. </p>
<p>For example, to create a simple toy container from this def file (<code>lolcow.def</code>):</p>
<div class="highlight"><pre><span></span><code>BootStrap: docker
From: ubuntu:20.04

%post
    apt-get -y update
    apt-get -y install cowsay lolcat

%environment
    export LC_ALL=C
    export PATH=/usr/games:$PATH

%runscript
    date | cowsay | lolcat
</code></pre></div>
<p>You can run:</p>
<div class="highlight"><pre><span></span><code>salloc -p interactive -c 4 
apptainer build lolcow.sif lolcow.def
</code></pre></div>
<p>This upgrade is live on Ruddle and will be applied on Grace during the December maintenance. For more information, please see the <a href="https://apptainer.org/docs/user/main/">Apptainer documentation site </a> and <a href="/clusters-at-yale/guides/containers/">our docs page on containers</a>.  </p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong>RELION/4.0.0-fosscuda-2020b</strong> for cryo-EM/cryo-tomography data processing is now available on Farnam. RELION/3.1 will no longer be updated by the RELION developer. Note that data processed with RELION 4 are not backwards compatible with RELION 3.</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: December 01, 2022
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="ruddle-maintenance">Ruddle Maintenance</h2>
<p><em>November 1, 2022</em></p>
<h3 id="software-updates">Software Updates</h3>
<ul>
<li>Security updates</li>
<li>Slurm updated to 22.05.5</li>
<li>Apptainer updated to 1.1.2 </li>
<li>Open OnDemand updated to 2.0.28</li>
</ul>
<h3 id="hardware-updates">Hardware Updates</h3>
<ul>
<li>No hardware changes during this maintenance.</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: November 01, 2022
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="november-2022">November 2022</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="ruddle-maintenance">Ruddle Maintenance</h4>
<p>The biannual scheduled maintenance for the Ruddle cluster will be occurring Nov 1-3. During this time, the cluster will be unavailable. See the Ruddle maintenance email announcements for more details.</p>
<h4 id="grace-and-milgram-maintenance-schedule-change">Grace and Milgram Maintenance Schedule Change</h4>
<p>We will be adjusting the timing of Grace and Milgram's scheduled maintenance periods. Starting this December, Grace's maintenance periods will occur in December and June, with the next scheduled for December 6-8, 2022. Milgram's next maintenance will instead be performed in February and August, with the next scheduled for February 7-9, 2023. Please refer to previously sent communications for more information and see the full maintenance schedule for next year on our status page.</p>
<h4 id="requeue-after-timeout">Requeue after Timeout</h4>
<p>The YCRC clusters all have maximum time-limits that sometimes are shorter than a job needs to finish. This can be a frustration for researchers trying to get a simulation or a project finished. However, a number of workflows have the ability to periodically save the status of a process to a file and restart from where it left off. This is often referred to as "checkpointing" and is built into many standard software tools, like Gaussian and Gromacs. </p>
<p>Slurm is able to send a signal to your job just before it runs out of time. Upon receiving this signal, you can have your job save its current status and automatically submit a new version of the job which picks up where it left off.  Here is an example of a simple script that resubmits a job after receiving the <code>TIMEOUT</code> signal:</p>
<div class="highlight"><pre><span></span><code>#!/bin/bash
#SBATCH -p day
#SBATCH -t 24:00:00
#SBATCH -c 1
#SBATCH --signal=B:10@30 # send the signal `10` at 30s before job finishes
#SBATCH --requeue        # mark this job eligible for requeueing

# define a `trap` that catches the signal and requeues the job
trap &quot;echo -n &#39;TIMEOUT @ &#39;; date; echo &#39;Resubmitting...&#39;; scontrol requeue ${SLURM_JOBID}  &quot; 10

# run the main code, with the `&amp;` to background the task
./my_code.exe &amp;

# wait for either the main code to finish to receive the signal
wait
</code></pre></div>
<p>This tells Slurm to send <code>SIGNAL10</code> at ~30s before the job finishes. Then we define an action (or <code>trap</code>) based on this signal which requeues the job. Dont forget to add the <code>&amp;</code> to the end of the main executable and the <code>wait</code> command so that the trap is able to catch the signal. </p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong>MATLAB/2022b</strong> is now available on all clusters.</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: November 01, 2022
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="farnam-maintenance">Farnam Maintenance</h2>
<p><em>October 4-5, 2022</em></p>
<h3 id="software-updates">Software Updates</h3>
<ul>
<li>Security updates</li>
<li>Slurm updated to 22.05.3</li>
<li>NVIDIA drivers updated to 515.65.01</li>
<li>Lmod updated to 8.7</li>
<li>Apptainer updated to 1.0.3 </li>
<li>Open OnDemand updated to 2.0.28</li>
</ul>
<h3 id="hardware-updates">Hardware Updates</h3>
<ul>
<li>No hardware changes during this maintenance.</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: October 05, 2022
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="october-2022">October 2022</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="farnam-maintenance">Farnam Maintenance</h4>
<p>The biannual scheduled maintenance for the Farnam cluster will be occurring Oct 4-6. During this time, the cluster will be unavailable. See the Farnam maintenance email announcements for more details.</p>
<h4 id="gibbs-maintenance">Gibbs Maintenance</h4>
<p>Additionally, the Gibbs storage system will be unavailable on Grace and Ruddle on Oct 4 to deploy an urgent firmware fix. All jobs on those clusters will be held, and no new jobs will be able to start during the upgrade to avoid job failures.</p>
<h4 id="new-command-for-interactive-jobs">New Command for Interactive Jobs</h4>
<p>The new version of Slurm (the scheduler) has improved the process of launching an interactive compute job. Instead of the clunky <code>srun --pty bash</code> syntax from previous versions, this is now replaced with <code>salloc</code>. In addition, the interactive partition is now the default partition for jobs launched using <code>salloc</code>. Thus a simple (1 core, 1 hour) interactive job can be requested like this:</p>
<div class="highlight"><pre><span></span><code>salloc
</code></pre></div>
<p>which will submit the job and then move your shell to the allocated compute node. </p>
<p>For MPI users, this allows multi-node parallel jobs to be properly launched inside an interactive compute job, which did not work as expected previously. For example, here is a two-node job, launched with <code>salloc</code> and then a parallel job-step launched with <code>srun</code>:</p>
<div class="highlight"><pre><span></span><code>[user@grace1 ~]$ salloc --nodes 2 --ntasks 2 --cpus-per-task 1
salloc: Nodes p09r07n[24,28] are ready for job

[user@p09r07n24 ~]$ srun hostname
p09r07n24.grace.hpc.yale.internal
P09r07n28.grace.hpc.yale.internal
</code></pre></div>
<p>For more information on <code>salloc</code>, please refer to <a href="https://slurm.schedmd.com/salloc.html">Slurms documentation</a>.</p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong>cellranger/7.0.1</strong> is now available on Farnam.</li>
<li><strong>LAMMPS/23Jun2022-foss-2020b-kokkos</strong> is now available on Grace.</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: October 01, 2022
    </div>
    <hr />
</div>

                
            
        </div>
    
        
        <div class="page" id="page11">

            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="september-2022">September 2022</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="software-module-extensions">Software Module Extensions</h4>
<p>Our software module utility (<a href="https://lmod.readthedocs.io/en/latest/">Lmod</a>) has been enhanced to enable searching for Python and R (among other software) extensions. This is a very helpful way to know which software modules contain a specific library or package. For example, to see what versions of <code>ggplot2</code> are available, use the <code>module spider</code> command. </p>
<div class="highlight"><pre><span></span><code>$ module spider ggplot2
--------------------------------------------------------
  ggplot2:
--------------------------------------------------------
     Versions:
        ggplot2/3.3.2 (E)
        ggplot2/3.3.3 (E)
        ggplot2/3.3.5 (E)
</code></pre></div>
<div class="highlight"><pre><span></span><code>$ module spider ggplot2/3.3.5
-----------------------------------------------------------
  ggplot2: ggplot2/3.3.5 (E)
-----------------------------------------------------------
    This extension is provided by the following modules. To access the extension you must load one of the following modules. Note that any module names in parentheses show the module location in the software hierarchy.

       R/4.2.0-foss-2020b
</code></pre></div>
<p>This indicates that by loading the <code>R/4.2.0-foss-2020b</code> module you will gain access to <code>ggplot2/3.3.5</code>. </p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong>topaz/0.2.5-fosscuda-2020b</strong> for use with RELION (fosscuda-2020b toolchain) is now available as a module on Farnam.</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: September 01, 2022
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="grace-maintenance">Grace Maintenance</h2>
<p><em>August 2-4, 2022</em></p>
<h3 id="software-updates">Software Updates</h3>
<ul>
<li>Security updates</li>
<li>Slurm updated to 22.05.2</li>
<li>NVIDIA drivers updated to 515.48.07 (except for nodes with K80 GPUs, which are stranded at 470.129.06)</li>
<li>Singularity replaced by Apptainer version 1.0.3 (note: the "singularity" command will still work as expected)</li>
<li>Lmod updated to 8.7</li>
<li>Open OnDemand updated to 2.0.26</li>
</ul>
<h3 id="hardware-updates">Hardware Updates</h3>
<ul>
<li>Core components of the ethernet network were upgraded to improve performance and increase overall capacity.</li>
</ul>
<h3 id="loomis-decommission-and-project-data-migration">Loomis Decommission and Project Data Migration</h3>
<p>After over eight years in service, the primary storage system on Grace, Loomis (<code>/gpfs/loomis</code>), will be retired later this year.</p>
<p><strong>Project.</strong> We have migrated all of the Loomis project space (<code>/gpfs/loomis/project</code>) to the Gibbs storage system at <code>/gpfs/gibbs/project</code> during the maintenance. You will need to update your scripts and workflows to point to the new location (<code>/gpfs/gibbs/project/&lt;group&gt;/&lt;netid&gt;</code>). The "project" symlink in your home directory has been updated to point to your new space (with a few exceptions described below), so scripts using the symlinked path will not need to be updated. If you have jobs in a pending state going into the maintenance that used the absolute Loomis path, we recommend canceling, updating and then re-submitting those jobs so they do not fail.</p>
<p>If you had a project space that exceeds the no-cost allocation (4 TiB), you have received a separate communication from us with details about your data migration. In these instances, your group has been granted a new, empty "project" space with the default no-cost quota. Any scripts will need to be updated accordingly.</p>
<p><strong>Conda.</strong>  By default, all conda environments are installed into your project directory. However, most conda environments do not survive being moved from one location to another, so you may need to regenerate your conda environment(s). To assist with this, we provide <a href="/clusters-at-yale/guides/conda-export/">conda-export documentation</a>.</p>
<p><strong>R.</strong>  Similarly, in 2022 we started redirecting user R packages to your project space to conserve home directory usage. If your R environment is not working as expected, we recommend deleting the old installation (found in <code>~/project/R/&lt;version&gt;</code>) and rerunning install.packages.</p>
<p><strong>Custom Software Installation.</strong> If you or your group had any self-installed software in the project directory, it is possible that the migration will have broken the software and it will need to be recompiled. <a href="/#get-help">Contact us</a> if you need assistance recompiling.</p>
<p><strong>Scratch60.</strong> The Loomis scratch space (<code>/gpfs/loomis/scratch60</code>) is now read-only. All data  in that directory will be purged in 60 days on <strong>October 3, 2022</strong>. Any data in <code>/gpfs/loomis/scratch60</code> you wish to retain needs to be copied into another location by that date (such as your Gibbs project or Palmer scratch).</p>
<h3 id="changes-to-non-interactive-sessions">Changes to Non-Interactive Sessions</h3>
<p>Non-interactive sessions (e.g. file transfers, commands sent over ssh) no longer load the standard cluster environment to alleviate performance issues due to unnecessary module loads. Please contact us if this change affects your workflow so we can resolve the issue or provide a workaround.</p>
    </div>
    <div class="blog-post-extra">
        Published: August 04, 2022
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="august-2022">August 2022</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="grace-maintenance-storage-changes">Grace Maintenance &amp; Storage Changes</h4>
<p>The biannual scheduled maintenance for the Grace cluster will be occurring August 2-4. During this time, the cluster will be unavailable. See the Grace maintenance email announcement for more details.</p>
<p>During the maintenance, significant changes will be made to the project and scratch60 directories on Grace. See <a href="https://docs.ycrc.yale.edu/data/loomis-decommission/">our website for more information and updates</a>.</p>
<h4 id="spinup-researcher-image-containers">SpinUp Researcher Image &amp; Containers</h4>
<p>Yale offers a simple portal for creating cloud-based compute resources called <a href="https://spinup.yalepages.org">SpinUp</a>. These cloud instances are hosted on Amazon Web Services, but have access to Yale services like Active Directory, DNS, and Storage at Yale. SpinUp offers a range of services including virtual machines, web servers, remote storage, and databases. </p>
<p>Part of this service is a Researcher Image, an Ubuntu-based system which contains a suite of pre-installed commonly utilized software utilities, including:
- PyTorch, TensorFlow, Keras, and other GPU-accelerated deep learning frameworks
- GCC, Cmake, Go, and other development tools
- Singularity/Apptainer and Docker for container development</p>
<p>We recommend researchers looking to develop containers for use on YCRC HPC resources to utilize SpinUp to build containers which can then be copied to the clusters. </p>
<p>If there are software utilities or commonly used tools that you would like added to the Researcher Image, let us know and we can work with the Cloud Team to get them integrated.</p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong>AFNI/2022.1.14</strong> is now available on Farnam and Milgram.</li>
<li><strong>cellranger/7.0.0</strong> is now available on Grace.</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: August 01, 2022
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="july-2022">July 2022</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="loomis-decommission">Loomis Decommission</h4>
<p>After almost a decade in service, the primary storage system on Grace, Loomis (<code>/gpfs/loomis</code>), will be retired later this year. The usage and capacity on Loomis will be replaced by two existing YCRC storage systems, Palmer and Gibbs, which are already available on Grace. Data in Loomis project storage will be migrated to <code>/gpfs/gibbs/project</code> during the upcoming August Grace maintenance. See the <a href="/data/loomis-decommission/">Loomis Decommission</a> documenation for more information and updates.</p>
<h4 id="updates-to-ood-jupyter-app">Updates to OOD Jupyter App</h4>
<p>OOD Jupyter App has been updated to handle conda environments more intelligently. Instead of listing all the conda envs in your account, the app now lists only the conda environments with Jupyter installed. If you do not see your desired environment listed in the dropdown, check that you have installed Jupyter in that environment. In addition, the jupyterlab checkbox in the app will only be visible if the environment selected has jupyterlab installed. </p>
<h4 id="ycrc-conda-environment">YCRC conda environment</h4>
<p><code>ycrc_conda_env.list</code> has been replaced by <code>ycrc_conda_env.sh</code>. To update your conda enviroments in OOD for the Jupyter App and RStudio Desktop (with Conda R), please run <code>ycrc_conda_env.sh update</code>.</p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong>miniconda/4.12.0</strong> is now available on all clusters</li>
<li><strong>RStudio/2022.02.3-492</strong> is now available on all clusters. This is currently the only version that is compatible with the graphic engine used by R/4.2.0-foss-2020b.</li>
<li><strong>fmriprep/21.0.2</strong> is now available on Milgram.</li>
<li><strong>cellranger/7.0.0</strong> is now available on Farnam.</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: July 01, 2022
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="milgram-maintenance">Milgram Maintenance</h2>
<p><em>June 7-8, 2022</em></p>
<h3 id="software-updates">Software Updates</h3>
<ul>
<li>Security updates</li>
<li>Slurm updated to 21.08.8-2</li>
<li>NVIDIA drivers updated to 515.43.04</li>
<li>Singularity replaced by Apptainer version 1.0.2 (note: the "singularity" command will still work as expected)</li>
<li>Lmod updated to 8.7</li>
<li>Open OnDemand updated to 2.0.23</li>
</ul>
<h3 id="hardware-updates">Hardware Updates</h3>
<ul>
<li>The hostnames of the compute nodes on Milgram were changed to bring them in line with YCRC naming conventions.</li>
</ul>
<h3 id="changes-to-non-interactive-sessions">Changes to non-interactive sessions</h3>
<p>Non-interactive sessions (e.g. file transfers, commands sent over ssh) no longer load the standard cluster environment to alleviate performance issues due to unnecessary module loads.
Please contact us if this change affects your workflow so we can resolve the issue or provide a workaround.</p>
    </div>
    <div class="blog-post-extra">
        Published: June 08, 2022
    </div>
    <hr />
</div>

                
            
        </div>
    
        
        <div class="page" id="page12">

            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="june-2022">June 2022</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="farnam-decommission-mccleary-announcement">Farnam Decommission &amp; McCleary Announcement</h4>
<p>After more than six years in service, we will be retiring the Farnam HPC cluster later this year. Farnam will be replaced with a new HPC cluster, McCleary. The McCleary HPC cluster will be Yale's first direct-to-chip liquid cooled cluster, moving the YCRC and the Yale research computing community into a more environmentally friendly future. For more information about the decommission process and the launch of McCleary, see <a href="/clusters/farnam-decommission">our website</a>.</p>
<h4 id="rstudio-with-module-r-has-been-retired-from-open-ondemand-as-of-june-1st">RStudio (with module R) has been retired from Open OnDemand as of June 1st</h4>
<p>Please switch to RStudio Server which provides a better user experience. For users using a conda environment with RStudio, RStudio (with Conda R) will continue to be served on Open OnDemand.</p>
<h4 id="milgram-maintenance">Milgram Maintenance</h4>
<p>The biannual scheduled maintenance for the Milgram cluster will be occurring June 7-9. During this time, the cluster will be unavailable. See the Milgram maintenance email announcements for more details.</p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong>QTLtools/1.3.1-foss-2020b</strong> is now available on Farnam.</li>
<li><strong>R/4.2.0-foss-2020b</strong> is available on all clusters.</li>
<li>Seurat for R/4.2.0 is now available on all clusters through the <strong>R-bundle-Bioconductor/3.15-foss-2020b-R-4.2.0</strong> module along with many other packages. Please check to see if any packages you need are available in these modules before running <code>install.packages</code>. </li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: June 01, 2022
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="ruddle-maintenance">Ruddle Maintenance</h2>
<p><em>May 2, 2022</em></p>
<h3 id="software-updates">Software Updates</h3>
<ul>
<li>Security updates</li>
<li>Slurm updated to 21.08.7</li>
<li>Singularity replaced by Apptainer version 1.0.1 (note: the "singularity" command will still work as expected)</li>
<li>Lmod updated to 8.7</li>
</ul>
<h3 id="changes-to-non-interactive-sessions">Changes to non-interactive sessions</h3>
<p>Non-interactive sessions (e.g. file transfers, commands sent over ssh) no longer load the standard cluster environment to alleviate performance issues due to unnecessary module loads.
Please contact us if this change affects your workflow so we can resolve the issue or provide a workaround.</p>
    </div>
    <div class="blog-post-extra">
        Published: May 01, 2022
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="may-2022">May 2022</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="ruddle-maintenance">Ruddle Maintenance</h4>
<p>The biannual scheduled maintenance for the Ruddle cluster will be occurring May 3-5. During this time, the cluster will be unavailable. See the Ruddle maintenance email announcements for more details.</p>
<h4 id="remote-visualization-with-hardware-acceleration">Remote Visualization with Hardware Acceleration</h4>
<p>VirtualGL is installed on all GPU nodes on Grace, Farnam, and Milgram to provide hardware accelerated 3D rendering. Instructions on how to use VirtualGL to accelerate your 3D applications can be found at <a href="https://docs.ycrc.yale.edu/clusters-at-yale/guides/virtualgl/">https://docs.ycrc.yale.edu/clusters-at-yale/guides/virtualgl/</a>. </p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong>Singularity is now called "Apptainer".</strong> Singularity is officially named Apptainer as part of its move to the Linux Foundation. The new command <code>apptainer</code> works as drop-in replacement for <code>singularity</code>. However, the previous <code>singularity</code> command will also continue to work for the foreseeable future so no change is needed. The upgrade to Apptainer is on Grace, Farnam and Ruddle (as of the maintenance completion). Milgram will be upgraded to Apptainer during the June maintenance.</li>
<li><strong>Slurm</strong> has been upgraded to version 21.08.6 on Grace</li>
<li><strong>MATLAB/2022a</strong> is available on all clusters</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: May 01, 2022
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="farnam-maintenance">Farnam Maintenance</h2>
<p><em>April 4-7, 2022</em></p>
<h3 id="software-updates">Software Updates</h3>
<ul>
<li>Security updates</li>
<li>Slurm updated to 21.08.6</li>
<li>NVIDIA drivers updated to 510.47.03 (note: driver for NVIDIA K80 GPUs was upgraded to 470.103.01)</li>
<li>Singularity replaced by Apptainer version 1.0.1 (note: the "singularity" command will still work as expected)</li>
<li>Open OnDemand updated to 2.0.20</li>
</ul>
<h3 id="hardware-updates">Hardware Updates</h3>
<ul>
<li>Four new nodes with 4 NVIDIA GTX3090 GPUs each have been added</li>
</ul>
<h3 id="changes-to-the-bigmem-partition">Changes to the <code>bigmem</code> Partition</h3>
<p>Jobs requesting less than 120G of memory are no longer allowed in the "bigmem" partition. Please submit these jobs to the general or scavenge partitions instead.</p>
<h3 id="changes-to-non-interactive-sessions">Changes to non-interactive sessions</h3>
<p>Non-interactive sessions (e.g. file transfers, commands sent over ssh) no longer load the standard cluster environment to alleviate performance issues due to unnecessary module loads.
Please contact us if this change affects your workflow so we can resolve the issue or provide a workaround.</p>
    </div>
    <div class="blog-post-extra">
        Published: April 07, 2022
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="april-2022">April 2022</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="updates-to-r-on-open-ondemand">Updates to R on Open OnDemand</h4>
<p>RStudio Server is out of beta! With the deprecation of R 3.x (see below), we will be removing RStudio Desktop with module R from Open OnDemand on June 1st.</p>
<h4 id="improvements-to-r-installpackages-paths">Improvements to R install.packages Paths</h4>
<p>Starting with the R 4.1.0 software module, we now automatically set an environment variable (<code>R_LIBS_USER</code>) which directs these packages to be stored in your project space. This will helps ensure that packages are not limited by home-space quotas and that packages installed for different versions of R are properly separated from each other. Previously installed packages should still be available and there should be no disruption from the change.</p>
<h4 id="instructions-for-running-a-mysql-server-on-the-clusters">Instructions for Running a MySQL Server on the Clusters</h4>
<p>Occasionally it could be useful for a user to run their own MySQL database server on one of the clusters.  Until now, that has not been possible, but recently we found a way using singularity.  Instructions may be found in our <a href="/clusters-at-yale/guides/mysql/">new MySQL guide</a>.</p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong>R 3.x modules have been deprecated</strong> on all clusters and are no longer supported. If you need to continue to use an older version of R, look at our <a href="https://docs.ycrc.yale.edu/clusters-at-yale/guides/r/#conda-based-r-environments">R conda documentation</a>.</li>
<li><strong>R/4.1.0-foss-2020b</strong> is now available on all clusters.</li>
<li><strong>Seurat/4.1.0-foss-2020b-R-4.1.0</strong> (for using the Seurat R package) is now available on all clusters.</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: April 01, 2022
    </div>
    <hr />
</div>

                
            
        </div>
    
        
        <div class="page" id="page13">

            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="march-2022">March 2022</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="snapshots">Snapshots</h4>
<p>Snapshots are now available on all clusters for home and project spaces. Snapshots enable self-service restoration of modified or deleted files for at least 2 days in the past. <a href="/data/backups">See our User Documentation for more details on availability and instructions.</a></p>
<h4 id="ood-file-browser-tip-shortcuts">OOD File Browser Tip: Shortcuts</h4>
<p>You can add shortcuts to your favorite paths in the OOD File Browser. <a href="https://docs.ycrc.yale.edu/clusters-at-yale/access/ood/#customize-favorite-paths">See our OOD documentation for instructions on setting up shortcuts.</a></p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong>R/4.1.0-foss-2020b</strong> is now on Grace.</li>
<li><strong>GCC/11.2.0</strong> is now on Grace.</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: March 01, 2022
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="grace-maintenance">Grace Maintenance</h2>
<p><em>February 3-6, 2022</em></p>
<h3 id="software-updates">Software Updates</h3>
<ul>
<li>Latest security patches applied</li>
<li>Slurm updated to version 21.08.5</li>
<li>NVIDIA driver updated to version 510.39.01 (except for nodes with K80 GPUs which are stranded at 470.82.01)</li>
<li>Singularity updated to version 3.8.5</li>
<li>Open OnDemand updated to version 2.0.20</li>
</ul>
<h3 id="hardware-updates">Hardware Updates</h3>
<ul>
<li>Changes have been made to networking to improve performance of certain older compute nodes</li>
</ul>
<h3 id="changes-to-grace-home-directories">Changes to Grace Home Directories</h3>
<p>During the maintenance, all home directories on Grace have been moved to our new all-flash storage filesystem, Palmer. The move is in anticipation of the decommissioning of Loomis at the end of the year and will provide a robust login experience by protecting home directory interactions from data intensive compute jobs.</p>
<p>Due to this migration, your home directory path has changed from <code>/gpfs/loomis/home.grace/&lt;netid&gt;</code> to <code>/vast/palmer/home.grace/&lt;netid&gt;</code>.
Your home directory can always be referenced in bash and submission scripts and from the command line with the <code>$HOME</code> variable. Please update any scripts and workflows accordingly.</p>
<h3 id="interactive-jobs">Interactive Jobs</h3>
<p>We have added an additional way to request an interactive job. The Slurm command <code>salloc</code> can be used to start an interactive job similar to <code>srun --pty bash</code>. In addition to being a simpler command (no <code>--pty bash</code> is needed), <code>salloc</code> jobs can be used to interactively test <code>mpirun</code> executables.</p>
<h3 id="palmer-scratch">Palmer scratch</h3>
<p>Palmer is out of beta! We have fixed the issue with Plink on Palmer, so now you can use Palmer scratch for any workloads. See <a href="/data/hpc-storage/#60-day-scratch">https://docs.ycrc.yale.edu/data/hpc-storage#60-day-scratch</a> for more information on Palmer scratch. </p>
    </div>
    <div class="blog-post-extra">
        Published: February 06, 2022
    </div>
    <hr />
</div>

                
            
                
                
                
                

                
                

                
                
                    
                
                
                <div class="blog-post">
    
    <div class="blog-post-description">
        <h2 id="february-2022">February 2022</h2>
<h3 id="announcements">Announcements</h3>
<h4 id="grace-maintenance">Grace Maintenance</h4>
<p>The biannual scheduled maintenance for the Grace cluster will be occurring February 1-3. During this time, the cluster will be unavailable. See the Grace maintenance email announcement for more details.</p>
<h4 id="data-transfers">Data Transfers</h4>
<p>For non-Milgram users doing data transfers, transfers should not be performed on the login nodes. We have a few alternative ways to get better networking and reduce the impact on the clusters login nodes:</p>
<ol>
<li><strong>Dedicated transfer node</strong>. Each cluster has a dedicated transfer node, <code>transfer-&lt;cluster&gt;.hpc.yale.edu</code>. You can ssh directly to this node and run commands. </li>
<li><strong>transfer Slurm partition</strong>. This is a small partition managed by the scheduler for doing data transfer. You can submit jobs to it using <code>srun/sbatch -p transfer </code> 
*For recurring or periodic data transfers (such as using cron), please use Slurms <a href="https://docs.ycrc.yale.edu/clusters-at-yale/job-scheduling/scrontab/">scrontab</a> to schedule jobs that run on the transfer partition instead.</li>
<li><strong>Globus</strong>. For robust transfers of larger amount of data, see our <a href="https://docs.ycrc.yale.edu/data/globus/">Globus</a> documentation.</li>
</ol>
<p>More info about data transfers can be found in our <a href="https://docs.ycrc.yale.edu/data/transfer/">Data Transfer</a> documentation.</p>
<h3 id="software-highlights">Software Highlights</h3>
<ul>
<li><strong>Rclone</strong> is now installed on all nodes and loading the module is no longer necessary.</li>
<li><strong>MATLAB/2021b</strong> is now on all clusters.</li>
<li><strong>Julia/1.7.1-linux-x86_64</strong> is now on all clusters.</li>
<li><strong>Mathematica/13.0.0</strong> is now on Grace.</li>
<li><strong>QuantumESPRESSO/6.8-intel-2020b</strong> and <strong>QuantumESPRESSO/7.0-intel-2020b</strong> are now on Grace.</li>
<li><strong>Mathematica</strong> <a href="https://docs.ycrc.yale.edu/clusters-at-yale/guides/mathematica/#configure-environment-for-parallel-jobs">documentation</a> has been updated with regards to configuring parallel jobs.</li>
</ul>
    </div>
    <div class="blog-post-extra">
        Published: February 01, 2022
    </div>
    <hr />
</div>

                
            
        </div>
    
</div>

<div class="blog-center ">
    <div class="blog-pagination " id="blog-pagination">
        
            <a class="page-number" href="#blog-p1">1</a>
        
            <a class="page-number" href="#blog-p2">2</a>
        
            <a class="page-number" href="#blog-p3">3</a>
        
            <a class="page-number" href="#blog-p4">4</a>
        
            <a class="page-number" href="#blog-p5">5</a>
        
            <a class="page-number" href="#blog-p6">6</a>
        
            <a class="page-number" href="#blog-p7">7</a>
        
            <a class="page-number" href="#blog-p8">8</a>
        
            <a class="page-number" href="#blog-p9">9</a>
        
            <a class="page-number" href="#blog-p10">10</a>
        
            <a class="page-number" href="#blog-p11">11</a>
        
            <a class="page-number" href="#blog-p12">12</a>
        
            <a class="page-number" href="#blog-p13">13</a>
        
    </div>
    
</div></p>
                
                  
                    

<hr>
<div class="md-source-date">
  <small>
    
      Last update: <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">June 9, 2022</span>
      
    
  </small>
</div>
                  
                
              
              
                


              
            </article>
          </div>
        </div>
        
      </main>
      
        <!--
  Copyright (c) 2016-2020 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->



<!-- Application footer -->
<footer class="md-footer">
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div>
        <a href="https://www.yale.edu" title="Yale" target="_blank">
          <img alt="Yale Logo" src="/img/yale-white.png" height="60" style="padding: 8px; height: 60px;">
	</a>
      </div>
 
      <!-- Accessability/Privacy information -->
      <div class="md-footer-copyright">
        <a href="https://usability.yale.edu/web-accessibility/accessibility-yale">Accessibility at Yale</a>
            
        <a title="Yale Privacy policy" href="http://www.yale.edu/privacy-policy">Privacy policy</a>
          
      </div>

      <!-- Social links -->
      
  <div class="md-footer-social">
    
      
      
        
        
      
      <a href="https://ycrc.yale.edu" target="_blank" rel="noopener" title="ycrc.yale.edu" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19.07 4.93C17.22 3 14.66 1.96 12 2c-2.66-.04-5.21 1-7.06 2.93C3 6.78 1.96 9.34 2 12c-.04 2.66 1 5.21 2.93 7.06C6.78 21 9.34 22.04 12 22c2.66.04 5.21-1 7.06-2.93C21 17.22 22.04 14.66 22 12c.04-2.66-1-5.22-2.93-7.07M17 12v6h-3.5v-5h-3v5H7v-6H5l7-7 7.5 7H17z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="http://help.ycrc.yale.edu/" target="_blank" rel="noopener" title="help.ycrc.yale.edu" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m15.07 11.25-.9.92C13.45 12.89 13 13.5 13 15h-2v-.5c0-1.11.45-2.11 1.17-2.83l1.24-1.26c.37-.36.59-.86.59-1.41a2 2 0 0 0-2-2 2 2 0 0 0-2 2H8a4 4 0 0 1 4-4 4 4 0 0 1 4 4 3.2 3.2 0 0 1-.93 2.25M13 19h-2v-2h2M12 2A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10c0-5.53-4.5-10-10-10z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="https://research.computing.yale.edu/system-status" target="_blank" rel="noopener" title="research.computing.yale.edu" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M7.5 4A5.5 5.5 0 0 0 2 9.5c0 .5.09 1 .22 1.5H6.3l1.27-3.37c.3-.8 1.48-.88 1.86 0L11.5 13l.59-1.42c.13-.33.48-.58.91-.58h8.78c.13-.5.22-1 .22-1.5A5.5 5.5 0 0 0 16.5 4c-1.86 0-3.5.93-4.5 2.34C11 4.93 9.36 4 7.5 4M3 12.5a1 1 0 0 0-1 1 1 1 0 0 0 1 1h2.44L11 20c1 .9 1 .9 2 0l5.56-5.5H21a1 1 0 0 0 1-1 1 1 0 0 0-1-1h-7.6l-.93 2.3c-.4 1.01-1.55.87-1.92.03L8.5 9.5l-.96 2.33c-.15.38-.49.67-.94.67H3z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="https://github.com/ycrc" target="_blank" rel="noopener" title="github.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="https://twitter.com/yalecrc" target="_blank" rel="noopener" title="twitter.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
      </a>
    
  </div>

    <!-- Repository containing source -->
    
      <div class="md-header-nav__source">
        
<a href="https://github.com/ycrc/ycrc.github.io/tree/src" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    github/ycrc/docs
  </div>
</a>
      </div>
    

    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.sections"], "search": "../assets/javascripts/workers/search.fcfe8b6d.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.b1047164.min.js"></script>
      
        <script src="../_static/js/extra.js"></script>
      
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
      
    
  </body>
</html><script>var currentPage = 0
const lastComponent = window.location.href.split("/").slice(-1).pop()

if (lastComponent && lastComponent.slice(0, 7) == "#blog-p") {
  const page = parseInt(lastComponent.slice(7))
  if (page) {
    currentPage = page - 1
  }
}

function scrollToTop() {
  setTimeout(function () {
    window.scrollTo(0, 0);
  }, 100);
}

var pagination = document.getElementById("blog-pagination");
if (pagination) {
  var links = pagination.getElementsByClassName("page-number");
  if (links.length) {
    for (var i = 0; i < links.length; i++) {
      // Toggle pagination highlight
      links[i].addEventListener("click", function () {
        var current = pagination.getElementsByClassName("active");
        if (current.length) {
          current[0].className = current[0].className.replace(
            " active", ""
          );
        }
        this.className += " active";

        // Togglg visibility of pages
        const destPage = parseInt(this.textContent)
        var pages = document.getElementsByClassName("page")
        if (destPage && pages.length) {
          for (var j = 0; j < pages.length; j++) {
            const pageId = parseInt(pages[j].id.replace("page", ""))
            if (pageId != destPage) {
              // This is not the destination page
              if (!pages[j].className.includes("blog-hidden")) {
                pages[j].className += " blog-hidden"
              }
            } else {
              // This is the destination page
              pages[j].className = pages[j].className.replace(" blog-hidden", "")
            }
            scrollToTop();
          }
        }
      });
    }
    links[currentPage].className += " active"
    links[currentPage].click();
  }
}
</script>